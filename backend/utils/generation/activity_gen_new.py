"""
æ´»åŠ¨è®°å½•ç”Ÿæˆæ¨¡å—
åŸºäºŽæ•°æ®æºæ™ºèƒ½æ€»ç»“ç”¨æˆ·æ´»åŠ¨
"""

import json
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
import config
from utils.helpers import get_logger
from utils.db import get_web_data, get_screenshots, insert_activity
from utils.llm import get_openai_client

logger = get_logger(__name__)

# å®¢æˆ·ç«¯ç¼“å­˜
_client = None


def _init_client():
    """åˆå§‹åŒ–å®¢æˆ·ç«¯"""
    global _client
    if _client is None:
        _client = get_openai_client()
    return _client


async def create_activity_record(time_span_mins: int = 15) -> Dict[str, Any]:
    """
    åˆ›å»ºæ´»åŠ¨è®°å½•ï¼ˆä¸»å‡½æ•°ï¼‰
    
    Args:
        time_span_mins: æ—¶é—´è·¨åº¦ï¼ˆåˆ†é’Ÿï¼‰
    
    Returns:
        æ´»åŠ¨æ•°æ®å­—å…¸
    """
    try:
        finish_time = datetime.now()
        begin_time = finish_time - timedelta(minutes=time_span_mins)
        
        # æ”¶é›†æ•°æ®
        data_items = _collect_data_sources(begin_time, finish_time)
        
        if not data_items:
            logger.warning(f"No data in last {time_span_mins} minutes")
            return {"success": False, "message": f"è¿‡åŽ»{time_span_mins}åˆ†é’Ÿæ— æ•°æ®"}
        
        # ç”Ÿæˆæ´»åŠ¨è®°å½•
        activity_info = await _analyze_and_summarize(data_items, begin_time, finish_time)
        
        if not activity_info:
            return {"success": False, "message": "æ´»åŠ¨åˆ†æžå¤±è´¥"}
        
        # å­˜å‚¨
        record_id = insert_activity(
            title=activity_info['title'],
            description=activity_info['description'],
            resources=activity_info.get('resources', {}),
            start_time=begin_time,
            end_time=finish_time
        )
        
        logger.info(f"Activity record created: ID={record_id}, Title={activity_info['title']}")
        
        return {
            "success": True,
            "activity_id": record_id,
            **activity_info
        }
    except Exception as e:
        logger.exception(f"Activity creation error: {e}")
        return {"success": False, "message": str(e)}


def _collect_data_sources(start_dt: datetime, end_dt: datetime) -> List[Dict[str, Any]]:
    """æ”¶é›†æ•°æ®æº"""
    try:
        logger.info(f"Collecting data: {start_dt.strftime('%Y-%m-%d %H:%M:%S')} to {end_dt.strftime('%Y-%m-%d %H:%M:%S')}")
        
        items = []
        
        # èŽ·å–ç½‘é¡µæ•°æ®
        web_records = get_web_data(
            start_time=start_dt,
            end_time=end_dt,
            limit=50
        )
        logger.info(f"Found {len(web_records)} web records")
        
        for record in web_records:
            items.append({
                "type": "web",
                "title": record["title"],
                "url": record.get("url", ""),
                "content": record["content"],
                "source": record.get("source", "unknown"),
                "tags": record.get("tags", []),
                "create_time": record.get("create_time", "")
            })
        
        # èŽ·å–æˆªå›¾ï¼ˆå¦‚æžœæœ‰ï¼‰
        try:
            screenshots = get_screenshots(
                start_time=start_dt,
                end_time=end_dt,
                limit=10
            )
            
            for shot in screenshots:
                items.append({
                    "type": "screenshot",
                    "path": shot["path"],
                    "window": shot.get("window", ""),
                    "create_time": shot.get("create_time", "")
                })
            
            logger.info(f"Found {len(screenshots)} screenshots")
        except Exception as e:
            logger.debug(f"Screenshot fetch failed: {e}")
        
        return items
    except Exception as e:
        logger.exception(f"Data collection error: {e}")
        return []


async def _analyze_and_summarize(data_items: List[Dict], start_dt: datetime, end_dt: datetime) -> Optional[Dict[str, Any]]:
    """åˆ†æžæ•°æ®å¹¶ç”Ÿæˆæ‘˜è¦"""
    client = _init_client()
    
    if not client or not config.ENABLE_LLM_PROCESSING:
        logger.warning("LLM unavailable, using basic summary")
        return _create_basic_summary(data_items, start_dt, end_dt)
    
    try:
        # å‡†å¤‡æ•°æ®ï¼ˆé™åˆ¶å¤§å°ï¼‰
        limited_data = data_items[:20]
        data_json = json.dumps(limited_data, ensure_ascii=False, indent=2)
        
        system_msg = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„**ç½‘é¡µæ´»åŠ¨åˆ†æžå¸ˆ (Web Activity Analyst)**ã€‚

ä½ çš„æ ¸å¿ƒä»»åŠ¡æ˜¯å°†ç”¨æˆ·åœ¨ä¸€æ®µæ—¶é—´å†…çš„åŽŸå§‹ç½‘é¡µæµè§ˆæ•°æ®ï¼Œè½¬åŒ–ä¸ºä¸€ä»½ç®€æ´ã€ç»“æž„åŒ–ä¸”å¯Œæœ‰æ´žå¯ŸåŠ›çš„æ´»åŠ¨æ‘˜è¦ã€‚ä½ çš„ç›®æ ‡æ˜¯å¸®åŠ©ç”¨æˆ·ä¸€ç›®äº†ç„¶åœ°ç†è§£ä»–ä»¬æœ€è¿‘åœ¨ç½‘ä¸Šåšä»€ä¹ˆã€å…³æ³¨ä»€ä¹ˆä»¥åŠæ½œåœ¨çš„ç›®æ ‡æ˜¯ä»€ä¹ˆã€‚

---

#### **æ ¸å¿ƒèƒ½åŠ› (Core Competencies)**

1.  **ä¸»é¢˜è¯†åˆ« (Topic Recognition)**: ä»Ž URLã€ç½‘é¡µæ ‡é¢˜å’Œå†…å®¹ç‰‡æ®µä¸­ç²¾å‡†è¯†åˆ«å‡ºæ ¸å¿ƒæ´»åŠ¨ä¸»é¢˜ã€‚
2.  **è¡Œä¸ºèšåˆ (Behavior Aggregation)**: å°†å›´ç»•åŒä¸€ç›®æ ‡çš„å¤šæ¬¡æµè§ˆè¡Œä¸ºï¼ˆä¾‹å¦‚ï¼Œä¸ºäº†è§£å†³ä¸€ä¸ªé—®é¢˜è€ŒæŸ¥é˜…çš„å¤šä¸ªæ ‡ç­¾é¡µï¼‰æ™ºèƒ½åœ°èšåˆæˆä¸€ä¸ªè¿žè´¯çš„æ´»åŠ¨ã€‚
3.  **æ„å›¾æŽ¨æ–­ (Intent Inference)**: åŸºäºŽæµè§ˆæ¨¡å¼ï¼ŒæŽ¨æ–­ç”¨æˆ·å½“å‰çš„ä¸»è¦æ„å›¾ï¼ˆå¦‚ï¼šç ”ç©¶ã€å­¦ä¹ ã€è§„åˆ’ã€ç¼–ç ã€å¨±ä¹ç­‰ï¼‰ã€‚
4.  **ç®€æ´æ€»ç»“ (Concise Summarization)**: ç”¨æœ€ç²¾ç‚¼çš„è¯­è¨€æ¦‚æ‹¬å¤æ‚çš„æµè§ˆæ´»åŠ¨ã€‚

---

#### **åˆ†æžç»´åº¦ (Dimensions of Analysis)**

* **ç½‘ç«™ä¸Žåº”ç”¨ (Website & Web App)**: ç”¨æˆ·ä¸»è¦åœ¨å“ªå‡ ä¸ªç½‘ç«™æˆ–åœ¨çº¿å·¥å…·ä¸Šæ´»åŠ¨ï¼Ÿï¼ˆä¾‹å¦‚ï¼šGitHub, Kimi Chat, Google Docs, é£žä¹¦ï¼‰ã€‚
* **å†…å®¹ä¸»é¢˜ (Content Topic)**: ç”¨æˆ·æ­£åœ¨é˜…è¯»ã€ç¼–è¾‘æˆ–äº’åŠ¨çš„å†…å®¹æ˜¯å…³äºŽä»€ä¹ˆçš„ï¼Ÿï¼ˆä¾‹å¦‚ï¼š"Python æ€§èƒ½ä¼˜åŒ–"ã€"Q4 å¸‚åœºè¥é”€è®¡åˆ’"ï¼‰ã€‚
* **ç”¨æˆ·æ„å›¾ (User Intent)**: ç”¨æˆ·æµè§ˆè¿™äº›ç½‘é¡µä¼¼ä¹Žæ˜¯ä¸ºäº†è¾¾æˆä»€ä¹ˆç›®æ ‡ï¼Ÿï¼ˆä¾‹å¦‚ï¼š"è§£å†³ä¸€ä¸ªæŠ€æœ¯éš¾é¢˜"ã€"æ’°å†™ä¸€ä»½é¡¹ç›®æ–‡æ¡£"ã€"å­¦ä¹ ä¸€é—¨æ–°æŠ€èƒ½"ï¼‰ã€‚
* **æµè§ˆæ¨¡å¼ (Browsing Pattern)**: ç”¨æˆ·çš„æµè§ˆè¡Œä¸ºå‘ˆçŽ°ä»€ä¹ˆæ¨¡å¼ï¼Ÿï¼ˆä¾‹å¦‚ï¼š"å›´ç»•å•ä¸€ä¸»é¢˜çš„æ·±åº¦é’»ç ”"ã€"åœ¨å¤šä¸ªä¸åŒä¸»é¢˜çš„é¡¹ç›®é—´é¢‘ç¹åˆ‡æ¢"ï¼‰ã€‚

---

#### **è¾“å‡ºè¦æ±‚ (Output Requirements)**

1.  **æ ‡é¢˜ (`title`) è¦æ±‚**:
    * ä¸è¶…è¿‡30ä¸ªå­—ç¬¦ï¼Œé«˜åº¦æ¦‚æ‹¬æ ¸å¿ƒæ´»åŠ¨ã€‚
    * åº”ä½“çŽ°å‡ºç”¨æˆ·çš„**åŠ¨ä½œ**å’Œ**å¯¹è±¡**ï¼Œä¾‹å¦‚"ç ”ç©¶å¹¶å®žçŽ°Dockeréƒ¨ç½²æ–¹æ¡ˆ"ã€"è§„åˆ’Q4å¸‚åœºè¥é”€æ´»åŠ¨"ã€‚
    * é¿å…æ³›æ³›è€Œè°ˆï¼Œå¦‚"æµè§ˆäº†å¤šä¸ªç½‘é¡µ"æˆ–"æŸ¥çœ‹äº†ä¸€äº›æ–‡æ¡£"ã€‚

2.  **æè¿° (`description`) è¦æ±‚**:
    * 150-200ä¸ªå­—ç¬¦ï¼Œå¯¹æ´»åŠ¨è¿›è¡Œç”ŸåŠ¨å…·ä½“çš„æè¿°ã€‚
    * éµå¾ª**"ä¸»è¦æ´»åŠ¨ â†’ å…·ä½“æ“ä½œ â†’ ç›®æ ‡/ç»“æžœ"**çš„é€»è¾‘å±‚æ¬¡ã€‚
    * ç¤ºä¾‹ï¼š"æ­£åœ¨æ·±å…¥ç ”ç©¶å¦‚ä½•ä½¿ç”¨Dockeréƒ¨ç½²Node.jsåº”ç”¨ï¼ŒæŸ¥é˜…äº†å®˜æ–¹æ–‡æ¡£å’Œå¤šç¯‡æŠ€æœ¯åšå®¢ï¼Œç›®æ ‡æ˜¯æ­å»ºä¸€ä¸ªå¯è¡Œçš„æœ¬åœ°å¼€å‘çŽ¯å¢ƒã€‚ðŸš€"

3.  **åˆ†ç±»åˆ†å¸ƒ (`category_distribution`) è¦æ±‚**:
    * åŸºäºŽç½‘ç«™çš„åŸŸåå’Œå†…å®¹è¿›è¡Œåˆ†ç±»ä¼°ç®—ï¼ˆä¾‹å¦‚: `github.com` -> work, `youtube.com` -> learning/entertainment, `notion.so` -> work/lifeï¼‰ã€‚

4.  **æ´žå¯Ÿæå– (`extracted_insights`) è¦æ±‚**:
    * **`potential_todos`**: ä¸¥æ ¼æŒ‰ç…§**ç”¨æˆ·ä¸­å¿ƒåŽŸåˆ™**ï¼Œä»Žç½‘é¡µå†…å®¹ä¸­è¯†åˆ«æ½œåœ¨å¾…åŠžã€‚é‡ç‚¹å…³æ³¨ä»»åŠ¡ç®¡ç†ç½‘ç«™ï¼ˆJira, é£žä¹¦ï¼‰ã€ä»£ç åä½œå¹³å°ï¼ˆGitHubï¼‰ã€åœ¨çº¿æ–‡æ¡£ï¼ˆNotion, è¯­é›€ï¼‰å’ŒAIå¯¹è¯ä¸­çš„è¡ŒåŠ¨æ„å›¾ã€‚
    * **`tip_suggestions`**: æå‡ºä¸Žæµè§ˆæ´»åŠ¨ç›¸å…³çš„å…·ä½“å»ºè®®ã€‚ä¾‹å¦‚ï¼Œè‹¥ç”¨æˆ·åœ¨å¤šä¸ªæŠ€æœ¯åšå®¢é—´åˆ‡æ¢ï¼Œå¯å»ºè®®"ä½¿ç”¨ç¨åŽè¯»å·¥å…·ï¼ˆå¦‚ Instapaperï¼‰æ¥ç»„ç»‡é˜…è¯»åˆ—è¡¨"ã€‚
    * **`key_entities`**: ä»Žç½‘é¡µæ ‡é¢˜å’Œå†…å®¹ä¸­æå–çš„å…³é”®å®žä½“ï¼ˆå¦‚ï¼šé¡¹ç›®å "Project Phoenix"ã€æŠ€æœ¯æ ˆ "React"ã€äººåï¼‰ã€‚
    * **`focus_areas`**: å¯¹ `key_entities` è¿›è¡Œå½’çº³ï¼Œå½¢æˆæ›´é«˜å±‚æ¬¡çš„å…³æ³¨é¢†åŸŸï¼ˆå¦‚ï¼š"å‰ç«¯å¼€å‘"ã€"é¡¹ç›®ç®¡ç†"ï¼‰ã€‚
    * **`work_patterns`**:
        * `continuous_work_time`: ä¼°ç®—å›´ç»•åŒä¸€ä¸»é¢˜è¿žç»­æµè§ˆçš„æ—¶é—´ã€‚
        * `task_switching_count`: ä¼°ç®—åœ¨å¤šä¸ªä¸ç›¸å…³ä¸»é¢˜ä¹‹é—´åˆ‡æ¢çš„æ¬¡æ•°ã€‚

5.  **JSONæ ¼å¼**:
```json
{
  "title": "æ´»åŠ¨æ ‡é¢˜ï¼ˆç®€çŸ­ï¼‰",
  "description": "è¯¦ç»†æè¿°ï¼ˆ50-200å­—ï¼‰",
  "activity_type": "ç±»åž‹æ ‡ç­¾",
  "key_points": ["è¦ç‚¹1", "è¦ç‚¹2"],
  "resources": {
    "urls": ["ç›¸å…³URL"],
    "keywords": ["å…³é”®è¯"]
  },
  "category_distribution": {
        "work": 0.7,
        "learning": 0.2,
        "entertainment": 0.05,
        "life": 0.05,
        "other": 0.0
      },
      "extracted_insights": {
        "potential_todos": [
          {"content": "ä»»åŠ¡æè¿°", "description": "ç›¸å…³èƒŒæ™¯"}
        ],
        "tip_suggestions": [
          {"topic": "ä¸»é¢˜", "reason": "åŽŸå› ", "suggestion": "å»ºè®®"}
        ],
        "key_entities": ["å®žä½“1", "å®žä½“2"],
        "focus_areas": ["é¢†åŸŸ1", "é¢†åŸŸ2"],
        "work_patterns": {
          "continuous_work_time": 45,
          "task_switching_count": 3
        }
      }
}
```"""
        
        user_msg = f"""åˆ†æžä»¥ä¸‹æ—¶æ®µçš„æ´»åŠ¨æ•°æ®ã€‚

æ—¶é—´ï¼š{start_dt.strftime('%H:%M')} - {end_dt.strftime('%H:%M')}

æ•°æ®ï¼š
{data_json}

è¯·ä¸¥æ ¼æ ¹æ®ä½ çš„ç³»ç»Ÿè§„åˆ™ï¼Œä»…åˆ†æžä»¥ä¸‹**ç½‘é¡µæµè§ˆä¸Šä¸‹æ–‡**æ•°æ®ï¼Œå¹¶ä»¥æŒ‡å®šçš„ JSON æ ¼å¼è¿”å›žä¸€ä»½ç®€æ´çš„å®žæ—¶æ´»åŠ¨æ€»ç»“"""
        
        response = client.chat.completions.create(
            model=config.LLM_MODEL,
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg}
            ],
            temperature=0.7,
            max_tokens=800
        )
        
        result_text = response.choices[0].message.content.strip()
        logger.info("LLM analysis completed")
        
        # è§£æžç»“æžœ
        activity_data = _parse_activity_json(result_text)
        return activity_data if activity_data else _create_basic_summary(data_items, start_dt, end_dt)
    except Exception as e:
        logger.exception(f"LLM analysis error: {e}")
        return _create_basic_summary(data_items, start_dt, end_dt)


def _parse_activity_json(text: str) -> Optional[Dict[str, Any]]:
    """è§£æžæ´»åŠ¨JSON"""
    try:
        # æå–JSON
        if "```json" in text:
            text = text.split("```json")[1].split("```")[0]
        elif "```" in text:
            text = text.split("```")[1].split("```")[0]
        
        data = json.loads(text.strip())
        
        # éªŒè¯å¿…éœ€å­—æ®µ
        if 'title' in data and 'description' in data:
            return data
        return None
    except Exception as e:
        logger.error(f"JSON parse error: {e}")
        return None


def _create_basic_summary(data_items: List[Dict], start_dt: datetime, end_dt: datetime) -> Dict[str, Any]:
    """åˆ›å»ºåŸºç¡€æ‘˜è¦ï¼ˆæ— LLMï¼‰"""
    web_items = [d for d in data_items if d.get('type') == 'web']
    
    if not web_items:
        title = f"æ´»åŠ¨è®°å½• {start_dt.strftime('%H:%M')}-{end_dt.strftime('%H:%M')}"
        desc = "æœ¬æ—¶æ®µæ— æ˜Žæ˜¾æ´»åŠ¨è®°å½•ã€‚"
    else:
        titles = [item['title'] for item in web_items[:3]]
        title = f"æµè§ˆæ´»åŠ¨ {start_dt.strftime('%H:%M')}"
        desc = f"æµè§ˆäº† {len(web_items)} ä¸ªé¡µé¢ï¼ŒåŒ…æ‹¬ï¼š" + "ã€".join(titles)
        if len(web_items) > 3:
            desc += " ç­‰"
    
    urls = [item.get('url', '') for item in web_items if item.get('url')]
    keywords = []
    for item in web_items:
        keywords.extend(item.get('tags', []))
    
    return {
        "title": title,
        "description": desc,
        "activity_type": "web_browsing",
        "key_points": titles[:5] if web_items else [],
        "resources": {
            "urls": urls[:10],
            "keywords": list(set(keywords))[:10]
        }
    }
