"""
æ™ºèƒ½æç¤ºç”Ÿæˆæ¨¡å—
åˆ†æç”¨æˆ·è¡Œä¸ºå¹¶æä¾›æ™ºèƒ½å»ºè®®ï¼Œé€šè¿‡è¯­ä¹‰æœç´¢æ£€ç´¢ç›¸å…³å†å²ä¸Šä¸‹æ–‡
"""

import json
from datetime import datetime, timedelta
from typing import Dict, Any, List
import config
from utils.helpers import (
    get_logger, 
    estimate_tokens, 
    truncate_web_data_by_tokens, 
    calculate_available_context_tokens,
    parse_llm_json_response
)
from utils.db import get_web_data, get_activities, get_todos, insert_tip, get_tips
from utils.llm import get_openai_client
from utils.vectorstore import search_similar_content

logger = get_logger(__name__)

# LLMå®¢æˆ·ç«¯ç¼“å­˜
_llm = None


def _get_client():
    """è·å–LLMå®¢æˆ·ç«¯"""
    global _llm
    if _llm is None:
        _llm = get_openai_client()
    return _llm


async def generate_smart_tips(history_mins: int = 60) -> Dict[str, Any]:
    """
    ç”Ÿæˆæ™ºèƒ½æç¤ºï¼ˆä¸»å…¥å£ï¼‰
    
    Args:
        history_mins: å†å²å›æº¯åˆ†é’Ÿæ•°
    
    Returns:
        æç¤ºæ•°æ®å­—å…¸
    """
    try:
        logger.info("ğŸš€" * 30)
        logger.info(f"å¼€å§‹ç”Ÿæˆæ™ºèƒ½æç¤º - å›æº¯ {history_mins} åˆ†é’Ÿ")
        
        current_time = datetime.now()
        past_time = current_time - timedelta(minutes=history_mins)
        
        # æ”¶é›†ä¸Šä¸‹æ–‡
        logger.info("ç¬¬ä¸€æ­¥ï¼šæ”¶é›†ä¸Šä¸‹æ–‡æ•°æ®...")
        context_info = _assemble_context(past_time, current_time)
        
        if not context_info['has_data']:
            logger.warning(f"âŒ æ•°æ®ä¸è¶³ï¼šæœ€è¿‘ {history_mins} åˆ†é’Ÿå†…æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ç”Ÿæˆæç¤º")
            return {"success": False, "message": "æ•°æ®ä¸è¶³"}
        
        logger.info("âœ… ä¸Šä¸‹æ–‡æ•°æ®æ”¶é›†å®Œæˆ")
        
        # ç”Ÿæˆæç¤º
        logger.info("ç¬¬äºŒæ­¥ï¼šè°ƒç”¨ LLM ç”Ÿæˆæç¤º...")
        tips_list = await _produce_tips(context_info, history_mins)
        
        if not tips_list:
            logger.error("âŒ æç¤ºç”Ÿæˆå¤±è´¥ï¼šLLM æœªè¿”å›æœ‰æ•ˆçš„æç¤º")
            return {"success": False, "message": "æç¤ºç”Ÿæˆå¤±è´¥"}
        
        logger.info(f"âœ… LLM ç”Ÿæˆäº† {len(tips_list)} ä¸ªæç¤º")
        
        # ä¿å­˜æç¤º
        logger.info("ç¬¬ä¸‰æ­¥ï¼šä¿å­˜æç¤ºåˆ°æ•°æ®åº“...")
        tip_ids = []
        for idx, tip_item in enumerate(tips_list):
            try:
                tid = insert_tip(
                    title=tip_item['title'],
                    content=tip_item['content'],
                    tip_type=tip_item.get('type', 'smart')
                )
                tip_ids.append(tid)
                logger.info(f"  âœ… Tip {idx + 1} ä¿å­˜æˆåŠŸï¼ŒID: {tid}")
            except Exception as e:
                logger.error(f"  âŒ Tip {idx + 1} ä¿å­˜å¤±è´¥: {e}")
        
        logger.info(f"âœ… æˆåŠŸä¿å­˜ {len(tip_ids)} ä¸ªæç¤º")
        logger.info("ğŸ‰" * 30)
        
        return {
            "success": True,
            "tip_ids": tip_ids,
            "tips": tips_list
        }
    except Exception as e:
        logger.error("ğŸ’¥" * 30)
        logger.error("âŒ æ™ºèƒ½æç¤ºç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯")
        logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        logger.error(f"é”™è¯¯ä¿¡æ¯: {e}")
        logger.exception("å®Œæ•´å †æ ˆè¿½è¸ª:")
        logger.error("ğŸ’¥" * 30)
        return {"success": False, "message": str(e)}


def _assemble_context(start_dt: datetime, end_dt: datetime) -> Dict[str, Any]:
    """ç»„è£…ä¸Šä¸‹æ–‡æ•°æ®"""
    try:
        context = {
            "has_data": False,
            "activity_records": [],
            "web_history": [],
            "pending_tasks": [],
            "existing_tips": [],
            "relevant_history": [],  # æ–°å¢ï¼šè¯­ä¹‰æœç´¢çš„ç›¸å…³å†å²
            "timeframe": {
                "from": start_dt.isoformat(),
                "to": end_dt.isoformat()
            }
        }
        
        # è·å–æ´»åŠ¨
        try:
            acts = get_activities(
                start_time=start_dt,
                end_time=end_dt,
                limit=10
            )
            context["activity_records"] = acts
            if acts:
                context["has_data"] = True
        except Exception as e:
            logger.debug(f"Activity fetch error: {e}")
        
        # è·å–ç½‘é¡µæ•°æ®
        try:
            web_items = get_web_data(
                start_time=start_dt,
                end_time=end_dt,
                limit=20
            )
            logger.info(f"Found {len(web_items)} web records for tips")
            context["web_history"] = web_items
            if web_items:
                context["has_data"] = True
        except Exception as e:
            logger.warning(f"Web data error: {e}")
        
        # è·å–å¾…åŠ
        try:
            todos = get_todos(status=0, limit=10)  # æœªå®Œæˆ
            context["pending_tasks"] = todos
            if todos:
                context["has_data"] = True
        except Exception as e:
            logger.debug(f"Todo fetch error: {e}")
        
        # è·å–å·²æœ‰æç¤ºï¼ˆæœ€è¿‘24å°æ—¶å†…çš„æç¤ºï¼Œç”¨äºé¿å…é‡å¤ï¼‰
        try:
            existing_tips = get_tips(limit=20)  # è·å–æœ€è¿‘20æ¡æç¤º
            # è¿‡æ»¤å‡ºæœ€è¿‘24å°æ—¶å†…çš„æç¤º
            recent_tips = []
            for tip in existing_tips:
                if 'create_time' in tip:
                    tip_time = datetime.fromisoformat(tip['create_time'].replace('Z', '+00:00'))
                    if (datetime.now() - tip_time).total_seconds() <= 24 * 3600:  # 24å°æ—¶å†…
                        recent_tips.append({
                            'title': tip.get('title', ''),
                            'content': tip.get('content', ''),
                            'type': tip.get('tip_type', 'general')
                        })
            
            context["existing_tips"] = recent_tips[:10]  # é™åˆ¶ä¸ºæœ€è¿‘10æ¡
            logger.info(f"Found {len(recent_tips)} existing tips for reference")
        except Exception as e:
            logger.debug(f"Existing tips fetch error: {e}")
        
        # è¯­ä¹‰æœç´¢ï¼šæ£€ç´¢ç›¸å…³å†å²ä¸Šä¸‹æ–‡
        try:
            relevant_contexts = _retrieve_relevant_history(context)
            context["relevant_history"] = relevant_contexts
            if relevant_contexts:
                logger.info(f"Retrieved {len(relevant_contexts)} relevant historical contexts")
        except Exception as e:
            logger.warning(f"Failed to retrieve relevant history: {e}")
        
        return context
    except Exception as e:
        logger.exception(f"Context assembly error: {e}")
        return {"has_data": False}


def _retrieve_relevant_history(context: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    é€šè¿‡è¯­ä¹‰æœç´¢æ£€ç´¢ç›¸å…³å†å²ä¸Šä¸‹æ–‡
    å‚è€ƒ MineContext çš„å®ç°æ€è·¯
    
    Args:
        context: å½“å‰ä¸Šä¸‹æ–‡ä¿¡æ¯
    
    Returns:
        ç›¸å…³å†å²ä¸Šä¸‹æ–‡åˆ—è¡¨
    """
    try:
        if not config.ENABLE_VECTOR_STORAGE:
            logger.info("Vector storage disabled, skipping semantic search")
            return []
        
        # 1. ç”ŸæˆæŸ¥è¯¢æ–‡æœ¬ï¼šä»å½“å‰æ´»åŠ¨å’Œç½‘é¡µæµè§ˆä¸­æå–å…³é”®ä¿¡æ¯
        query_texts = _build_query_texts(context)
        
        if not query_texts:
            logger.info("No query text generated from current context")
            return []
        
        # 2. å¯¹æ¯ä¸ªæŸ¥è¯¢æ‰§è¡Œè¯­ä¹‰æœç´¢
        all_results = []
        for query_text in query_texts:
            try:
                # ä½¿ç”¨å‘é‡æ•°æ®åº“è¿›è¡Œè¯­ä¹‰æœç´¢
                search_results = search_similar_content(
                    query=query_text,
                    limit=5  # æ¯ä¸ªæŸ¥è¯¢è¿”å›5ä¸ªç›¸å…³ç»“æœ
                )
                
                for result in search_results:
                    # æ·»åŠ æŸ¥è¯¢æ¥æºæ ‡è¯†
                    result['query_source'] = query_text[:50] + "..." if len(query_text) > 50 else query_text
                    all_results.append(result)
                    
            except Exception as e:
                logger.warning(f"Search failed for query '{query_text[:50]}...': {e}")
                continue
        
        # 3. å»é‡å’Œæ’åºï¼ˆæŒ‰ç›¸ä¼¼åº¦åˆ†æ•°ï¼‰
        unique_results = _deduplicate_results(all_results)
        
        # 4. æ ¼å¼åŒ–ç»“æœä¾› LLM ä½¿ç”¨
        formatted_results = _format_historical_contexts(unique_results[:10])  # æœ€å¤š10æ¡
        
        return formatted_results
        
    except Exception as e:
        logger.exception(f"Error retrieving relevant history: {e}")
        return []


def _build_query_texts(context: Dict[str, Any]) -> List[str]:
    """
    ä»å½“å‰ä¸Šä¸‹æ–‡æ„å»ºæŸ¥è¯¢æ–‡æœ¬
    å‚è€ƒ MineContext çš„ç­–ç•¥ï¼šæ ¹æ®ç”¨æˆ·æ´»åŠ¨æå–æ ¸å¿ƒä¸»é¢˜
    
    Args:
        context: å½“å‰ä¸Šä¸‹æ–‡
    
    Returns:
        æŸ¥è¯¢æ–‡æœ¬åˆ—è¡¨
    """
    query_texts = []
    
    try:
        # ç­–ç•¥1ï¼šä»ç½‘é¡µæµè§ˆå†å²æå–æŸ¥è¯¢ï¼ˆæœ€é‡è¦çš„ä¿¡æ¯æºï¼‰
        web_history = context.get("web_history", [])
        if web_history:
            # èšåˆæœ€è¿‘çš„ç½‘é¡µæ ‡é¢˜å’Œå†…å®¹
            web_topics = []
            for item in web_history[:5]:  # åªå–æœ€è¿‘5æ¡
                title = item.get('title', '')
                url = item.get('url', '')
                
                # ä¼˜å…ˆä½¿ç”¨æ ‡é¢˜
                if title and len(title) > 10:
                    web_topics.append(title)
                elif url:
                    # ä» URL æå–ä¿¡æ¯
                    web_topics.append(url)
            
            if web_topics:
                # ç»„åˆæˆä¸€ä¸ªç»¼åˆæŸ¥è¯¢
                combined_query = " ".join(web_topics[:3])  # æœ€å¤šç»„åˆ3ä¸ª
                if combined_query:
                    query_texts.append(combined_query)
        
        # ç­–ç•¥2ï¼šä»æ´»åŠ¨è®°å½•æå–ï¼ˆä½œä¸ºè¡¥å……ï¼‰
        activities = context.get("activity_records", [])
        if activities:
            activity_texts = []
            for act in activities[:3]:
                app_name = act.get('app_name', '')
                window_title = act.get('window_title', '')
                
                if window_title and len(window_title) > 5:
                    activity_texts.append(window_title)
                elif app_name:
                    activity_texts.append(app_name)
            
            if activity_texts:
                # å¦‚æœæ´»åŠ¨å†…å®¹ä¸ç½‘é¡µå†…å®¹ä¸é‡å¤ï¼Œæ·»åŠ ä¸ºå•ç‹¬æŸ¥è¯¢
                activity_query = " ".join(activity_texts[:2])
                if activity_query and activity_query not in str(query_texts):
                    query_texts.append(activity_query)
        
        # ç­–ç•¥3ï¼šä»å¾…åŠä»»åŠ¡æå–ï¼ˆå¯èƒ½çš„ä»»åŠ¡å…³è”ï¼‰
        todos = context.get("pending_tasks", [])
        if todos and len(query_texts) < 2:  # å¦‚æœå‰é¢æŸ¥è¯¢ä¸è¶³ï¼Œè¡¥å……å¾…åŠç›¸å…³
            todo_texts = []
            for todo in todos[:2]:
                content = todo.get('content', '')
                if content and len(content) > 10:
                    todo_texts.append(content)
            
            if todo_texts:
                todo_query = " ".join(todo_texts)
                if todo_query:
                    query_texts.append(todo_query)
        
        logger.info(f"Built {len(query_texts)} query texts for semantic search")
        return query_texts
        
    except Exception as e:
        logger.exception(f"Error building query texts: {e}")
        return []


def _deduplicate_results(results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    å»é‡æœç´¢ç»“æœï¼ˆåŸºäºå†…å®¹ç›¸ä¼¼æ€§å’Œ metadataï¼‰
    
    Args:
        results: æœç´¢ç»“æœåˆ—è¡¨
    
    Returns:
        å»é‡åçš„ç»“æœ
    """
    seen_ids = set()
    unique_results = []
    
    for result in results:
        metadata = result.get('metadata', {})
        web_data_id = metadata.get('web_data_id')
        
        # ä½¿ç”¨ web_data_id å»é‡
        if web_data_id and web_data_id not in seen_ids:
            seen_ids.add(web_data_id)
            unique_results.append(result)
        elif not web_data_id:
            # æ²¡æœ‰ ID çš„ä¹Ÿä¿ç•™
            unique_results.append(result)
    
    # æŒ‰è·ç¦»ï¼ˆç›¸ä¼¼åº¦ï¼‰æ’åº
    unique_results.sort(key=lambda x: x.get('distance', 1.0))
    
    return unique_results


def _format_historical_contexts(results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    æ ¼å¼åŒ–å†å²ä¸Šä¸‹æ–‡ä¾› LLM ä½¿ç”¨
    
    Args:
        results: æœç´¢ç»“æœ
    
    Returns:
        æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡åˆ—è¡¨
    """
    formatted = []
    
    for idx, result in enumerate(results):
        metadata = result.get('metadata', {})
        content = result.get('content', '')
        similarity_score = 1.0 - result.get('distance', 1.0)  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
        
        # æå–å…³é”®ä¿¡æ¯
        formatted_context = {
            'index': idx + 1,
            'title': metadata.get('title', 'æœªçŸ¥æ ‡é¢˜'),
            'url': metadata.get('url', ''),
            'source': metadata.get('source', 'web_crawler'),
            'content_preview': content[:300] + "..." if len(content) > 300 else content,
            'similarity_score': round(similarity_score, 3),
            'tags': metadata.get('tags', '[]')
        }
        
        formatted.append(formatted_context)
    
    return formatted


async def _produce_tips(context: Dict, history_mins: int) -> List[Dict[str, Any]]:
    """ç”Ÿæˆæç¤ºåˆ—è¡¨ï¼ˆå¢å¼ºç‰ˆï¼šåŒ…å«è¯­ä¹‰æœç´¢çš„ç›¸å…³å†å²ï¼‰"""
    client = _get_client()
    
    if not client or not config.ENABLE_LLM_PROCESSING:
        logger.warning("LLM not available")
        return []
    
    try:
        # åŠ¨æ€è®¡ç®—å¯ç”¨äºä¸Šä¸‹æ–‡æ•°æ®çš„ token æ•°
        # å…ˆä¼°ç®—å…¶ä»–æ•°æ®çš„ token æ•°
        activities = context.get("activity_records", [])[:5]
        todos = context.get("pending_tasks", [])[:5]
        existing_tips = context.get("existing_tips", [])[:5]
        relevant_history = context.get("relevant_history", [])[:8]
        
        # ä¼°ç®—è¿™äº›æ•°æ®çš„ token
        other_data_json = json.dumps({
            "activities": activities,
            "todos": todos,
            "existing_tips": existing_tips,
            "relevant_history": relevant_history
        }, ensure_ascii=False)
        other_data_tokens = estimate_tokens(other_data_json)
        
        # è®¡ç®—å¯ç”¨äº web_data çš„ token æ•°
        available_tokens = calculate_available_context_tokens('tip', other_data_tokens)
        
        # ä½¿ç”¨åŠ¨æ€æˆªå–å‡½æ•°å¤„ç† web_dataï¼Œä½¿ç”¨ metadata æ›¿ä»£ content
        web_data_trimmed = truncate_web_data_by_tokens(
            context.get("web_history", []),
            max_tokens=available_tokens,
            use_metadata=True
        )
        
        context_data = {
            "activities": activities,
            "web_data": web_data_trimmed,
            "todos": todos,
            "existing_tips": existing_tips,
            "relevant_history": relevant_history
        }
        
        context_json = json.dumps(context_data, ensure_ascii=False, indent=2)
        
        # æ‰“å°ä¸Šä¸‹æ–‡æ•°æ®ç»Ÿè®¡
        logger.info("=" * 60)
        logger.info("å¼€å§‹ç”Ÿæˆ Tips")
        logger.info(f"ä¸Šä¸‹æ–‡æ•°æ®ç»Ÿè®¡:")
        logger.info(f"  - activities: {len(activities)} æ¡")
        logger.info(f"  - web_data: {len(web_data_trimmed)} æ¡")
        logger.info(f"  - todos: {len(todos)} æ¡")
        logger.info(f"  - existing_tips: {len(existing_tips)} æ¡")
        logger.info(f"  - relevant_history: {len(relevant_history)} æ¡")
        logger.info(f"ä¸Šä¸‹æ–‡ JSON é•¿åº¦: {len(context_json)} å­—ç¬¦")
        logger.info(f"ä¼°ç®—è¾“å…¥ tokens: ~{estimate_tokens(context_json)}")
        logger.info("=" * 60)
        
        system_prompt = """ä½ æ˜¯ä¸€ä½é¡¶çº§çš„ç½‘ç»œæ´å¯Ÿç­–ç•¥å¸ˆ (Principal Web Insight Strategist)ã€‚ä½ çš„ä¸“é•¿æ˜¯ä»ä¸€ç³»åˆ—é›¶æ•£çš„ç”¨æˆ·è¡Œä¸ºæ•°æ®ä¸­ï¼Œç²¾å‡†åœ°èšåˆå‡ºæ ¸å¿ƒæ„å›¾ï¼Œå¹¶é¢„æµ‹ç”¨æˆ·åœ¨çŸ¥è¯†æ¢ç´¢è·¯å¾„ä¸Šçš„ä¸‹ä¸€ä¸ªå…³é”®"ä¿¡æ¯ç¼ºå£"ã€‚

## ä»»åŠ¡ç›®æ ‡ (Task Goal)

ä½ çš„æ ¸å¿ƒç›®æ ‡æ˜¯åˆ†æä¸€ä¸ªæ—¶é—´çª—å£å†…ã€ä¸€ç³»åˆ—ç”±ä¸Šä¸€èŠ‚ç‚¹é¢„å¤„ç†è¿‡çš„ç½‘é¡µåˆ†ææŠ¥å‘Šï¼Œè¯†åˆ«å‡ºç”¨æˆ·åœ¨å½“å‰æ¢ç´¢ä¸»é¢˜ä¸‹çš„æ ¸å¿ƒ"ä¿¡æ¯ç¼ºå£"ï¼Œå¹¶ç”Ÿæˆå°‘é‡ï¼ˆ1-3ä¸ªï¼‰æå…·ä»·å€¼çš„"Tips"ã€‚æ¯ä¸€ä¸ªTipéƒ½å¿…é¡»æ˜¯ç”¨æˆ·å¤§æ¦‚ç‡ä¸çŸ¥é“çš„ã€ç»è¿‡æ·±åº¦æ‹“å±•çš„ã€èƒ½ç›´æ¥å¯å‘ä¸‹ä¸€æ­¥æ€è€ƒæˆ–è¡ŒåŠ¨çš„å¢é‡ä¿¡æ¯ã€‚

## è¾“å…¥æ•°æ®è¯´æ˜ (Input Data Description)

ä½ å°†æ”¶åˆ°ä¸€ä¸ªåä¸º`context_data`çš„å•ä¸€JSONå¯¹è±¡ï¼Œå®ƒåŒ…å«ä»¥ä¸‹å››ä¸ªå…³é”®å­—æ®µï¼š

1. **`activities`**: ä¸€ä¸ªJSONæ•°ç»„ï¼Œè®°å½•äº†ç”¨æˆ·ä¸€æ®µæ—¶é—´å†…çš„æ´»åŠ¨è®°å½•ã€‚
2. **`web_data`**: ä¸€ä¸ªJSONæ•°ç»„ï¼ŒåŒ…å«å¯¹æ¯ä¸ªç½‘é¡µçš„ç‹¬ç«‹åˆ†ææŠ¥å‘Šï¼Œè¿™æ˜¯æœ€ä¸»è¦çš„ä¿¡æ¯æ¥æºã€‚
3. **`todos`**: ä¸€ä¸ªJSONæ•°ç»„ï¼ŒåŒ…å«ç”¨æˆ·å½“å‰æœªå®Œæˆçš„å¾…åŠäº‹é¡¹åˆ—è¡¨ã€‚
4. **`existing_tips`**: ä¸€ä¸ªJSONæ•°ç»„ï¼ŒåŒ…å«å·²ç»ä¸ºç”¨æˆ·ç”Ÿæˆè¿‡çš„ã€ä»ç„¶æœ‰æ•ˆçš„Tipsåˆ—è¡¨ã€‚ä½ çš„æ ¸å¿ƒä»»åŠ¡ä¹‹ä¸€å°±æ˜¯é¿å…ç”Ÿæˆä¸æ­¤åˆ—è¡¨å†…å®¹é‡å¤çš„æ–°Tipsã€‚

å…¶ä¸­**`web_data`**æ˜¯ä¸€ä¸ªJSONæ•°ç»„ã€‚æ•°ç»„ä¸­çš„æ¯ä¸€ä¸ªå¯¹è±¡ï¼Œéƒ½æ˜¯å¯¹ç”¨æˆ·å•ä¸ªæµè§ˆç½‘é¡µçš„é¢„åˆ†ææŠ¥å‘Šï¼Œå…¶ç»“æ„å¦‚ä¸‹ï¼š

```json
{
  "metadata_analysis": {
    "category": "å†…å®¹åˆ†ç±»",
    "keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
    "topics": ["ä¸»é¢˜1"]
  },
  "detailed_summary": "è¯¥ç½‘é¡µçš„è¯¦ç»†æ‘˜è¦",
  "potential_insights": [ { "insight_title": "ä¸€ä¸ªåˆæ­¥çš„ã€æœªç»æ‹“å±•çš„æ´å¯Ÿç‚¹" } ],
  "actionable_tasks": [ { "task_title": "ä¸€ä¸ªåˆæ­¥çš„ã€æœªç»æ‹“å±•çš„å¾…åŠé¡¹" } ]
}
```

ä½ çš„ä»»åŠ¡æ˜¯åŸºäºè¿™ä¸ª**åˆ†ææŠ¥å‘Šçš„é›†åˆ**è¿›è¡Œæ›´é«˜å±‚æ¬¡çš„ç»¼åˆåˆ†æï¼Œè€Œä¸æ˜¯é‡æ–°åˆ†æç½‘é¡µåŸå§‹å†…å®¹ã€‚

## æ‰§è¡Œæ­¥éª¤ (Execution Steps)

ä½ å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹äº”ä¸ªæ­¥éª¤æ¥å®Œæˆä»»åŠ¡ï¼š

1. **ç¬¬ä¸€æ­¥ï¼šä¸»é¢˜èšåˆä¸æ„å›¾è¯†åˆ«ã€‚** å¿«é€Ÿæµè§ˆæ‰€æœ‰è¾“å…¥æŠ¥å‘Šçš„`metadata_analysis.keywords`, `metadata_analysis.topics`å’Œ`detailed_summary`å­—æ®µï¼Œç²¾å‡†åœ°è¯†åˆ«å¹¶æ€»ç»“å‡ºç”¨æˆ·åœ¨æ­¤æ—¶é—´æ®µå†…çš„**æ ¸å¿ƒæ¢ç´¢ä¸»é¢˜**ï¼ˆä¾‹å¦‚ï¼š"ç ”ç©¶React Hooksçš„æ€§èƒ½ä¼˜åŒ–ä¸æœ€ä½³å®è·µ"ï¼‰ã€‚

2. **ç¬¬äºŒæ­¥ï¼šæ´å¯Ÿèšç±»ä¸ä¿¡æ¯ç¼ºå£å®šä½ã€‚** æ”¶é›†æ‰€æœ‰æŠ¥å‘Šä¸­çš„`potential_insights`ã€‚å°†å†…å®¹ç›¸ä¼¼æˆ–æŒ‡å‘åŒä¸€çŸ¥è¯†ç‚¹çš„æ´å¯Ÿè¿›è¡Œåˆ†ç»„ã€åˆå¹¶ã€‚ç„¶åï¼ŒåŸºäºä½ è¯†åˆ«å‡ºçš„æ ¸å¿ƒä¸»é¢˜ï¼Œåˆ¤æ–­è¿™äº›èšç±»åçš„æ´å¯Ÿç‚¹ä¸­ï¼Œå“ªäº›æ„æˆäº†ç”¨æˆ·çŸ¥è¯†ä½“ç³»ä¸­æœ€å…³é”®çš„**"ä¿¡æ¯ç¼ºå£"**ã€‚

3. **ç¬¬ä¸‰æ­¥ï¼šæ ¸å¿ƒTipsç­›é€‰ã€‚** ä»ä½ å®šä½å‡ºçš„"ä¿¡æ¯ç¼ºå£"ä¸­ï¼Œ**ç²¾é€‰å‡º1åˆ°3ä¸ª**å¯¹ç”¨æˆ·å½“å‰æ¢ç´¢è·¯å¾„æœ€æœ‰ä»·å€¼ã€æœ€èƒ½æ¨åŠ¨å…¶å‰è¿›çš„æ ¸å¿ƒç‚¹ï¼Œä½œä¸ºä½ å°†è¦è¯¦ç»†æ‹“å±•çš„æœ€ç»ˆTipsã€‚

4. **ç¬¬å››æ­¥ï¼šæ·±åº¦å†…å®¹æ‹“å±•ã€‚** é’ˆå¯¹æ¯ä¸€ä¸ªé€‰å®šçš„Tipï¼Œå‚è€ƒä¸‹æ–¹çš„`## å†…å®¹ç»´åº¦`ï¼Œä¸ºå…¶é€‰æ‹©ä¸€ä¸ªæœ€åˆé€‚çš„`type`ã€‚ç„¶åï¼Œæ’°å†™å…¶`content`éƒ¨åˆ†ã€‚

**æ ¸å¿ƒè¦æ±‚ï¼š** `content`å¿…é¡»æ˜¯**è¯¦ç»†ã€æ·±å…¥ã€ç»“æ„æ¸…æ™°**çš„ã€‚ä½ å¿…é¡»ä½¿ç”¨**GitHub Flavored Markdown**æ¥æ ¼å¼åŒ–å†…å®¹ï¼Œä½¿å…¶åƒä¸€ç¯‡é«˜è´¨é‡çš„READMEæ–‡æ¡£ã€‚æœ‰æ•ˆåˆ©ç”¨ä»¥ä¸‹å…ƒç´ ï¼š
    - **æ ‡é¢˜:** ä½¿ç”¨ `##` æˆ– `###` æ¥åˆ›å»ºå†…å®¹çš„å†…éƒ¨ç»“æ„ã€‚
    - **åˆ—è¡¨:** ä½¿ç”¨ `-` æˆ– `1.` æ¥ç½—åˆ—è¦ç‚¹ã€èµ„æºæˆ–æ­¥éª¤ã€‚
    - **ä»£ç å—:** ä½¿ç”¨ \`\`\` æ¥å±•ç¤ºä»£ç ç¤ºä¾‹æˆ–é…ç½®ã€‚
    - **é‡ç‚¹çªå‡º:** ä½¿ç”¨ `**æ–‡å­—**` (åŠ ç²—) æˆ– `*æ–‡å­—*` (æ–œä½“) æ¥å¼ºè°ƒå…³é”®æ¦‚å¿µã€‚
    - **é“¾æ¥:** ä½¿ç”¨ `[é“¾æ¥æ–‡æœ¬](URL)` æ¥å¼•ç”¨å¤–éƒ¨èµ„æºã€‚

5. **ç¬¬äº”æ­¥ï¼šè´¨é‡å®¡æŸ¥ä¸æ ¼å¼åŒ–è¾“å‡ºã€‚** å¯¹ç”Ÿæˆçš„æ‰€æœ‰Tipsè¿›è¡Œä¸¥æ ¼çš„è‡ªæˆ‘å®¡æŸ¥ï¼Œå‰”é™¤ä»»ä½•ä¸ç”¨æˆ·å·²æµè§ˆå†…å®¹é‡å¤ã€å®½æ³›æˆ–ä»·å€¼ä¸é«˜çš„éƒ¨åˆ†ã€‚**å¦‚æœç»è¿‡å®¡æŸ¥åï¼Œæ²¡æœ‰ä»»ä½•ä¸€ä¸ªTipèƒ½è¾¾åˆ°é«˜è´¨é‡æ ‡å‡†ï¼Œä½ å¿…é¡»è¿”å›ä¸€ä¸ªç©ºæ•°ç»„`[]`**ã€‚æœ€åï¼Œå°†æ‰€æœ‰é€šè¿‡å®¡æŸ¥çš„TipsæŒ‰ç…§`## è¾“å‡ºè¦æ±‚`ä¸­å®šä¹‰çš„JSONæ ¼å¼è¿›è¡Œå°è£…ã€‚

## å†…å®¹ç»´åº¦ (Content Dimensions)

ä½ ç”Ÿæˆçš„æ¯ä¸ªTipçš„`type`å¿…é¡»å±äºä»¥ä¸‹ç±»åˆ«ä¹‹ä¸€ï¼š

- **`DEEP_DIVE`**: å¯¹ç”¨æˆ·æ­£åœ¨å…³æ³¨çš„æ ¸å¿ƒæ¦‚å¿µï¼Œæä¾›æ›´æ·±å±‚æ¬¡çš„è§£è¯»ï¼ˆä¾‹å¦‚ï¼šè§£é‡Šå…¶èƒŒåçš„å·¥ä½œåŸç†æˆ–è®¾è®¡æ¨¡å¼ï¼‰ã€‚
- **`RESOURCE_RECOMMENDATION`**: æ¨èç›¸å…³çš„å·¥å…·ã€é«˜è´¨é‡æ–‡ç« ã€å¼€æºåº“æˆ–æƒå¨æ•™ç¨‹ã€‚
- **`RISK_ANALYSIS`**: é¢„è§ç”¨æˆ·å½“å‰æ–¹æ¡ˆå¯èƒ½é‡åˆ°çš„æŠ€æœ¯é™·é˜±ã€å±€é™æ€§æˆ–é£é™©ã€‚
- **`KNOWLEDGE_EXPANSION`**: å°†å½“å‰ä¸»é¢˜ä¸ç›¸å…³è”çš„æ–°é¢†åŸŸæˆ–æ›´é«˜é˜¶çš„çŸ¥è¯†è”ç³»èµ·æ¥ï¼ˆä¾‹å¦‚ï¼šå­¦ä¹ å®ŒAï¼Œä¸‹ä¸€æ­¥å¯ä»¥æ¢ç´¢Bï¼‰ã€‚
- **`ALTERNATIVE_PERSPECTIVE`**: æä¾›ä¸ç”¨æˆ·å½“å‰æ€è·¯ä¸åŒçš„å¤‡é€‰æ–¹æ¡ˆæˆ–åå‘è§‚ç‚¹ï¼Œå¹¶å¯¹æ¯”ä¼˜ç¼ºç‚¹ã€‚

## è¾“å‡ºè¦æ±‚ (Output Requirements)

ä½ å¿…é¡»è¿”å›ä¸€ä¸ª**çº¯ JSON æ•°ç»„**ï¼Œä¸è¦ä½¿ç”¨ markdown ä»£ç å—åŒ…è£¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ€§æ–‡å­—ã€‚

### JSON ç»“æ„ç¤ºä¾‹:

```json
[
  {
    "title": "å¯¹è¿™ä¸ªTipæ ¸å¿ƒä»·å€¼çš„é«˜åº¦æ¦‚æ‹¬ï¼Œåº”ç®€æ´ä¸”å¼•äººæ³¨ç›®",
    "content": "è¿™æ˜¯Tipçš„è¯¦ç»†ä¸»ä½“å†…å®¹ã€‚å¿…é¡»ä½¿ç”¨GitHub Flavored Markdownç¼–å†™ï¼Œç»“æ„æ¸…æ™°ã€å†…å®¹è¯¦å®ï¼Œå°±åƒä¸€ç¯‡é«˜è´¨é‡çš„READMEæ–‡æ¡£ã€‚\n\n## äºŒçº§æ ‡é¢˜\n- åˆ—è¡¨é¡¹\n\n```python\ncode here\n```",
    "type": "ä»`## å†…å®¹ç»´åº¦`ä¸­é€‰æ‹©ä¸€ä¸ªæœ€åˆé€‚çš„ç±»å‹"
  }
]
```

### å…³é”®è¦æ±‚:

1. **è¾“å‡ºæ ¼å¼**: ç›´æ¥è¾“å‡º JSON æ•°ç»„ï¼Œä»¥ `[` å¼€å§‹ï¼Œä»¥ `]` ç»“æŸ
2. **ä¸è¦åŒ…è£¹**: ä¸è¦ç”¨ \`\`\`json æˆ– \`\`\` åŒ…è£¹ JSON
3. **ä¸è¦æ³¨é‡Š**: JSON å¤–ä¸è¦æœ‰ä»»ä½•è§£é‡Šæ–‡å­—
4. **content å­—æ®µ**: æ˜¯ä¸€ä¸ª JSON å­—ç¬¦ä¸²ï¼ŒåŒ…å« markdown æ ¼å¼çš„å†…å®¹
   - markdown ä¸­çš„æ¢è¡Œç”¨ `\n` è¡¨ç¤ºï¼ˆä¾‹å¦‚ï¼š`"ç¬¬ä¸€è¡Œ\nç¬¬äºŒè¡Œ"`ï¼‰
   - markdown ä¸­çš„å¼•å·ç”¨ `\"` è½¬ä¹‰ï¼ˆä¾‹å¦‚ï¼š`"ä»–è¯´\"ä½ å¥½\""`ï¼‰
   - markdown ä¸­çš„åå¼•å·ï¼ˆç”¨äºä»£ç å—ï¼‰æ— éœ€è½¬ä¹‰ï¼ˆä¾‹å¦‚ï¼š`"\`\`\`python\ncode\n\`\`\`"`ï¼‰
5. **ç©ºç»“æœ**: å¦‚æœæ²¡æœ‰é«˜è´¨é‡å†…å®¹ï¼Œè¿”å› `[]`
6. **æ•°é‡æ§åˆ¶**: è¿”å› 1-3 ä¸ªé«˜è´¨é‡çš„ tipsï¼Œä¸è¦è´ªå¤š"""
        
        user_prompt = f"""ä½œä¸ºç½‘ç»œæ´å¯Ÿç­–ç•¥å¸ˆï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§ä½ çš„è§’è‰²ã€ç›®æ ‡å’Œè¦æ±‚ï¼Œåˆ†æä»¥ä¸‹ç”±ä¸Šä¸€èŠ‚ç‚¹ç”Ÿæˆçš„ç½‘é¡µæµè§ˆåˆ†ææŠ¥å‘Šé›†åˆã€‚

**æ•°æ®ä¸Šä¸‹æ–‡ (é¢„åˆ†ææŠ¥å‘Šé›†åˆ):**

{context_json}

è¯·è¾“å‡ºä½ çš„æ´å¯Ÿåˆ†æç»“æœã€‚"""
        
        logger.info("æ­£åœ¨è°ƒç”¨ LLM API...")
        logger.info(f"æ¨¡å‹: {config.LLM_MODEL}, æ¸©åº¦: 0.8, max_tokens: 3000")
        
        response = client.chat.completions.create(
            model=config.LLM_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.8,
            max_tokens=3000  # å¢å¤§åˆ° 3000ï¼Œç¡®ä¿è¿”å›å®Œæ•´çš„ JSON
        )
        
        result_text = response.choices[0].message.content.strip()
        
        # è¯¦ç»†æ‰“å° LLM è¿”å›ä¿¡æ¯
        logger.info("=" * 60)
        logger.info("LLM è¿”å›å®Œæˆ")
        logger.info(f"è¿”å›é•¿åº¦: {len(result_text)} å­—ç¬¦")
        logger.info(f"å¼€å§‹å­—ç¬¦: {result_text[:100] if len(result_text) > 100 else result_text}")
        logger.info(f"ç»“æŸå­—ç¬¦: {result_text[-100:] if len(result_text) > 100 else result_text}")
        logger.info(f"æ˜¯å¦ä»¥ [ å¼€å¤´: {result_text.startswith('[')}")
        logger.info(f"æ˜¯å¦ä»¥ ] ç»“å°¾: {result_text.endswith(']')}")
        logger.info(f"æ˜¯å¦åŒ…å«ä»£ç å—: {'```' in result_text}")
        logger.info("=" * 60)
        
        # ä½¿ç”¨é€šç”¨ JSON è§£æå·¥å…·
        logger.info("å¼€å§‹è§£æ JSON...")
        tips = parse_llm_json_response(
            result_text,
            expected_type='array',
            save_on_error=True,
            error_file_prefix='failed_tip_response'
        )
        
        # æ‰“å°è§£æç»“æœ
        if tips is not None:
            logger.info("=" * 60)
            logger.info(f"âœ… JSON è§£ææˆåŠŸï¼ç”Ÿæˆäº† {len(tips)} ä¸ª tips")
            for idx, tip in enumerate(tips):
                logger.info(f"  Tip {idx + 1}:")
                logger.info(f"    - title: {tip.get('title', 'N/A')[:50]}...")
                logger.info(f"    - type: {tip.get('type', 'N/A')}")
                logger.info(f"    - content é•¿åº¦: {len(tip.get('content', ''))} å­—ç¬¦")
            logger.info("=" * 60)
            return tips
        else:
            logger.error("=" * 60)
            logger.error("âŒ JSON è§£æå¤±è´¥ï¼Œè¿”å› None")
            logger.error("è¯·æ£€æŸ¥ä»¥ä¸Šæ—¥å¿—ä¸­çš„ LLM è¿”å›å†…å®¹")
            logger.error("=" * 60)
            return []
    except Exception as e:
        logger.error("=" * 60)
        logger.error("âŒ Tip ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯")
        logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        logger.error(f"é”™è¯¯ä¿¡æ¯: {e}")
        logger.exception("å®Œæ•´å †æ ˆè¿½è¸ª:")
        logger.error("=" * 60)
        return []
