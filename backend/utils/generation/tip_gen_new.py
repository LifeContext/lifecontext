"""
æ™ºèƒ½æç¤ºç”Ÿæˆæ¨¡å—
åˆ†æç”¨æˆ·è¡Œä¸ºå¹¶æä¾›æ™ºèƒ½å»ºè®®ï¼Œé€šè¿‡è¯­ä¹‰æœç´¢æ£€ç´¢ç›¸å…³å†å²ä¸Šä¸‹æ–‡
"""

import json
from datetime import datetime, timedelta
from typing import Dict, Any, List
import config
from utils.helpers import (
    get_logger, 
    estimate_tokens, 
    truncate_web_data_by_tokens, 
    calculate_available_context_tokens,
    parse_llm_json_response
)
from utils.db import get_web_data, get_activities, get_todos, insert_tip, get_tips
from utils.llm import get_openai_client
from utils.vectorstore import search_similar_content

logger = get_logger(__name__)

# LLMå®¢æˆ·ç«¯ç¼“å­˜
_llm = None


def _get_client():
    """è·å–LLMå®¢æˆ·ç«¯"""
    global _llm
    if _llm is None:
        _llm = get_openai_client()
    return _llm


async def generate_smart_tips(history_mins: int = 60) -> Dict[str, Any]:
    """
    ç”Ÿæˆæ™ºèƒ½æç¤ºï¼ˆä¸»å…¥å£ï¼‰
    
    Args:
        history_mins: å†å²å›æº¯åˆ†é’Ÿæ•°
    
    Returns:
        æç¤ºæ•°æ®å­—å…¸
    """
    try:
        logger.info("ğŸš€" * 30)
        logger.info(f"å¼€å§‹ç”Ÿæˆæ™ºèƒ½æç¤º - å›æº¯ {history_mins} åˆ†é’Ÿ")
        
        current_time = datetime.now()
        past_time = current_time - timedelta(minutes=history_mins)
        
        # æ”¶é›†ä¸Šä¸‹æ–‡
        logger.info("ç¬¬ä¸€æ­¥ï¼šæ”¶é›†ä¸Šä¸‹æ–‡æ•°æ®...")
        context_info = _assemble_context(past_time, current_time)
        
        if not context_info['has_data']:
            logger.warning(f"âŒ æ•°æ®ä¸è¶³ï¼šæœ€è¿‘ {history_mins} åˆ†é’Ÿå†…æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ç”Ÿæˆæç¤º")
            return {"success": False, "message": "æ•°æ®ä¸è¶³"}
        
        logger.info("âœ… ä¸Šä¸‹æ–‡æ•°æ®æ”¶é›†å®Œæˆ")
        
        # ç”Ÿæˆæç¤º
        logger.info("ç¬¬äºŒæ­¥ï¼šè°ƒç”¨ LLM ç”Ÿæˆæç¤º...")
        tips_list = await _produce_tips(context_info, history_mins)
        
        if not tips_list:
            logger.error("âŒ æç¤ºç”Ÿæˆå¤±è´¥ï¼šLLM æœªè¿”å›æœ‰æ•ˆçš„æç¤º")
            return {"success": False, "message": "æç¤ºç”Ÿæˆå¤±è´¥"}
        
        logger.info(f"âœ… LLM ç”Ÿæˆäº† {len(tips_list)} ä¸ªæç¤º")
        
        # ä¿å­˜æç¤º
        logger.info("ç¬¬ä¸‰æ­¥ï¼šä¿å­˜æç¤ºåˆ°æ•°æ®åº“...")
        tip_ids = []
        for idx, tip_item in enumerate(tips_list):
            try:
                tid = insert_tip(
                    title=tip_item['title'],
                    content=tip_item['content'],
                    tip_type=tip_item.get('type', 'smart')
                )
                tip_ids.append(tid)
                logger.info(f"  âœ… Tip {idx + 1} ä¿å­˜æˆåŠŸï¼ŒID: {tid}")
            except Exception as e:
                logger.error(f"  âŒ Tip {idx + 1} ä¿å­˜å¤±è´¥: {e}")
        
        logger.info(f"âœ… æˆåŠŸä¿å­˜ {len(tip_ids)} ä¸ªæç¤º")
        logger.info("ğŸ‰" * 30)
        
        return {
            "success": True,
            "tip_ids": tip_ids,
            "tips": tips_list
        }
    except Exception as e:
        logger.error("ğŸ’¥" * 30)
        logger.error("âŒ æ™ºèƒ½æç¤ºç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿä¸¥é‡é”™è¯¯")
        logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        logger.error(f"é”™è¯¯ä¿¡æ¯: {e}")
        logger.exception("å®Œæ•´å †æ ˆè¿½è¸ª:")
        logger.error("ğŸ’¥" * 30)
        return {"success": False, "message": str(e)}


def _assemble_context(start_dt: datetime, end_dt: datetime) -> Dict[str, Any]:
    """ç»„è£…ä¸Šä¸‹æ–‡æ•°æ®"""
    try:
        context = {
            "has_data": False,
            "activity_records": [],
            "web_history": [],
            "pending_tasks": [],
            "existing_tips": [],
            "relevant_history": [],  # æ–°å¢ï¼šè¯­ä¹‰æœç´¢çš„ç›¸å…³å†å²
            "timeframe": {
                "from": start_dt.isoformat(),
                "to": end_dt.isoformat()
            }
        }
        
        # è·å–æ´»åŠ¨
        try:
            acts = get_activities(
                start_time=start_dt,
                end_time=end_dt,
                limit=10
            )
            context["activity_records"] = acts
            if acts:
                context["has_data"] = True
        except Exception as e:
            logger.debug(f"Activity fetch error: {e}")
        
        # è·å–ç½‘é¡µæ•°æ®
        try:
            web_items = get_web_data(
                start_time=start_dt,
                end_time=end_dt,
                limit=20
            )
            logger.info(f"Found {len(web_items)} web records for tips")
            context["web_history"] = web_items
            if web_items:
                context["has_data"] = True
        except Exception as e:
            logger.warning(f"Web data error: {e}")
        
        # è·å–å¾…åŠ
        try:
            todos = get_todos(status=0, limit=10)  # æœªå®Œæˆ
            context["pending_tasks"] = todos
            if todos:
                context["has_data"] = True
        except Exception as e:
            logger.debug(f"Todo fetch error: {e}")
        
        # è·å–å·²æœ‰æç¤ºï¼ˆæœ€è¿‘24å°æ—¶å†…çš„æç¤ºï¼Œç”¨äºé¿å…é‡å¤ï¼‰
        try:
            existing_tips = get_tips(limit=20)  # è·å–æœ€è¿‘20æ¡æç¤º
            # è¿‡æ»¤å‡ºæœ€è¿‘24å°æ—¶å†…çš„æç¤º
            recent_tips = []
            for tip in existing_tips:
                if 'create_time' in tip:
                    tip_time = datetime.fromisoformat(tip['create_time'].replace('Z', '+00:00'))
                    if (datetime.now() - tip_time).total_seconds() <= 24 * 3600:  # 24å°æ—¶å†…
                        recent_tips.append({
                            'title': tip.get('title', ''),
                            'content': tip.get('content', ''),
                            'type': tip.get('tip_type', 'general')
                        })
            
            context["existing_tips"] = recent_tips[:10]  # é™åˆ¶ä¸ºæœ€è¿‘10æ¡
            logger.info(f"Found {len(recent_tips)} existing tips for reference")
        except Exception as e:
            logger.debug(f"Existing tips fetch error: {e}")
        
        # è¯­ä¹‰æœç´¢ï¼šæ£€ç´¢ç›¸å…³å†å²ä¸Šä¸‹æ–‡
        try:
            relevant_contexts = _retrieve_relevant_history(context)
            context["relevant_history"] = relevant_contexts
            if relevant_contexts:
                logger.info(f"Retrieved {len(relevant_contexts)} relevant historical contexts")
        except Exception as e:
            logger.warning(f"Failed to retrieve relevant history: {e}")
        
        return context
    except Exception as e:
        logger.exception(f"Context assembly error: {e}")
        return {"has_data": False}


def _retrieve_relevant_history(context: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    é€šè¿‡è¯­ä¹‰æœç´¢æ£€ç´¢ç›¸å…³å†å²ä¸Šä¸‹æ–‡
    å‚è€ƒ MineContext çš„å®ç°æ€è·¯
    
    Args:
        context: å½“å‰ä¸Šä¸‹æ–‡ä¿¡æ¯
    
    Returns:
        ç›¸å…³å†å²ä¸Šä¸‹æ–‡åˆ—è¡¨
    """
    try:
        if not config.ENABLE_VECTOR_STORAGE:
            logger.info("Vector storage disabled, skipping semantic search")
            return []
        
        # 1. ç”ŸæˆæŸ¥è¯¢æ–‡æœ¬ï¼šä»å½“å‰æ´»åŠ¨å’Œç½‘é¡µæµè§ˆä¸­æå–å…³é”®ä¿¡æ¯
        query_texts = _build_query_texts(context)
        
        if not query_texts:
            logger.info("No query text generated from current context")
            return []
        
        # 2. å¯¹æ¯ä¸ªæŸ¥è¯¢æ‰§è¡Œè¯­ä¹‰æœç´¢
        all_results = []
        for query_text in query_texts:
            try:
                # ä½¿ç”¨å‘é‡æ•°æ®åº“è¿›è¡Œè¯­ä¹‰æœç´¢
                search_results = search_similar_content(
                    query=query_text,
                    limit=5  # æ¯ä¸ªæŸ¥è¯¢è¿”å›5ä¸ªç›¸å…³ç»“æœ
                )
                
                for result in search_results:
                    # æ·»åŠ æŸ¥è¯¢æ¥æºæ ‡è¯†
                    result['query_source'] = query_text[:50] + "..." if len(query_text) > 50 else query_text
                    all_results.append(result)
                    
            except Exception as e:
                logger.warning(f"Search failed for query '{query_text[:50]}...': {e}")
                continue
        
        # 3. å»é‡å’Œæ’åºï¼ˆæŒ‰ç›¸ä¼¼åº¦åˆ†æ•°ï¼‰
        unique_results = _deduplicate_results(all_results)
        
        # 4. æ ¼å¼åŒ–ç»“æœä¾› LLM ä½¿ç”¨
        formatted_results = _format_historical_contexts(unique_results[:10])  # æœ€å¤š10æ¡
        
        return formatted_results
        
    except Exception as e:
        logger.exception(f"Error retrieving relevant history: {e}")
        return []


def _build_query_texts(context: Dict[str, Any]) -> List[str]:
    """
    ä»å½“å‰ä¸Šä¸‹æ–‡æ„å»ºæŸ¥è¯¢æ–‡æœ¬
    å‚è€ƒ MineContext çš„ç­–ç•¥ï¼šæ ¹æ®ç”¨æˆ·æ´»åŠ¨æå–æ ¸å¿ƒä¸»é¢˜
    
    Args:
        context: å½“å‰ä¸Šä¸‹æ–‡
    
    Returns:
        æŸ¥è¯¢æ–‡æœ¬åˆ—è¡¨
    """
    query_texts = []
    
    try:
        # ç­–ç•¥1ï¼šä»ç½‘é¡µæµè§ˆå†å²æå–æŸ¥è¯¢ï¼ˆæœ€é‡è¦çš„ä¿¡æ¯æºï¼‰
        web_history = context.get("web_history", [])
        if web_history:
            # èšåˆæœ€è¿‘çš„ç½‘é¡µæ ‡é¢˜å’Œå†…å®¹
            web_topics = []
            for item in web_history[:5]:  # åªå–æœ€è¿‘5æ¡
                title = item.get('title', '')
                url = item.get('url', '')
                
                # ä¼˜å…ˆä½¿ç”¨æ ‡é¢˜
                if title and len(title) > 10:
                    web_topics.append(title)
                elif url:
                    # ä» URL æå–ä¿¡æ¯
                    web_topics.append(url)
            
            if web_topics:
                # ç»„åˆæˆä¸€ä¸ªç»¼åˆæŸ¥è¯¢
                combined_query = " ".join(web_topics[:3])  # æœ€å¤šç»„åˆ3ä¸ª
                if combined_query:
                    query_texts.append(combined_query)
        
        # ç­–ç•¥2ï¼šä»æ´»åŠ¨è®°å½•æå–ï¼ˆä½œä¸ºè¡¥å……ï¼‰
        activities = context.get("activity_records", [])
        if activities:
            activity_texts = []
            for act in activities[:3]:
                app_name = act.get('app_name', '')
                window_title = act.get('window_title', '')
                
                if window_title and len(window_title) > 5:
                    activity_texts.append(window_title)
                elif app_name:
                    activity_texts.append(app_name)
            
            if activity_texts:
                # å¦‚æœæ´»åŠ¨å†…å®¹ä¸ç½‘é¡µå†…å®¹ä¸é‡å¤ï¼Œæ·»åŠ ä¸ºå•ç‹¬æŸ¥è¯¢
                activity_query = " ".join(activity_texts[:2])
                if activity_query and activity_query not in str(query_texts):
                    query_texts.append(activity_query)
        
        # ç­–ç•¥3ï¼šä»å¾…åŠä»»åŠ¡æå–ï¼ˆå¯èƒ½çš„ä»»åŠ¡å…³è”ï¼‰
        todos = context.get("pending_tasks", [])
        if todos and len(query_texts) < 2:  # å¦‚æœå‰é¢æŸ¥è¯¢ä¸è¶³ï¼Œè¡¥å……å¾…åŠç›¸å…³
            todo_texts = []
            for todo in todos[:2]:
                content = todo.get('content', '')
                if content and len(content) > 10:
                    todo_texts.append(content)
            
            if todo_texts:
                todo_query = " ".join(todo_texts)
                if todo_query:
                    query_texts.append(todo_query)
        
        logger.info(f"Built {len(query_texts)} query texts for semantic search")
        return query_texts
        
    except Exception as e:
        logger.exception(f"Error building query texts: {e}")
        return []


def _deduplicate_results(results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    å»é‡æœç´¢ç»“æœï¼ˆåŸºäºå†…å®¹ç›¸ä¼¼æ€§å’Œ metadataï¼‰
    
    Args:
        results: æœç´¢ç»“æœåˆ—è¡¨
    
    Returns:
        å»é‡åçš„ç»“æœ
    """
    seen_ids = set()
    unique_results = []
    
    for result in results:
        metadata = result.get('metadata', {})
        web_data_id = metadata.get('web_data_id')
        
        # ä½¿ç”¨ web_data_id å»é‡
        if web_data_id and web_data_id not in seen_ids:
            seen_ids.add(web_data_id)
            unique_results.append(result)
        elif not web_data_id:
            # æ²¡æœ‰ ID çš„ä¹Ÿä¿ç•™
            unique_results.append(result)
    
    # æŒ‰è·ç¦»ï¼ˆç›¸ä¼¼åº¦ï¼‰æ’åº
    unique_results.sort(key=lambda x: x.get('distance', 1.0))
    
    return unique_results


def _format_historical_contexts(results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    æ ¼å¼åŒ–å†å²ä¸Šä¸‹æ–‡ä¾› LLM ä½¿ç”¨
    
    Args:
        results: æœç´¢ç»“æœ
    
    Returns:
        æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡åˆ—è¡¨
    """
    formatted = []
    
    for idx, result in enumerate(results):
        metadata = result.get('metadata', {})
        content = result.get('content', '')
        similarity_score = 1.0 - result.get('distance', 1.0)  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
        
        # æå–å…³é”®ä¿¡æ¯
        formatted_context = {
            'index': idx + 1,
            'title': metadata.get('title', 'æœªçŸ¥æ ‡é¢˜'),
            'url': metadata.get('url', ''),
            'source': metadata.get('source', 'web_crawler'),
            'content_preview': content[:300] + "..." if len(content) > 300 else content,
            'similarity_score': round(similarity_score, 3),
            'tags': metadata.get('tags', '[]')
        }
        
        formatted.append(formatted_context)
    
    return formatted


async def _produce_tips(context: Dict, history_mins: int) -> List[Dict[str, Any]]:
    """ç”Ÿæˆæç¤ºåˆ—è¡¨ï¼ˆå¢å¼ºç‰ˆï¼šåŒ…å«è¯­ä¹‰æœç´¢çš„ç›¸å…³å†å²ï¼‰"""
    client = _get_client()
    
    if not client or not config.ENABLE_LLM_PROCESSING:
        logger.warning("LLM not available")
        return []
    
    try:
        # åŠ¨æ€è®¡ç®—å¯ç”¨äºä¸Šä¸‹æ–‡æ•°æ®çš„ token æ•°
        # å…ˆä¼°ç®—å…¶ä»–æ•°æ®çš„ token æ•°
        activities = context.get("activity_records", [])[:5]
        todos = context.get("pending_tasks", [])[:5]
        existing_tips = context.get("existing_tips", [])[:5]
        relevant_history = context.get("relevant_history", [])[:8]
        
        # ä¼°ç®—è¿™äº›æ•°æ®çš„ token
        other_data_json = json.dumps({
            "activities": activities,
            "todos": todos,
            "existing_tips": existing_tips,
            "relevant_history": relevant_history
        }, ensure_ascii=False)
        other_data_tokens = estimate_tokens(other_data_json)
        
        # è®¡ç®—å¯ç”¨äº web_data çš„ token æ•°
        available_tokens = calculate_available_context_tokens('tip', other_data_tokens)
        
        # ä½¿ç”¨åŠ¨æ€æˆªå–å‡½æ•°å¤„ç† web_dataï¼Œä½¿ç”¨ metadata æ›¿ä»£ content
        web_data_trimmed = truncate_web_data_by_tokens(
            context.get("web_history", []),
            max_tokens=available_tokens,
            use_metadata=True
        )
        
        context_data = {
            "activities": activities,
            "web_data": web_data_trimmed,
            "todos": todos,
            "existing_tips": existing_tips,
            "relevant_history": relevant_history
        }
        
        context_json = json.dumps(context_data, ensure_ascii=False, indent=2)
        
        # æ‰“å°ä¸Šä¸‹æ–‡æ•°æ®ç»Ÿè®¡
        logger.info("=" * 60)
        logger.info("å¼€å§‹ç”Ÿæˆ Tips")
        logger.info(f"ä¸Šä¸‹æ–‡æ•°æ®ç»Ÿè®¡:")
        logger.info(f"  - activities: {len(activities)} æ¡")
        logger.info(f"  - web_data: {len(web_data_trimmed)} æ¡")
        logger.info(f"  - todos: {len(todos)} æ¡")
        logger.info(f"  - existing_tips: {len(existing_tips)} æ¡")
        logger.info(f"  - relevant_history: {len(relevant_history)} æ¡")
        logger.info(f"ä¸Šä¸‹æ–‡ JSON é•¿åº¦: {len(context_json)} å­—ç¬¦")
        logger.info(f"ä¼°ç®—è¾“å…¥ tokens: ~{estimate_tokens(context_json)}")
        logger.info("=" * 60)
        
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æç¤ºç”ŸæˆåŠ©æ‰‹ï¼Œä»»åŠ¡æ˜¯ç”Ÿæˆ1-3ä¸ªæœ‰ä»·å€¼çš„Tipsï¼ˆå»ºè®®ï¼‰ã€‚

## æ ¸å¿ƒä»»åŠ¡
åˆ†æç”¨æˆ·çš„ç½‘é¡µæµè§ˆè®°å½•ã€æ´»åŠ¨è®°å½•å’Œå¾…åŠäº‹é¡¹ï¼Œè¯†åˆ«ç”¨æˆ·çš„å…´è¶£ä¸»é¢˜å’ŒçŸ¥è¯†ç¼ºå£ï¼Œç”Ÿæˆæœ‰ä»·å€¼çš„å»ºè®®ã€‚

## è¾“å…¥æ•°æ®è¯´æ˜
ä½ ä¼šæ”¶åˆ°JSONå¯¹è±¡ï¼ŒåŒ…å«ï¼š
- `activities`: ç”¨æˆ·æ´»åŠ¨è®°å½•
- `web_data`: ç½‘é¡µåˆ†ææŠ¥å‘Šï¼ˆåŒ…å«metadata_analysisã€detailed_summaryç­‰å­—æ®µï¼‰
- `todos`: å¾…åŠäº‹é¡¹
- `existing_tips`: å·²ç”Ÿæˆçš„æç¤ºï¼ˆé¿å…é‡å¤ï¼‰
- `relevant_history`: ç›¸å…³çš„å†å²ä¸Šä¸‹æ–‡

## Tipsç±»å‹
æ¯ä¸ªtipçš„`type`å¿…é¡»æ˜¯ä»¥ä¸‹ä¹‹ä¸€ï¼š
- `DEEP_DIVE`: æ·±å…¥è§£ææ ¸å¿ƒæ¦‚å¿µ
- `RESOURCE_RECOMMENDATION`: æ¨èå·¥å…·ã€æ–‡ç« ã€æ•™ç¨‹ç­‰èµ„æº
- `RISK_ANALYSIS`: æŒ‡å‡ºæ½œåœ¨é£é™©æˆ–é™·é˜±
- `KNOWLEDGE_EXPANSION`: å…³è”æ–°çŸ¥è¯†é¢†åŸŸ
- `ALTERNATIVE_PERSPECTIVE`: æä¾›æ›¿ä»£æ–¹æ¡ˆæˆ–ä¸åŒè§†è§’

## å†…å®¹è¦æ±‚
- `title`: ç®€çŸ­ç²¾ç‚¼çš„æ ‡é¢˜ï¼ˆ10-30å­—ï¼‰
- `content`: ä½¿ç”¨Markdownæ ¼å¼çš„è¯¦ç»†å†…å®¹ï¼ŒåŒ…å«æ ‡é¢˜ã€åˆ—è¡¨ã€ä»£ç å—ç­‰
- å†…å®¹åº”æ·±å…¥ã€æœ‰ä»·å€¼ï¼Œé¿å…æ³›æ³›è€Œè°ˆ
- é¿å…ä¸existing_tipsé‡å¤
- âš ï¸ æ•°å­¦å…¬å¼ä½¿ç”¨æ™®é€šæ–‡æœ¬æˆ–ä»£ç å—ï¼Œä¸è¦ä½¿ç”¨LaTeXè¯­æ³•ï¼ˆå¦‚\\[ã€\\fracç­‰ï¼‰

## âš ï¸ è¾“å‡ºæ ¼å¼ï¼ˆæå…¶é‡è¦ï¼‰
ç›´æ¥è¿”å›JSONå¯¹è±¡ï¼ŒåŒ…å«ä¸€ä¸ªtipsæ•°ç»„ã€‚

âœ… æ­£ç¡®æ ¼å¼ï¼š
{
  "tips": [
    {
      "title": "React Hooksæ€§èƒ½ä¼˜åŒ–å…³é”®æŠ€å·§",
      "content": "## æ ¸å¿ƒä¼˜åŒ–ç­–ç•¥\n\n### 1. ä½¿ç”¨useMemoå’ŒuseCallback\n\nè¿™ä¸¤ä¸ªHookå¯ä»¥é¿å…ä¸å¿…è¦çš„é‡æ–°è®¡ç®—å’Œé‡æ–°æ¸²æŸ“...\n\n```javascript\nconst memoizedValue = useMemo(() => computeExpensiveValue(a, b), [a, b]);\n```",
      "type": "DEEP_DIVE"
    }
  ]
}

âŒ é”™è¯¯æ ¼å¼ï¼ˆåƒä¸‡ä¸è¦è¿™æ ·ï¼‰ï¼š
- ä¸è¦ç”¨```jsonåŒ…è£¹JSON
- ä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæ–‡å­—
- ä¸è¦è¿”å›Markdownåˆ†ææ–‡ç« 
- ä¸è¦è¿”å›æŠ€æœ¯æŠ¥å‘Š

å¦‚æœæ²¡æœ‰åˆé€‚çš„æç¤ºï¼Œè¿”å›ç©ºtipsæ•°ç»„: {"tips": []}

è®°ä½ï¼šè¿”å›JSONå¯¹è±¡ï¼ŒåŒ…å«tipsæ•°ç»„ï¼"""
        
        user_prompt = f"""è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·è¡Œä¸ºæ•°æ®ï¼Œç”Ÿæˆ1-3ä¸ªæœ‰ä»·å€¼çš„Tipsã€‚

**ä¸Šä¸‹æ–‡æ•°æ®:**

{context_json}

âš ï¸ é‡è¦æé†’ï¼šç›´æ¥è¿”å›JSONå¯¹è±¡ï¼Œæ ¼å¼ä¸º {{"tips": [{{"title": "...", "content": "...", "type": "..."}}]}}ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡å­—æˆ–ä»£ç å—æ ‡è®°ã€‚"""
        
        logger.info("æ­£åœ¨è°ƒç”¨ LLM API...")
        logger.info(f"æ¨¡å‹: {config.LLM_MODEL}, æ¸©åº¦: 0.3, max_tokens: 8196")
        
        # æ„å»ºè¯·æ±‚å‚æ•°
        request_params = {
            "model": config.LLM_MODEL,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "temperature": 0.3,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¨³å®šçš„JSONè¾“å‡º
            "max_tokens": 8196
        }
        
        # å°è¯•å¯ç”¨JSON modeï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
        try:
            # OpenAIçš„gpt-4-turbo-previewå’Œgpt-3.5-turbo-1106åŠä»¥åç‰ˆæœ¬æ”¯æŒJSON mode
            if config.LLM_MODEL and ('gpt-4' in config.LLM_MODEL or 'gpt-3.5' in config.LLM_MODEL):
                request_params["response_format"] = {"type": "json_object"}
                logger.info("âœ… å·²å¯ç”¨JSON mode")
        except Exception as e:
            logger.debug(f"JSON modeä¸å¯ç”¨: {e}")
        
        response = client.chat.completions.create(**request_params)
        
        result_text = response.choices[0].message.content.strip()
        
        # è¯¦ç»†æ‰“å° LLM è¿”å›ä¿¡æ¯
        logger.info("=" * 60)
        logger.info("LLM è¿”å›å®Œæˆ")
        logger.info(f"è¿”å›é•¿åº¦: {len(result_text)} å­—ç¬¦")
        logger.info(f"å¼€å§‹å­—ç¬¦: {result_text[:100] if len(result_text) > 100 else result_text}")
        logger.info(f"ç»“æŸå­—ç¬¦: {result_text[-100:] if len(result_text) > 100 else result_text}")
        logger.info(f"æ˜¯å¦ä»¥ {{ å¼€å¤´: {result_text.startswith('{')}")
        logger.info(f"æ˜¯å¦ä»¥ }} ç»“å°¾: {result_text.endswith('}')}")
        logger.info(f"æ˜¯å¦åŒ…å«ä»£ç å—: {'```' in result_text}")
        logger.info("=" * 60)
        
        # ä½¿ç”¨é€šç”¨ JSON è§£æå·¥å…·
        logger.info("å¼€å§‹è§£æ JSON...")
        result = parse_llm_json_response(
            result_text,
            expected_type='object',  # ç°åœ¨æœŸæœ›è¿”å›å¯¹è±¡
            save_on_error=True,
            error_file_prefix='failed_tip_response'
        )
        
        # æå–tipsæ•°ç»„
        if result is not None:
            tips = result.get('tips', [])
            logger.info("=" * 60)
            logger.info(f"âœ… JSON è§£ææˆåŠŸï¼ç”Ÿæˆäº† {len(tips)} ä¸ª tips")
            for idx, tip in enumerate(tips):
                logger.info(f"  Tip {idx + 1}:")
                logger.info(f"    - title: {tip.get('title', 'N/A')[:50]}...")
                logger.info(f"    - type: {tip.get('type', 'N/A')}")
                logger.info(f"    - content é•¿åº¦: {len(tip.get('content', ''))} å­—ç¬¦")
            logger.info("=" * 60)
            return tips
        else:
            logger.error("=" * 60)
            logger.error("âŒ JSON è§£æå¤±è´¥ï¼Œè¿”å› None")
            logger.error("è¯·æ£€æŸ¥ä»¥ä¸Šæ—¥å¿—ä¸­çš„ LLM è¿”å›å†…å®¹")
            logger.error("=" * 60)
            return []
    except Exception as e:
        logger.error("=" * 60)
        logger.error("âŒ Tip ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯")
        logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        logger.error(f"é”™è¯¯ä¿¡æ¯: {e}")
        logger.exception("å®Œæ•´å †æ ˆè¿½è¸ª:")
        logger.error("=" * 60)
        return []
