"""
æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆæ¨¡å—
åŸºäºæ—¶é—´èŒƒå›´å’Œç”¨æˆ·æ•°æ®ç”Ÿæˆæ´»åŠ¨åˆ†ææŠ¥å‘Š
"""

import json
from datetime import datetime
from typing import Dict, Any, List, Optional
import config
from utils.helpers import get_logger
from utils.db import get_web_data, get_tips, get_todos, insert_report
from utils.llm import get_openai_client

logger = get_logger(__name__)

# å…¨å±€LLMå®¢æˆ·ç«¯
_client_cache = None


def _init_llm():
    """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
    global _client_cache
    if _client_cache is None:
        _client_cache = get_openai_client()
    return _client_cache


async def create_activity_report(start_ts: int, end_ts: int) -> Dict[str, Any]:
    """
    ç”Ÿæˆæ´»åŠ¨æŠ¥å‘Šï¼ˆä¸»å…¥å£ï¼‰
    
    Args:
        start_ts: èµ·å§‹Unixæ—¶é—´æˆ³
        end_ts: ç»“æŸUnixæ—¶é—´æˆ³
    
    Returns:
        åŒ…å«æŠ¥å‘Šæ•°æ®çš„å­—å…¸
    """
    try:
        hours = (end_ts - start_ts) / 3600
        
        # æ ¹æ®æ—¶é—´è·¨åº¦é€‰æ‹©ç­–ç•¥
        if hours > 1:
            report_text = await _generate_segmented_report(start_ts, end_ts)
        else:
            report_text = await _generate_direct_report(start_ts, end_ts)
        
        if not report_text:
            return {"success": False, "message": "ç¼ºå°‘æ•°æ®æ— æ³•ç”Ÿæˆ"}
        
        # æ ¼å¼åŒ–æ—¶é—´
        dt_start = datetime.fromtimestamp(start_ts)
        dt_end = datetime.fromtimestamp(end_ts)
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        title = f"æ´»åŠ¨æŠ¥å‘Š {dt_start.strftime('%Y-%m-%d %H:%M')} è‡³ {dt_end.strftime('%H:%M')}"
        brief = _extract_brief(report_text)
        
        rid = insert_report(
            title=title,
            content=report_text,
            summary=brief,
            document_type="activity_report"
        )
        
        logger.info(f"Report created: ID={rid}")
        
        return {
            "success": True,
            "report_id": rid,
            "content": report_text,
            "time_range": {
                "start": dt_start.isoformat(),
                "end": dt_end.isoformat()
            }
        }
    except Exception as e:
        logger.exception(f"Failed to create report: {e}")
        return {"success": False, "message": str(e)}


async def _generate_direct_report(start_ts: int, end_ts: int) -> Optional[str]:
    """ç›´æ¥ç”ŸæˆæŠ¥å‘Šï¼ˆçŸ­æ—¶é—´æ®µï¼‰"""
    data_dict = _fetch_time_range_data(start_ts, end_ts)
    
    if not data_dict.get("has_data"):
        logger.warning("No data for report")
        return None
    
    return await _ask_llm_for_report(data_dict, start_ts, end_ts)


async def _generate_segmented_report(start_ts: int, end_ts: int) -> Optional[str]:
    """åˆ†æ®µç”ŸæˆæŠ¥å‘Šï¼ˆé•¿æ—¶é—´æ®µï¼‰"""
    logger.info("Using segmented generation for long time range")
    
    # æŒ‰å°æ—¶åˆ‡åˆ†
    segments = []
    current = start_ts
    
    while current < end_ts:
        next_point = min(current + 3600, end_ts)
        segments.append((current, next_point))
        current = next_point
    
    # ç”Ÿæˆå„æ®µæ‘˜è¦
    summaries = []
    for seg_start, seg_end in segments:
        data = _fetch_time_range_data(seg_start, seg_end)
        if data:
            summary = await _make_segment_summary(data, seg_start, seg_end)
            if summary:
                summaries.append({
                    'time_start': seg_start,
                    'time_end': seg_end,
                    'text': summary
                })
    
    if not summaries:
        return None
    
    # æ±‡æ€»æˆå®Œæ•´æŠ¥å‘Š
    return await _combine_summaries(summaries, start_ts, end_ts)


def _fetch_time_range_data(start_ts: int, end_ts: int) -> Dict[str, Any]:
    """
    è·å–æ—¶é—´èŒƒå›´å†…çš„æ‰€æœ‰æ•°æ®ï¼ˆç½‘é¡µã€Tipsã€Todosï¼‰
    
    Returns:
        {
            "web_data": [...],
            "tips": [...],
            "todos": [...],
            "has_data": True/False
        }
    """
    try:
        dt_start = datetime.fromtimestamp(start_ts)
        dt_end = datetime.fromtimestamp(end_ts)
        
        logger.info(f"Fetching data: {dt_start.strftime('%Y-%m-%d %H:%M:%S')} to {dt_end.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # 1. è·å–ç½‘é¡µæ•°æ®
        raw_web_data = get_web_data(
            start_time=dt_start,
            end_time=dt_end,
            limit=100
        )
        
        web_data_list = []
        for item in raw_web_data:
            web_data_list.append({
                "id": item["id"],
                "title": item["title"],
                "url": item.get("url", ""),
                "content": item["content"],
                "source": item.get("source", "unknown"),
                "tags": item.get("tags", []),
                "create_time": item.get("create_time", "")
            })
        
        logger.info(f"Found {len(web_data_list)} web records")
        
        # 2. è·å–Tipsï¼ˆæ™ºèƒ½æç¤ºï¼‰
        tips_list = []
        try:
            all_tips = get_tips(limit=100)
            for tip in all_tips:
                tip_time_str = tip.get('create_time', '')
                if tip_time_str:
                    try:
                        tip_time = datetime.strptime(tip_time_str, '%Y-%m-%d %H:%M:%S')
                        if dt_start <= tip_time <= dt_end:
                            tips_list.append({
                                "id": tip.get("id"),
                                "title": tip.get("title", ""),
                                "content": tip.get("content", ""),
                                "type": tip.get("tip_type", "general"),
                                "create_time": tip_time_str
                            })
                    except Exception as e:
                        logger.debug(f"Failed to parse tip time: {e}")
            
            logger.info(f"Found {len(tips_list)} tips")
        except Exception as e:
            logger.warning(f"Failed to fetch tips: {e}")
        
        # 3. è·å–Todosï¼ˆå¾…åŠäº‹é¡¹ï¼‰
        todos_list = []
        try:
            all_todos = get_todos(limit=200)
            for todo in all_todos:
                todo_time_str = todo.get('create_time', '')
                if todo_time_str:
                    try:
                        todo_time = datetime.strptime(todo_time_str, '%Y-%m-%d %H:%M:%S')
                        if dt_start <= todo_time <= dt_end:
                            todos_list.append({
                                "id": todo.get("id"),
                                "title": todo.get("title", ""),
                                "description": todo.get("description", ""),
                                "status": todo.get("status", 0),  # 0=æœªå®Œæˆ, 1=å·²å®Œæˆ
                                "priority": todo.get("priority", 0),
                                "create_time": todo_time_str,
                                "end_time": todo.get("end_time", "")
                            })
                    except Exception as e:
                        logger.debug(f"Failed to parse todo time: {e}")
            
            logger.info(f"Found {len(todos_list)} todos")
        except Exception as e:
            logger.warning(f"Failed to fetch todos: {e}")
        
        # ç»„è£…ç»“æœ
        result = {
            "web_data": web_data_list,
            "tips": tips_list,
            "todos": todos_list,
            "has_data": bool(web_data_list or tips_list or todos_list)
        }
        
        if not result["has_data"]:
            # è°ƒè¯•ä¿¡æ¯
            all_records = get_web_data(limit=5)
            logger.info(f"Latest 5 web records in DB: {len(all_records)}")
            for rec in all_records:
                logger.info(f"  ID={rec['id']}, Title={rec['title']}, Time={rec.get('create_time')}")
        
        return result
    except Exception as e:
        logger.exception(f"Error fetching data: {e}")
        return {
            "web_data": [],
            "tips": [],
            "todos": [],
            "has_data": False
        }


async def _ask_llm_for_report(data_dict: Dict[str, Any], start_ts: int, end_ts: int) -> Optional[str]:
    """è°ƒç”¨LLMç”ŸæˆæŠ¥å‘Š"""
    client = _init_llm()
    
    if not client or not config.ENABLE_LLM_PROCESSING:
        logger.warning("LLM unavailable, using fallback")
        return _create_plain_report(data_dict, start_ts, end_ts)
    
    try:
        dt_start = datetime.fromtimestamp(start_ts)
        dt_end = datetime.fromtimestamp(end_ts)
        
        # é™åˆ¶æ•°æ®é‡é¿å…Tokenè¶…é™
        report_data = {
            "web_data": data_dict.get("web_data", [])[:50],  # æœ€å¤š50æ¡ç½‘é¡µæ•°æ®
            "tips": data_dict.get("tips", [])[:20],           # æœ€å¤š20æ¡æç¤º
            "todos": data_dict.get("todos", [])[:30]          # æœ€å¤š30æ¡å¾…åŠ
        }
        
        data_json = json.dumps(report_data, ensure_ascii=False, indent=2)
        
        sys_msg = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI ä¸ªäººåˆ†æå¸ˆä¸ç­–ç•¥å¸ˆã€‚

ä½ çš„æ ¸å¿ƒä»»åŠ¡æ˜¯æ•´åˆç”¨æˆ·åœ¨ç‰¹å®šæ—¶é—´æ®µå†…çš„æ´»åŠ¨æ•°æ®ã€å¾…åŠäº‹é¡¹åˆ—è¡¨ä»¥åŠæ¥æ”¶åˆ°çš„æ™ºèƒ½æé†’ï¼Œä¸ä»…è¦ç”Ÿæˆä¸€ä»½ç²¾å‡†çš„æ´»åŠ¨æ€»ç»“æŠ¥å‘Šï¼Œè¿˜è¦æä¾›å…·æœ‰å‰ç»æ€§çš„ä¼˜åŒ–å»ºè®®ã€‚ä½ å¿…é¡»ä¸¥æ ¼ä¾æ®æ‰€æä¾›çš„å…¨éƒ¨ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¿›è¡Œæ·±åº¦èšåˆã€åˆ†æä¸å‘ˆç°ã€‚

---

#### **è¾“å…¥æ•°æ®è¯´æ˜**

1. **web_dataï¼ˆç½‘é¡µæ•°æ®ï¼‰**: ç”¨æˆ·æµè§ˆçš„ç½‘é¡µè®°å½•
   - title: ç½‘é¡µæ ‡é¢˜
   - url: ç½‘é¡µé“¾æ¥
   - content: ç½‘é¡µå†…å®¹
   - tags: æ ‡ç­¾
   - create_time: æµè§ˆæ—¶é—´

2. **tipsï¼ˆæ™ºèƒ½æç¤ºï¼‰**: AIç”Ÿæˆçš„æ™ºèƒ½å»ºè®®å’Œæ´å¯Ÿ
   - title: æç¤ºæ ‡é¢˜
   - content: æç¤ºå†…å®¹
   - type: æç¤ºç±»å‹ï¼ˆDEEP_DIVE/RESOURCE_RECOMMENDATION/RISK_ANALYSISç­‰ï¼‰
   - create_time: ç”Ÿæˆæ—¶é—´

3. **todosï¼ˆå¾…åŠäº‹é¡¹ï¼‰**: AIæå–æˆ–ç”¨æˆ·åˆ›å»ºçš„ä»»åŠ¡
   - title: ä»»åŠ¡æ ‡é¢˜
   - description: ä»»åŠ¡æè¿°
   - status: çŠ¶æ€ï¼ˆ0=æœªå®Œæˆ, 1=å·²å®Œæˆï¼‰
   - priority: ä¼˜å…ˆçº§
   - create_time: åˆ›å»ºæ—¶é—´
   - end_time: å®Œæˆæ—¶é—´

---

#### **æŒ‡å¯¼åŸåˆ™ (Guiding Principles)**

1.  **æ•°æ®ä¿çœŸ (Data Fidelity)**: ä½ çš„æ‰€æœ‰åˆ†æã€æŠ¥å‘Šå’Œå»ºè®®ï¼Œéƒ½å¿…é¡»ä¸¥æ ¼æºäºæä¾›çš„ `web_data`ï¼ˆæ´»åŠ¨ä¸Šä¸‹æ–‡ï¼‰ã€`todos`ï¼ˆå¾…åŠåˆ—è¡¨ï¼‰å’Œ `tips`ï¼ˆæ™ºèƒ½æé†’ï¼‰æ•°æ®ã€‚ä¸¥ç¦ä»»ä½•å½¢å¼çš„çŒœæµ‹æˆ–ä¿¡æ¯æé€ ã€‚

2.  **æ´å¯Ÿæç‚¼ (Insight Extraction)**: ä½ çš„æ ¸å¿ƒä»·å€¼åœ¨äºä»åŸå§‹æ´»åŠ¨æ•°æ® (`web_data`) ä¸­æç‚¼å‡ºçœŸæ­£çš„æˆå°±å’Œæ¨¡å¼ï¼Œè€Œä¸ä»…ä»…æ˜¯ç½—åˆ—ã€‚ä½ éœ€è¦è¯†åˆ«å› æœã€æ€»ç»“è¿›å±•ã€‚

3.  **ä»·å€¼å¯¼å‘ (Value-Oriented)**: æŠ¥å‘Šçš„é‡ç‚¹æ˜¯å½±å“å’Œæˆé•¿ã€‚ä¼˜å…ˆçªå‡ºå…³é”®æˆæœã€å­¦ä¹ æ”¶è·ä»¥åŠèƒ½å¤Ÿå¸®åŠ©ç”¨æˆ·æ”¹è¿›çš„ actionable å»ºè®®ã€‚

---

#### **æŠ¥å‘Šç»“æ„ä¸æ ¼å¼ (Report Structure & Formatting)**

ä½ å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹ Markdown ç»“æ„è¾“å‡ºæŠ¥å‘Šï¼š

# æ¯æ—¥æ´å¯Ÿï¼š{YYYY-MM-DD}

## æ¦‚è§ˆ (Overview)
* *ç”¨2-3å¥è¯ï¼Œç»“åˆæˆ‘çš„**ä¸»è¦æ´»åŠ¨**å’Œæ”¶åˆ°çš„**å…³é”®æé†’ (tips)**ï¼Œé«˜åº¦æ¦‚æ‹¬è¿™æ®µæ—¶é—´çš„æ ¸å¿ƒç„¦ç‚¹ä¸èŠ‚å¥ã€‚*

## æ ¸å¿ƒæˆå°± (Key Achievements)
* *ä½ çš„ä»»åŠ¡æ˜¯**ä» `web_data` çš„æ´»åŠ¨æè¿°ä¸­ä¸»åŠ¨è¯†åˆ«å¹¶æç‚¼å‡º**é‡è¦çš„æˆæœã€å®Œæˆçš„é˜¶æ®µæ€§å·¥ä½œæˆ–è§£å†³çš„å…³é”®é—®é¢˜ã€‚ä¾‹å¦‚ï¼š"æˆ‘å®Œæˆäº†XXé¡¹ç›®åŸå‹è®¾è®¡"ã€"æˆ‘è§£å†³äº†ä¸€ä¸ªå›°æ‰°å·²ä¹…çš„Bug"ã€"æˆ‘è¾“å‡ºäº†ä¸€ä»½æ·±åº¦çš„å¸‚åœºåˆ†ææŠ¥å‘Š"ç­‰ã€‚*
* **[æˆå°±1]**: æˆ‘å®Œæˆäº†...ï¼ˆæè¿°ä¸€é¡¹ä»ä¸Šä¸‹æ–‡ä¸­åˆ†æå‡ºçš„å…³é”®æˆæœï¼‰
* **[æˆå°±2]**: æˆ‘è§£å†³äº†...

## å­¦ä¹ ä¸æˆé•¿ (Learning & Growth)
* *æ•´åˆ `web_data` ä¸­çš„å­¦ä¹ æ¢ç´¢è¡Œä¸ºå’Œ `tips` ä¸­è·å¾—çš„å¯å‘æ€§çŸ¥è¯†ã€‚*
* **[æ–°çŸ¥è¯†/æŠ€èƒ½1]**: æˆ‘å­¦ä¹ äº†...ï¼ˆè®°å½•é€šè¿‡ç ”ç©¶ã€å®è·µæˆ–æé†’è·å¾—çš„æ–°çŸ¥è¯†ç‚¹æˆ–æŠ€èƒ½ï¼‰
* **[æ–°çŸ¥è¯†/æŠ€èƒ½2]**: æˆ‘æŒæ¡äº†...

## å¾…åŠä¸è®¡åˆ’ (Action Items & Plans)
* *ç»¼åˆä»¥ä¸‹ä¸¤ä¸ªæ¥æºï¼Œç”Ÿæˆå¾…åŠåˆ—è¡¨ï¼š1. `todos` ä¸­æ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡ï¼ˆstatus=0ï¼‰ã€‚2. `web_data` ä¸­æ˜ç¡®æåŠçš„æœªæ¥è®¡åˆ’æˆ–ä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚*
* **[ä»»åŠ¡1]**: æˆ‘è®¡åˆ’...ï¼ˆæ¸…æ™°åˆ—å‡ºè¯†åˆ«å‡ºçš„ã€è®¡åˆ’åœ¨æœªæ¥è¿›è¡Œçš„ä»»åŠ¡ï¼‰
* **[ä»»åŠ¡2]**: æˆ‘éœ€è¦...

## ä¼˜åŒ–å»ºè®® (Suggestions for Improvement)
* *åŸºäºå¯¹ä»Šå¤©æ‰€æœ‰æ´»åŠ¨çš„æ•´ä½“åˆ†æï¼Œæå‡º1-2æ¡å…·ä½“ã€å¯è¡Œçš„å»ºè®®ï¼Œæ—¨åœ¨æé«˜æ•ˆç‡ã€è§„é¿é£é™©æˆ–å¼€æ‹“æ€è·¯ã€‚*
* **[å»ºè®®1]**: ä¾‹å¦‚ï¼šâ€œé‰´äºä»Šå¤©åœ¨è°ƒè¯•XXä¸ŠèŠ±è´¹äº†è¾ƒå¤šæ—¶é—´ï¼Œæœªæ¥å¯ä»¥è€ƒè™‘å¼•å…¥è‡ªåŠ¨åŒ–æµ‹è¯•æ¡†æ¶æ¥æé«˜æ•ˆç‡ã€‚â€

## è¯¦ç»†è¶³è¿¹ (Detailed Timeline)
* *æŒ‰æ—¶é—´é¡ºåºï¼Œä»…æ•´åˆ `web_data` ä¸­çš„å…·ä½“æ´»åŠ¨è®°å½•ã€‚*
* **[HH:MM]**: æˆ‘æµè§ˆäº†...ï¼ˆæè¿°ä¸€é¡¹å…·ä½“çš„æ´»åŠ¨ï¼Œé™„ä¸Šå…³é”®ç»†èŠ‚ï¼‰
* **[HH:MM]**: æˆ‘ç ”ç©¶äº†...
```

---

"""
        
        # è®¡ç®—å¾…åŠç»Ÿè®¡
        todos_list = data_dict.get('todos', [])
        todos_pending = [t for t in todos_list if t.get('status') == 0]
        todos_completed = [t for t in todos_list if t.get('status') == 1]
        
        user_msg = f"""**è¯·æ±‚ (REQUEST):**
è¯·ä¸ºæˆ‘ç”Ÿæˆä¸€ä»½ä¸ªäººæ´»åŠ¨æŠ¥å‘ŠåŠä¼˜åŒ–å»ºè®®ï¼Œæ—¶é—´èŒƒå›´ä» **{dt_start.strftime('%Y-%m-%d %H:%M:%S')}** åˆ° **{dt_end.strftime('%Y-%m-%d %H:%M:%S')}**ã€‚

---

**æ•°æ®é›† (DATASET):**

* **åˆ†ææ—¶é—´æˆ³èŒƒå›´**: `{int(start_ts)}` è‡³ `{int(end_ts)}`

* **æ´»åŠ¨æ•°æ®è®°å½• (Activity Contexts / web_data)**:
```json
{json.dumps(report_data.get('web_data', []), ensure_ascii=False, indent=2)}
```

* **å¾…åŠäº‹é¡¹åˆ—è¡¨ (Todo List / todos)**:
```json
{json.dumps(report_data.get('todos', []), ensure_ascii=False, indent=2)}
```
*æ³¨ï¼šæ­¤åˆ—è¡¨ç”¨äºè¡¥å……"å¾…åŠä¸è®¡åˆ’"ï¼Œä½†ä¸ä½œä¸ºåˆ¤æ–­"æ ¸å¿ƒæˆå°±"çš„ä¾æ®ã€‚*

* **æ­¤æœŸé—´ç”Ÿæˆçš„æé†’ (Generated Tips / tips)**:
```json
{json.dumps(report_data.get('tips', []), ensure_ascii=False, indent=2)}
```
*æ³¨ï¼šè¯·å°†è¿™äº›æé†’ä½œä¸ºå½’çº³"æ¦‚è§ˆ"å’Œ"å­¦ä¹ ä¸æˆé•¿"æ—¶çš„é‡è¦å‚è€ƒã€‚*

---

**æ•°æ®ç»Ÿè®¡æ‘˜è¦ï¼š**
- æ´»åŠ¨è®°å½•ï¼ˆweb_dataï¼‰ï¼š{len(data_dict.get('web_data', []))} æ¡
- æ™ºèƒ½æé†’ï¼ˆtipsï¼‰ï¼š{len(data_dict.get('tips', []))} æ¡
- å¾…åŠäº‹é¡¹ï¼ˆtodosï¼‰ï¼š{len(todos_list)} æ¡
  - æœªå®Œæˆï¼š{len(todos_pending)} é¡¹
  - å·²å®Œæˆï¼š{len(todos_completed)} é¡¹

---

**å…³é”®æŒ‡ä»¤ (CRITICAL INSTRUCTIONS):**

1. **æˆå°±æ¨æ–­**: ä½ å¿…é¡»ä» `web_data` æ•°æ®ä¸­**åˆ†æå¹¶æ¨æ–­**å‡ºæˆ‘çš„æ ¸å¿ƒæˆå°±ã€‚è¿™æ˜¯å¯¹ä½ åˆ†æèƒ½åŠ›çš„æ ¸å¿ƒè€ƒéªŒã€‚

2. **è®¡åˆ’æ•´åˆ**: åœ¨æ„å»º"å¾…åŠä¸è®¡åˆ’"éƒ¨åˆ†æ—¶ï¼ŒåŠ¡å¿…**ç»“åˆ** `todos` çš„æœªå®Œæˆé¡¹å’Œ `web_data` ä¸­éšå«çš„æœªæ¥æ„å›¾ã€‚

3. **å»ºè®®ç”Ÿæˆ**: "ä¼˜åŒ–å»ºè®®"éƒ¨åˆ†å¿…é¡»æ˜¯**åŸåˆ›çš„ã€å»ºè®¾æ€§çš„**ï¼Œä¸”ç´§å¯†å…³è”å½“å¤©æ´»åŠ¨ä¸­å‘ç°çš„æ¨¡å¼æˆ–æŒ‘æˆ˜ã€‚

4. **ä¿¡æ¯æºåˆ†ç¦»**: ä¸¥æ ¼éµå®ˆæ¯ä¸ªæ¿å—çš„æ•°æ®æ¥æºæŒ‡ç¤ºï¼Œç¡®ä¿æŠ¥å‘Šçš„å‡†ç¡®æ€§å’Œé€»è¾‘æ€§ã€‚

5. **ç¬¬ä¸€äººç§°è§†è§’**: ä½¿ç”¨"æˆ‘"è¿›è¡Œå™è¿°ï¼Œå¢å¼ºæŠ¥å‘Šçš„ä»£å…¥æ„Ÿã€‚

---

è¯·ä¸¥æ ¼æŒ‰ç…§ç³»ç»ŸæŒ‡ä»¤ä¸­å®šä¹‰çš„æŠ¥å‘Šç»“æ„ç”Ÿæˆå®Œæ•´çš„ä¸ªäººæ´»åŠ¨æ´å¯ŸæŠ¥å‘Šã€‚"""
        
        response = client.chat.completions.create(
            model=config.LLM_MODEL,
            messages=[
                {"role": "system", "content": sys_msg},
                {"role": "user", "content": user_msg}
            ],
            temperature=0.7,
            max_tokens=2500
        )
        
        result = response.choices[0].message.content
        logger.info("LLM report generated successfully")
        return result
    except Exception as e:
        logger.exception(f"LLM error: {e}")
        return _create_plain_report(data_dict, start_ts, end_ts)


async def _make_segment_summary(data_list: List[Dict], start_ts: int, end_ts: int) -> Optional[str]:
    """ç”Ÿæˆæ—¶æ®µæ‘˜è¦"""
    client = _init_llm()
    
    if not client or not config.ENABLE_LLM_PROCESSING:
        return _simple_summary(data_list)
    
    try:
        data_json = json.dumps(data_list, ensure_ascii=False, indent=2)
        dt_start = datetime.fromtimestamp(start_ts)
        dt_end = datetime.fromtimestamp(end_ts)
        
        prompt = f"""è¯·ç”¨ç¬¬ä¸€äººç§°ï¼ˆ"æˆ‘"ï¼‰ç®€è¦æ€»ç»“ä»¥ä¸‹æ—¶æ®µçš„æ´»åŠ¨ï¼ˆä¸è¶…è¿‡100å­—ï¼‰ã€‚

**æ—¶æ®µ**: {dt_start.strftime('%H:%M')} - {dt_end.strftime('%H:%M')}

**æ´»åŠ¨æ•°æ®**:
{data_json}

**è¦æ±‚**:
- ä½¿ç”¨ç¬¬ä¸€äººç§°å™è¿°ï¼ˆä¾‹å¦‚ï¼š"æˆ‘æµè§ˆäº†..."ã€"æˆ‘ç ”ç©¶äº†..."ï¼‰
- æç‚¼å…³é”®æ´»åŠ¨ï¼Œè€Œéç®€å•ç½—åˆ—
- çªå‡ºæˆæœå’Œé‡ç‚¹

**æ‘˜è¦**:"""
        
        response = client.chat.completions.create(
            model=config.LLM_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            max_tokens=200
        )
        
        return response.choices[0].message.content
    except Exception as e:
        logger.error(f"Summary error: {e}")
        return _simple_summary(data_list)


async def _combine_summaries(summaries: List[Dict], start_ts: int, end_ts: int) -> str:
    """åˆå¹¶æ—¶æ®µæ‘˜è¦ä¸ºå®Œæ•´æŠ¥å‘Š"""
    client = _init_llm()
    
    if not client or not config.ENABLE_LLM_PROCESSING:
        return _merge_text(summaries, start_ts, end_ts)
    
    try:
        summary_text = "\n\n".join([
            f"**{datetime.fromtimestamp(s['time_start']).strftime('%H:%M')} - {datetime.fromtimestamp(s['time_end']).strftime('%H:%M')}:**\n{s['text']}"
            for s in summaries
        ])
        
        dt_start = datetime.fromtimestamp(start_ts)
        dt_end = datetime.fromtimestamp(end_ts)
        
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ AI ä¸ªäººåˆ†æå¸ˆä¸ç­–ç•¥å¸ˆã€‚

**ä»»åŠ¡**: åŸºäºä»¥ä¸‹å„æ—¶æ®µæ‘˜è¦ï¼Œä¸ºæˆ‘ç”Ÿæˆä¸€ä»½å®Œæ•´çš„ä¸ªäººæ´»åŠ¨æ´å¯ŸæŠ¥å‘Šã€‚

**æ—¶é—´èŒƒå›´**: {dt_start.strftime('%Y-%m-%d %H:%M')} è‡³ {dt_end.strftime('%H:%M')}

**æ—¶æ®µæ‘˜è¦æ•°æ®**:
{summary_text}

**è¾“å‡ºè¦æ±‚**:

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ Markdown ç»“æ„ç”ŸæˆæŠ¥å‘Šï¼ˆä½¿ç”¨ç¬¬ä¸€äººç§°"æˆ‘"ï¼‰ï¼š

# æ¯æ—¥æ´å¯Ÿï¼š{dt_start.strftime('%Y-%m-%d')}

## æ¦‚è§ˆ (Overview)
* ç”¨2-3å¥è¯ï¼Œé«˜åº¦æ¦‚æ‹¬æˆ‘è¿™ä¸€å¤©çš„æ ¸å¿ƒç„¦ç‚¹ä¸èŠ‚å¥ã€‚

## æ ¸å¿ƒæˆå°± (Key Achievements)
* **[æˆå°±1]**: æˆ‘å®Œæˆäº†...ï¼ˆä»å„æ—¶æ®µæ‘˜è¦ä¸­æç‚¼å‡ºå…³é”®æˆæœï¼‰
* **[æˆå°±2]**: æˆ‘è§£å†³äº†...

## å­¦ä¹ ä¸æˆé•¿ (Learning & Growth)
* **[æ–°çŸ¥è¯†/æŠ€èƒ½1]**: æˆ‘å­¦ä¹ äº†...
* **[æ–°çŸ¥è¯†/æŠ€èƒ½2]**: æˆ‘æŒæ¡äº†...

## å¾…åŠä¸è®¡åˆ’ (Action Items & Plans)
* **[ä»»åŠ¡1]**: æˆ‘è®¡åˆ’...ï¼ˆä»æ‘˜è¦ä¸­è¯†åˆ«çš„æœªæ¥è¡ŒåŠ¨ï¼‰
* **[ä»»åŠ¡2]**: æˆ‘éœ€è¦...

## ä¼˜åŒ–å»ºè®® (Suggestions for Improvement)
* **[å»ºè®®1]**: åŸºäºä»Šå¤©çš„æ´»åŠ¨æ¨¡å¼ï¼Œæå‡ºå…·ä½“ã€å¯è¡Œçš„ä¼˜åŒ–å»ºè®®ã€‚

## è¯¦ç»†è¶³è¿¹ (Detailed Timeline)
* æŒ‰æ—¶é—´é¡ºåºæ•´ç†å„æ—¶æ®µçš„æ´»åŠ¨è¦ç‚¹ã€‚

**å…³é”®è¦æ±‚**:
1. ä½¿ç”¨ç¬¬ä¸€äººç§°ï¼ˆ"æˆ‘"ï¼‰è¿›è¡Œå™è¿°
2. ä»æ—¶æ®µæ‘˜è¦ä¸­æç‚¼çœŸæ­£çš„æˆå°±ï¼Œè€Œéç®€å•ç½—åˆ—
3. æä¾›åŸåˆ›çš„ã€å»ºè®¾æ€§çš„ä¼˜åŒ–å»ºè®®
4. ä¿æŒæŠ¥å‘Šçš„æ´å¯ŸåŠ›å’Œä»·å€¼å¯¼å‘
5. æ®µè½åˆ†æ˜ï¼Œæ¯æ®µæ ‡é¢˜å‰é¢è¦æœ‰å›¾æ ‡ï¼Œç±»ä¼¼ github çš„ readme æ ¼å¼"""
        
        response = client.chat.completions.create(
            model=config.LLM_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=2000
        )
        
        return response.choices[0].message.content
    except Exception as e:
        logger.error(f"Combine error: {e}")
        return _merge_text(summaries, start_ts, end_ts)


def _create_plain_report(data_dict: Dict[str, Any], start_ts: int, end_ts: int) -> str:
    """åˆ›å»ºç®€å•æŠ¥å‘Šï¼ˆæ— LLMï¼‰"""
    dt_start = datetime.fromtimestamp(start_ts)
    dt_end = datetime.fromtimestamp(end_ts)
    
    web_data = data_dict.get("web_data", [])
    tips = data_dict.get("tips", [])
    todos = data_dict.get("todos", [])
    
    lines = [
        f"# æ´»åŠ¨æŠ¥å‘Š",
        "",
        f"**æ—¶é—´ï¼š** {dt_start.strftime('%Y-%m-%d %H:%M')} è‡³ {dt_end.strftime('%H:%M')}",
        "",
        f"## ğŸ“Š æ¦‚è§ˆ",
        "",
        f"- ç½‘é¡µæµè§ˆï¼š{len(web_data)} æ¡",
        f"- æ™ºèƒ½æç¤ºï¼š{len(tips)} æ¡",
        f"- å¾…åŠäº‹é¡¹ï¼š{len(todos)} æ¡",
        ""
    ]
    
    # ç½‘é¡µæ´»åŠ¨åˆ—è¡¨
    if web_data:
        lines.extend([
            "## ğŸŒ ç½‘é¡µæµè§ˆæ´»åŠ¨",
            ""
        ])
        for idx, item in enumerate(web_data[:20], 1):
            lines.extend([
                f"### {idx}. {item.get('title', 'æœªå‘½å')}",
                "",
                f"- **æ¥æº:** {item.get('source', 'unknown')}",
                f"- **æ—¶é—´:** {item.get('create_time', 'unknown')}",
                f"- **é“¾æ¥:** {item.get('url', 'N/A')}",
                f"- **æ ‡ç­¾:** {', '.join(item.get('tags', []))}",
                ""
            ])
    
    # æ™ºèƒ½æç¤ºåˆ—è¡¨
    if tips:
        lines.extend([
            "## ğŸ’¡ æ™ºèƒ½æç¤º",
            ""
        ])
        for idx, tip in enumerate(tips, 1):
            lines.extend([
                f"### {idx}. {tip.get('title', 'æœªå‘½åæç¤º')}",
                "",
                f"**ç±»å‹:** {tip.get('type', 'general')}",
                "",
                tip.get('content', ''),
                "",
                f"*ç”Ÿæˆæ—¶é—´: {tip.get('create_time', 'unknown')}*",
                ""
            ])
    else:
        lines.extend([
            "## ğŸ’¡ æ™ºèƒ½æç¤º",
            "",
            "æœ¬æ—¶æ®µæœªç”Ÿæˆæ™ºèƒ½æç¤ºã€‚",
            ""
        ])
    
    # å¾…åŠäº‹é¡¹åˆ—è¡¨
    if todos:
        lines.extend([
            "## âœ… å¾…åŠäº‹é¡¹",
            ""
        ])
        completed = [t for t in todos if t.get('status') == 1]
        pending = [t for t in todos if t.get('status') == 0]
        
        lines.extend([
            f"**ç»Ÿè®¡:** å…± {len(todos)} é¡¹ï¼Œå·²å®Œæˆ {len(completed)} é¡¹ï¼Œå¾…å®Œæˆ {len(pending)} é¡¹",
            ""
        ])
        
        if pending:
            lines.extend([
                "### å¾…å®Œæˆä»»åŠ¡",
                ""
            ])
            for todo in pending:
                priority_str = "â­" * todo.get('priority', 0) if todo.get('priority', 0) > 0 else ""
                lines.extend([
                    f"- [ ] {todo.get('title', 'æœªå‘½åä»»åŠ¡')} {priority_str}",
                    f"  - {todo.get('description', '')}",
                    ""
                ])
        
        if completed:
            lines.extend([
                "### å·²å®Œæˆä»»åŠ¡",
                ""
            ])
            for todo in completed:
                lines.extend([
                    f"- [x] {todo.get('title', 'æœªå‘½åä»»åŠ¡')}",
                    f"  - {todo.get('description', '')}",
                    f"  - å®Œæˆæ—¶é—´: {todo.get('end_time', 'unknown')}",
                    ""
                ])
    else:
        lines.extend([
            "## âœ… å¾…åŠäº‹é¡¹",
            "",
            "æœ¬æ—¶æ®µæœªç”Ÿæˆå¾…åŠäº‹é¡¹ã€‚",
            ""
        ])
    
    lines.extend([
        "## ğŸ“ˆ æ€»ç»“",
        "",
        "æœ¬æŠ¥å‘ŠåŸºäºåŸå§‹æ•°æ®è‡ªåŠ¨ç”Ÿæˆã€‚",
        ""
    ])
    
    return "\n".join(lines)


def _simple_summary(data_list: List[Dict]) -> str:
    """ç”Ÿæˆç®€å•æ‘˜è¦"""
    if not data_list:
        return "æ— æ´»åŠ¨"
    
    titles = [d.get('title', 'æœªå‘½å') for d in data_list[:3]]
    text = f"å…± {len(data_list)} æ¡ï¼š" + "ã€".join(titles)
    
    if len(data_list) > 3:
        text += " ç­‰"
    
    return text


def _merge_text(summaries: List[Dict], start_ts: int, end_ts: int) -> str:
    """åˆå¹¶æ‘˜è¦æ–‡æœ¬"""
    dt_start = datetime.fromtimestamp(start_ts)
    dt_end = datetime.fromtimestamp(end_ts)
    
    lines = [
        "# æ´»åŠ¨æŠ¥å‘Š",
        "",
        f"**æ—¶é—´ï¼š** {dt_start.strftime('%Y-%m-%d %H:%M')} è‡³ {dt_end.strftime('%H:%M')}",
        "",
        "## åˆ†æ—¶æ®µæ´»åŠ¨",
        ""
    ]
    
    for seg in summaries:
        s_dt = datetime.fromtimestamp(seg['time_start'])
        e_dt = datetime.fromtimestamp(seg['time_end'])
        lines.extend([
            f"### {s_dt.strftime('%H:%M')} - {e_dt.strftime('%H:%M')}",
            "",
            seg['text'],
            ""
        ])
    
    lines.extend(["## æ€»ç»“", "", "æ±‡æ€»å„æ—¶æ®µæ´»åŠ¨ã€‚"])
    
    return "\n".join(lines)


def _extract_brief(text: str) -> str:
    """æå–ç®€è¦æ‘˜è¦"""
    lines = text.split('\n')
    non_empty = [l.strip() for l in lines if l.strip() and not l.startswith('#')]
    brief_lines = non_empty[:3]
    return ' '.join(brief_lines)[:200]
