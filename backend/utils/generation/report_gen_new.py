"""
æ™ºèƒ½æŠ¥å‘Šç”Ÿæˆæ¨¡å—
åŸºäºæ—¶é—´èŒƒå›´å’Œç”¨æˆ·æ•°æ®ç”Ÿæˆæ´»åŠ¨åˆ†ææŠ¥å‘Š
"""

import json
from datetime import datetime
from typing import Dict, Any, List, Optional
import config
from utils.helpers import (
    get_logger,
    estimate_tokens,
    truncate_web_data_by_tokens,
    calculate_available_context_tokens
)
from utils.db import get_web_data, get_tips, get_todos, insert_report
from utils.llm import get_openai_client

logger = get_logger(__name__)

# å…¨å±€LLMå®¢æˆ·ç«¯
_client_cache = None


def _init_llm():
    """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
    global _client_cache
    if _client_cache is None:
        _client_cache = get_openai_client()
    return _client_cache


async def create_activity_report(start_ts: int, end_ts: int) -> Dict[str, Any]:
    """
    ç”Ÿæˆæ´»åŠ¨æŠ¥å‘Šï¼ˆä¸»å…¥å£ï¼‰
    
    Args:
        start_ts: èµ·å§‹Unixæ—¶é—´æˆ³
        end_ts: ç»“æŸUnixæ—¶é—´æˆ³
    
    Returns:
        åŒ…å«æŠ¥å‘Šæ•°æ®çš„å­—å…¸
    """
    try:
        hours = (end_ts - start_ts) / 3600
        
        # æ ¹æ®æ—¶é—´è·¨åº¦é€‰æ‹©ç­–ç•¥
        if hours > 1:
            report_text = await _generate_segmented_report(start_ts, end_ts)
        else:
            report_text = await _generate_direct_report(start_ts, end_ts)
        
        if not report_text:
            return {"success": False, "message": "ç¼ºå°‘æ•°æ®æ— æ³•ç”Ÿæˆ"}
        
        # æ ¼å¼åŒ–æ—¶é—´
        dt_start = datetime.fromtimestamp(start_ts)
        dt_end = datetime.fromtimestamp(end_ts)
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        title = f"æ´»åŠ¨æŠ¥å‘Š {dt_end.strftime('%Y-%m-%d %H:%M')}"
        brief = _extract_brief(report_text)
        
        rid = insert_report(
            title=title,
            content=report_text,
            summary=brief,
            document_type="activity_report"
        )
        
        logger.info(f"Report created: ID={rid}")
        
        return {
            "success": True,
            "report_id": rid,
            "content": report_text,
            "time_range": {
                "start": dt_start.isoformat(),
                "end": dt_end.isoformat()
            }
        }
    except Exception as e:
        logger.exception(f"Failed to create report: {e}")
        return {"success": False, "message": str(e)}


async def _generate_direct_report(start_ts: int, end_ts: int) -> Optional[str]:
    """ç›´æ¥ç”ŸæˆæŠ¥å‘Šï¼ˆçŸ­æ—¶é—´æ®µï¼‰"""
    data_dict = _fetch_time_range_data(start_ts, end_ts)
    
    if not data_dict.get("has_data"):
        logger.warning("No data for report")
        return None
    
    return await _ask_llm_for_report(data_dict, start_ts, end_ts)


async def _generate_segmented_report(start_ts: int, end_ts: int) -> Optional[str]:
    """åˆ†æ®µç”ŸæˆæŠ¥å‘Šï¼ˆé•¿æ—¶é—´æ®µï¼‰"""
    logger.info("Using segmented generation for long time range")
    
    # æŒ‰å°æ—¶åˆ‡åˆ†
    segments = []
    current = start_ts
    
    while current < end_ts:
        next_point = min(current + 3600, end_ts)
        segments.append((current, next_point))
        current = next_point
    
    # ç”Ÿæˆå„æ®µæ‘˜è¦
    summaries = []
    for seg_start, seg_end in segments:
        data = _fetch_time_range_data(seg_start, seg_end)
        if data:
            summary = await _make_segment_summary(data, seg_start, seg_end)
            if summary:
                summaries.append({
                    'time_start': seg_start,
                    'time_end': seg_end,
                    'text': summary
                })
    
    if not summaries:
        return None
    
    # æ±‡æ€»æˆå®Œæ•´æŠ¥å‘Š
    return await _combine_summaries(summaries, start_ts, end_ts)


def _fetch_time_range_data(start_ts: int, end_ts: int) -> Dict[str, Any]:
    """
    è·å–æ—¶é—´èŒƒå›´å†…çš„æ‰€æœ‰æ•°æ®ï¼ˆç½‘é¡µã€Tipsã€Todosï¼‰
    
    Returns:
        {
            "web_data": [...],
            "tips": [...],
            "todos": [...],
            "has_data": True/False
        }
    """
    try:
        dt_start = datetime.fromtimestamp(start_ts)
        dt_end = datetime.fromtimestamp(end_ts)
        
        logger.info(f"Fetching data: {dt_start.strftime('%Y-%m-%d %H:%M:%S')} to {dt_end.strftime('%Y-%m-%d %H:%M:%S')}")
        
        # 1. è·å–ç½‘é¡µæ•°æ®
        raw_web_data = get_web_data(
            start_time=dt_start,
            end_time=dt_end,
            limit=100
        )
        
        web_data_list = []
        for item in raw_web_data:
            web_data_list.append({
                "id": item["id"],
                "title": item["title"],
                "url": item.get("url", ""),
                "metadata": item.get("metadata", {}),
                "source": item.get("source", "unknown"),
                "tags": item.get("tags", []),
                "create_time": item.get("create_time", "")
            })
        
        logger.info(f"Found {len(web_data_list)} web records")
        
        # 2. è·å–Tipsï¼ˆæ™ºèƒ½æç¤ºï¼‰
        tips_list = []
        try:
            all_tips = get_tips(limit=100)
            for tip in all_tips:
                tip_time_str = tip.get('create_time', '')
                if tip_time_str:
                    try:
                        tip_time = datetime.strptime(tip_time_str, '%Y-%m-%d %H:%M:%S')
                        if dt_start <= tip_time <= dt_end:
                            tips_list.append({
                                "id": tip.get("id"),
                                "title": tip.get("title", ""),
                                "content": tip.get("content", ""),
                                "type": tip.get("tip_type", "general"),
                                "create_time": tip_time_str
                            })
                    except Exception as e:
                        logger.debug(f"Failed to parse tip time: {e}")
            
            logger.info(f"Found {len(tips_list)} tips")
        except Exception as e:
            logger.warning(f"Failed to fetch tips: {e}")
        
        # 3. è·å–Todosï¼ˆå¾…åŠäº‹é¡¹ï¼‰
        todos_list = []
        try:
            all_todos = get_todos(limit=200)
            for todo in all_todos:
                todo_time_str = todo.get('create_time', '')
                if todo_time_str:
                    try:
                        todo_time = datetime.strptime(todo_time_str, '%Y-%m-%d %H:%M:%S')
                        if dt_start <= todo_time <= dt_end:
                            todos_list.append({
                                "id": todo.get("id"),
                                "title": todo.get("title", ""),
                                "description": todo.get("description", ""),
                                "status": todo.get("status", 0),  # 0=æœªå®Œæˆ, 1=å·²å®Œæˆ
                                "priority": todo.get("priority", 0),
                                "create_time": todo_time_str,
                                "end_time": todo.get("end_time", "")
                            })
                    except Exception as e:
                        logger.debug(f"Failed to parse todo time: {e}")
            
            logger.info(f"Found {len(todos_list)} todos")
        except Exception as e:
            logger.warning(f"Failed to fetch todos: {e}")
        
        # ç»„è£…ç»“æœ
        result = {
            "web_data": web_data_list,
            "tips": tips_list,
            "todos": todos_list,
            "has_data": bool(web_data_list or tips_list or todos_list)
        }
        
        if not result["has_data"]:
            # è°ƒè¯•ä¿¡æ¯
            all_records = get_web_data(limit=5)
            logger.info(f"Latest 5 web records in DB: {len(all_records)}")
            for rec in all_records:
                logger.info(f"  ID={rec['id']}, Title={rec['title']}, Time={rec.get('create_time')}")
        
        return result
    except Exception as e:
        logger.exception(f"Error fetching data: {e}")
        return {
            "web_data": [],
            "tips": [],
            "todos": [],
            "has_data": False
        }


async def _ask_llm_for_report(data_dict: Dict[str, Any], start_ts: int, end_ts: int) -> Optional[str]:
    """è°ƒç”¨LLMç”ŸæˆæŠ¥å‘Š"""
    client = _init_llm()
    
    if not client or not config.ENABLE_LLM_PROCESSING:
        logger.warning("LLM unavailable, using fallback")
        return _create_plain_report(data_dict, start_ts, end_ts)
    
    try:
        dt_start = datetime.fromtimestamp(start_ts)
        dt_end = datetime.fromtimestamp(end_ts)
        
        # é™åˆ¶æ•°æ®é‡é¿å…Tokenè¶…é™
        tips = data_dict.get("tips", [])[:20]    # æœ€å¤š20æ¡æç¤º
        todos = data_dict.get("todos", [])[:30]  # æœ€å¤š30æ¡å¾…åŠ
        
        # ä¼°ç®— tips å’Œ todos çš„ token
        other_data_json = json.dumps({
            "tips": tips,
            "todos": todos
        }, ensure_ascii=False)
        other_data_tokens = estimate_tokens(other_data_json)
        
        # è®¡ç®—å¯ç”¨äº web_data çš„ token æ•°
        available_tokens = calculate_available_context_tokens('report', other_data_tokens)
        
        # ä½¿ç”¨åŠ¨æ€æˆªå–å‡½æ•°å¤„ç† web_dataï¼Œä½¿ç”¨ metadata æ›¿ä»£ content
        web_data_trimmed = truncate_web_data_by_tokens(
            data_dict.get("web_data", []),
            max_tokens=available_tokens,
            use_metadata=True
        )
        
        report_data = {
            "web_data": web_data_trimmed,
            "tips": tips,
            "todos": todos
        }
        
        data_json = json.dumps(report_data, ensure_ascii=False, indent=2)
        
        sys_msg = """ä½ æ˜¯ä¸€ä½é¡¶çº§çš„ä¸ªäººæ™ºèƒ½åˆ†æå¸ˆä¸é¦–å¸­ç­–ç•¥å¸ˆ (Principal Personal Intelligence Analyst & Chief Strategist)ã€‚ä½ æ“…é•¿èåˆå¤šæºæ•°æ®ï¼Œæç‚¼æ ¸å¿ƒæ´å¯Ÿï¼Œå¹¶ä»¥æ¸…æ™°ã€å®¢è§‚çš„ç»“æ„åŒ–æŠ¥å‘Šå½¢å¼å‘ˆç°ã€‚

## ä»»åŠ¡ç›®æ ‡ (Task Goal)

ä½ çš„æ ¸å¿ƒç›®æ ‡æ˜¯æ•´åˆç”¨æˆ·åœ¨ç‰¹å®šæ—¶é—´æ®µå†…çš„æ‰€æœ‰æ´»åŠ¨æ•°æ®ã€æ™ºèƒ½æé†’å’Œå¾…åŠäº‹é¡¹ï¼Œç”Ÿæˆä¸€ä»½ç²¾å‡†ã€æ·±åˆ»ã€ä»·å€¼å¯¼å‘çš„æ´å¯ŸæŠ¥å‘Šã€‚è¿™ä»½æŠ¥å‘Šä¸ä»…è¦æ€»ç»“ç”¨æˆ·çš„æ´»åŠ¨ï¼Œæ›´è¦æç‚¼å‡ºå…³é”®æˆå°±ã€å­¦ä¹ æˆé•¿ï¼Œå¹¶ä¸ºç”¨æˆ·æ¥ä¸‹æ¥çš„è¡ŒåŠ¨æä¾›å…·ä½“å»ºè®®ã€‚

## è¾“å…¥æ•°æ®è¯´æ˜ (Input Data Description)

ä½ å°†æ”¶åˆ°ä¸€ä¸ªåä¸º`context_data`çš„å•ä¸€JSONå¯¹è±¡ï¼Œå®ƒåŒ…å«ä»¥ä¸‹ä¸‰ä¸ªå…³é”®å­—æ®µï¼š

1. **`activities`**: ä¸€ä¸ªJSONæ•°ç»„ï¼Œè®°å½•äº†ç”¨æˆ·ä¸€æ®µæ—¶é—´å†…çš„æ´»åŠ¨è®°å½•ã€‚
2. **`tips`**: ä¸€ä¸ªJSONæ•°ç»„ï¼ŒåŒ…å«å·²ç”Ÿæˆçš„ä¿¡æ¯æ´å¯Ÿã€‚
3. **`todos`**: ä¸€ä¸ªJSONæ•°ç»„ï¼ŒåŒ…å«ç”¨æˆ·å·²æœ‰çš„ã€æ‰€æœ‰æœªå®Œæˆçš„å¾…åŠäº‹é¡¹åˆ—è¡¨ã€‚

## æ‰§è¡Œæ­¥éª¤ (Execution Steps)

ä½ å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹å…«ä¸ªæ­¥éª¤ï¼Œä»¥ä¸­ç«‹ã€å®¢è§‚çš„è§†è§’æ’°å†™æŠ¥å‘Šï¼š

1. **æ•°æ®å…¨å±€ç†è§£**: é¦–å…ˆï¼Œé€šè¯»æ‰€æœ‰ä¸‰ä¸ªè¾“å…¥æ•°æ®æºï¼Œå¯¹ç”¨æˆ·åœ¨æ­¤æœŸé—´çš„æ´»åŠ¨ç„¦ç‚¹ã€çŸ¥è¯†ç¼ºå£å’Œä»»åŠ¡å‹åŠ›å½¢æˆä¸€ä¸ªæ•´ä½“å°è±¡ã€‚
2. **æ’°å†™ã€Œæ¦‚è§ˆã€**: ç»“åˆ`web_analysis_reports`çš„ä¸»é¢˜å’Œ`generated_tips`çš„å…³é”®æ´å¯Ÿï¼Œç”¨2-3å¥è¯é«˜åº¦æ¦‚æ‹¬è¯¥ç”¨æˆ·çš„æ ¸å¿ƒç„¦ç‚¹ä¸èŠ‚å¥ã€‚
3. **æ¨æ–­ã€Œæ ¸å¿ƒæˆå°±ã€**: **è¿™æ˜¯æœ€å…·æŒ‘æˆ˜æ€§çš„ä¸€æ­¥ã€‚** ä¸¥æ ¼åªåˆ†æ`web_analysis_reports`ï¼Œä»ä¸­å¯»æ‰¾é—®é¢˜è¢«è§£å†³ã€é¡¹ç›®è¢«æ¨è¿›ã€çŸ¥è¯†è¢«åº”ç”¨çš„**è¯æ®**ï¼Œå¹¶å°†å…¶æç‚¼ä¸ºå…·ä½“çš„æˆå°±ã€‚ä¸¥ç¦ä½¿ç”¨`task_list`ä¸­çš„å·²å®Œæˆé¡¹æ¥æœæ’°æˆå°±ã€‚
4. **æ•´åˆã€Œå­¦ä¹ ä¸æˆé•¿ã€**: èåˆ`web_analysis_reports`ä¸­ä½“ç°çš„å­¦ä¹ æ¢ç´¢è¡Œä¸ºï¼ˆä¾‹å¦‚ï¼Œæµè§ˆæ•™ç¨‹ã€æ–‡æ¡£ï¼‰å’Œ`generated_tips`ä¸­æä¾›çš„å¯å‘æ€§çŸ¥è¯†ï¼Œæ€»ç»“å‡ºå…·ä½“çš„æ–°çŸ¥è¯†æˆ–æŠ€èƒ½æ”¶è·ã€‚
5. **æ„å»ºã€Œå¾…åŠä¸è®¡åˆ’ã€**: æ•´åˆä¸¤ä¸ªæ¥æºï¼š`task_list`ä¸­æ‰€æœ‰**æœªå®Œæˆ**çš„ä»»åŠ¡ï¼Œä»¥åŠä»`web_analysis_reports`ä¸­æ–°å‘ç°çš„ã€æ˜ç¡®æåŠçš„æœªæ¥è®¡åˆ’ã€‚
6. **ç”Ÿæˆã€Œä¼˜åŒ–å»ºè®®ã€**: åŸºäºå¯¹**æ‰€æœ‰æ•°æ®**çš„æ•´ä½“åˆ†æï¼Œè¯†åˆ«å‡ºæ¨¡å¼ã€æŒ‘æˆ˜æˆ–æœºä¼šï¼Œæå‡º1-2æ¡åŸåˆ›çš„ã€å¯æ‰§è¡Œçš„ä¼˜åŒ–å»ºè®®ã€‚
7. **ç½—åˆ—ã€Œè¯¦ç»†è¶³è¿¹ã€**: æŒ‰æ—¶é—´é¡ºåºï¼Œç®€å•ã€å®¢è§‚åœ°ç½—åˆ—`web_analysis_reports`ä¸­çš„å…³é”®æ´»åŠ¨è®°å½•ã€‚
8. **æœ€ç»ˆç»„è£…**: å°†ä»¥ä¸Šæ‰€æœ‰éƒ¨åˆ†ï¼Œä¸¥æ ¼æŒ‰ç…§`## è¾“å‡ºè¦æ±‚`ä¸­å®šä¹‰çš„Markdownæ ¼å¼å’Œ**ä¸­ç«‹çš„ç¬¬ä¸‰äººç§°è§†è§’**ï¼Œç»„è£…æˆæœ€ç»ˆæŠ¥å‘Šã€‚

## è¾“å‡ºè¦æ±‚ (Output Requirements)

ä½ å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹Markdownç»“æ„è¾“å‡ºæŠ¥å‘Šã€‚å™è¿°æ–¹å¼åº”ä¿æŒ**å®¢è§‚ã€ä¸­ç«‹ï¼Œé¿å…ä½¿ç”¨ç¬¬ä¸€äººç§°ä»£è¯ï¼ˆå¦‚â€œæˆ‘â€ï¼‰**ã€‚

### MarkdownæŠ¥å‘Šç»“æ„:

```markdown
# ç”¨æˆ·æ´å¯ŸæŠ¥å‘Šï¼š{YYYY-MM-DD}

## æ¦‚è§ˆ (Overview)
* *ç”¨2-3å¥è¯ï¼Œç»“åˆç”¨æˆ·çš„ä¸»è¦æ´»åŠ¨å’Œæ”¶åˆ°çš„å…³é”®æé†’(tips)ï¼Œé«˜åº¦æ¦‚æ‹¬è¿™æ®µæ—¶é—´çš„æ ¸å¿ƒç„¦ç‚¹ä¸èŠ‚å¥ã€‚*

## æ ¸å¿ƒæˆå°± (Key Achievements)
* *ä»»åŠ¡æ˜¯**ä» `web_analysis_reports` çš„æ´»åŠ¨æè¿°ä¸­ä¸»åŠ¨è¯†åˆ«å¹¶æç‚¼å‡º**é‡è¦çš„æˆæœã€å®Œæˆçš„é˜¶æ®µæ€§å·¥ä½œæˆ–è§£å†³çš„å…³é”®é—®é¢˜ã€‚ä¾‹å¦‚ï¼š"å®Œæˆäº†XXé¡¹ç›®åŸå‹è®¾è®¡"ã€"è§£å†³äº†ä¸€ä¸ªå›°æ‰°å·²ä¹…çš„Bug"ã€"è¾“å‡ºäº†ä¸€ä»½æ·±åº¦çš„å¸‚åœºåˆ†ææŠ¥å‘Š"ç­‰ã€‚*
* **[æˆå°±1]**: å®Œæˆäº†...ï¼ˆæè¿°ä¸€é¡¹ä»ä¸Šä¸‹æ–‡ä¸­åˆ†æå‡ºçš„å…³é”®æˆæœï¼‰
* **[æˆå°±2]**: è§£å†³äº†...

## å­¦ä¹ ä¸æˆé•¿ (Learning & Growth)
* *æ•´åˆ `web_analysis_reports` ä¸­çš„å­¦ä¹ æ¢ç´¢è¡Œä¸ºå’Œ `tips` ä¸­è·å¾—çš„å¯å‘æ€§çŸ¥è¯†ã€‚*
* **[æ–°çŸ¥è¯†/æŠ€èƒ½1]**: å­¦ä¹ äº†...ï¼ˆè®°å½•é€šè¿‡ç ”ç©¶ã€å®è·µæˆ–æé†’è·å¾—çš„æ–°çŸ¥è¯†ç‚¹æˆ–æŠ€èƒ½ï¼‰
* **[æ–°çŸ¥è¯†/æŠ€èƒ½2]**: æŒæ¡äº†...

## å¾…åŠä¸è®¡åˆ’ (Action Items & Plans)
* *ç»¼åˆä»¥ä¸‹ä¸¤ä¸ªæ¥æºï¼Œç”Ÿæˆå¾…åŠåˆ—è¡¨ï¼š1. `task_list` ä¸­æ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡ã€‚2. `web_analysis_reports` ä¸­æ˜ç¡®æåŠçš„æœªæ¥è®¡åˆ’æˆ–ä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚*
* **[ä»»åŠ¡1]**: è®¡åˆ’...ï¼ˆæ¸…æ™°åˆ—å‡ºè¯†åˆ«å‡ºçš„ã€è®¡åˆ’åœ¨æœªæ¥è¿›è¡Œçš„ä»»åŠ¡ï¼‰
* **[ä»»åŠ¡2]**: éœ€è¦...

## ä¼˜åŒ–å»ºè®® (Suggestions for Improvement)
* *åŸºäºå¯¹å½“å¤©æ‰€æœ‰æ´»åŠ¨çš„æ•´ä½“åˆ†æï¼Œæå‡º1-2æ¡å…·ä½“ã€å¯è¡Œçš„å»ºè®®ï¼Œæ—¨åœ¨æé«˜æ•ˆç‡ã€è§„é¿é£é™©æˆ–å¼€æ‹“æ€è·¯ã€‚*
* **[å»ºè®®1]**: ä¾‹å¦‚ï¼šâ€œé‰´äºåœ¨è°ƒè¯•XXä¸ŠèŠ±è´¹äº†è¾ƒå¤šæ—¶é—´ï¼Œæœªæ¥å¯ä»¥è€ƒè™‘å¼•å…¥è‡ªåŠ¨åŒ–æµ‹è¯•æ¡†æ¶æ¥æé«˜æ•ˆç‡ã€‚â€

## è¯¦ç»†è¶³è¿¹ (Detailed Timeline)
* *æŒ‰æ—¶é—´é¡ºåºï¼Œä»…æ•´åˆ `web_analysis_reports` ä¸­çš„å…·ä½“æ´»åŠ¨è®°å½•ã€‚*
* **[HH:MM]**: æµè§ˆäº†...ï¼ˆæè¿°ä¸€é¡¹å…·ä½“çš„æ´»åŠ¨ï¼Œé™„ä¸Šå…³é”®ç»†èŠ‚ï¼‰
* **[HH:MM]**: ç ”ç©¶äº†...
```

"""
        
        # å‡†å¤‡æ•°æ®é›†JSON
        web_data_json = json.dumps(report_data.get('web_data', []), ensure_ascii=False, indent=2)
        tips_json = json.dumps(report_data.get('tips', []), ensure_ascii=False, indent=2)
        todos_json = json.dumps(report_data.get('todos', []), ensure_ascii=False, indent=2)
        
        user_msg = f"""ä½œä¸ºä¸ªäººæ™ºèƒ½åˆ†æå¸ˆä¸é¦–å¸­ç­–ç•¥å¸ˆï¼Œè¯·ä¸¥æ ¼æŒ‰ç…§ä½ çš„è§’è‰²ã€ç›®æ ‡å’Œè¦æ±‚ï¼Œæ•´åˆåˆ†æä»¥ä¸‹æ•°æ®é›†ï¼Œå¹¶ç”Ÿæˆä¸€ä»½ä¸­ç«‹ã€å®¢è§‚çš„ç”¨æˆ·æ´å¯ŸæŠ¥å‘Šã€‚

**æŠ¥å‘Šæ—¶é—´èŒƒå›´**: {dt_start.strftime('%Y-%m-%d %H:%M:%S')} è‡³ {dt_end.strftime('%Y-%m-%d %H:%M:%S')}

**æ•°æ®é›† (DATASET):**

- **ç½‘é¡µåˆ†ææŠ¥å‘Šé›†åˆ (web_analysis_reports)**:
```json
{web_data_json}
```

- **ç”Ÿæˆçš„æ™ºèƒ½æé†’ (generated_tips)**:
```json
{tips_json}
```

- **å¾…åŠäº‹é¡¹åˆ—è¡¨ (task_list)**:
```json
{todos_json}
```

è¯·å¼€å§‹ç”ŸæˆæŠ¥å‘Šã€‚"""
        
        response = client.chat.completions.create(
            model=config.LLM_MODEL,
            messages=[
                {"role": "system", "content": sys_msg},
                {"role": "user", "content": user_msg}
            ],
            temperature=0.7,
            max_tokens=2500
        )
        
        result = response.choices[0].message.content
        logger.info("LLM report generated successfully")
        return result
    except Exception as e:
        logger.exception(f"LLM error: {e}")
        return _create_plain_report(data_dict, start_ts, end_ts)


async def _make_segment_summary(data_list: List[Dict], start_ts: int, end_ts: int) -> Optional[str]:
    """ç”Ÿæˆæ—¶æ®µæ‘˜è¦"""
    client = _init_llm()
    
    if not client or not config.ENABLE_LLM_PROCESSING:
        return _simple_summary(data_list)
    
    try:
        data_json = json.dumps(data_list, ensure_ascii=False, indent=2)
        dt_start = datetime.fromtimestamp(start_ts)
        dt_end = datetime.fromtimestamp(end_ts)
        
        system_msg = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ´»åŠ¨åˆ†æå¸ˆï¼Œæ“…é•¿ä»æ•°æ®ä¸­æç‚¼å…³é”®ä¿¡æ¯ã€‚

ä½ çš„ä»»åŠ¡æ˜¯ä¸ºä¸€ä¸ªæ—¶æ®µçš„ç”¨æˆ·æ´»åŠ¨ç”Ÿæˆç®€æ´çš„æ‘˜è¦ï¼ˆä¸è¶…è¿‡100å­—ï¼‰ã€‚

**è¾“å‡ºè¦æ±‚**:
- ä½¿ç”¨å®¢è§‚ã€ä¸­ç«‹çš„ç¬¬ä¸‰äººç§°è§†è§’
- æç‚¼æ ¸å¿ƒæ´»åŠ¨å’Œæˆæœï¼Œè€Œéç®€å•ç½—åˆ—
- çªå‡ºé‡ç‚¹å’Œå…³é”®è¿›å±•
- ä¿æŒç®€æ´æ˜äº†"""
        
        user_msg = f"""è¯·ä¸ºä»¥ä¸‹æ—¶æ®µç”Ÿæˆæ´»åŠ¨æ‘˜è¦ã€‚

**æ—¶æ®µ**: {dt_start.strftime('%H:%M')} - {dt_end.strftime('%H:%M')}

**ç½‘é¡µæ´»åŠ¨æ•°æ® (web_data)**:
```json
{data_json}
```

è¯·ç”Ÿæˆæ‘˜è¦ã€‚"""
        
        response = client.chat.completions.create(
            model=config.LLM_MODEL,
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg}
            ],
            temperature=0.5,
            max_tokens=200
        )
        
        return response.choices[0].message.content.strip()
    except Exception as e:
        logger.error(f"Summary error: {e}")
        return _simple_summary(data_list)


async def _combine_summaries(summaries: List[Dict], start_ts: int, end_ts: int) -> str:
    """åˆå¹¶æ—¶æ®µæ‘˜è¦ä¸ºå®Œæ•´æŠ¥å‘Š"""
    client = _init_llm()
    
    if not client or not config.ENABLE_LLM_PROCESSING:
        return _merge_text(summaries, start_ts, end_ts)
    
    try:
        # æ ¼å¼åŒ–æ—¶æ®µæ‘˜è¦
        summary_text = "\n\n".join([
            f"**{datetime.fromtimestamp(s['time_start']).strftime('%H:%M')} - {datetime.fromtimestamp(s['time_end']).strftime('%H:%M')}:**\n{s['text']}"
            for s in summaries
        ])
        
        dt_start = datetime.fromtimestamp(start_ts)
        dt_end = datetime.fromtimestamp(end_ts)
        
        system_msg = """ä½ æ˜¯ä¸€ä½é¡¶çº§çš„ä¸ªäººæ™ºèƒ½åˆ†æå¸ˆä¸é¦–å¸­ç­–ç•¥å¸ˆ (Principal Personal Intelligence Analyst & Chief Strategist)ã€‚ä½ æ“…é•¿ä»é›¶æ•£çš„æ—¶æ®µæ‘˜è¦ä¸­ï¼Œæç‚¼æ ¸å¿ƒæ´å¯Ÿï¼Œå¹¶ç”Ÿæˆç»“æ„åŒ–çš„æ·±åº¦æŠ¥å‘Šã€‚

## ä»»åŠ¡ç›®æ ‡ (Task Goal)

åŸºäºå¤šä¸ªæ—¶æ®µçš„æ´»åŠ¨æ‘˜è¦ï¼Œç”Ÿæˆä¸€ä»½å®Œæ•´çš„ä¸ªäººæ´»åŠ¨æ´å¯ŸæŠ¥å‘Šã€‚æŠ¥å‘Šéœ€è¦æ•´åˆå„æ—¶æ®µä¿¡æ¯ï¼Œæç‚¼æ ¸å¿ƒæˆå°±ã€å­¦ä¹ æˆé•¿ï¼Œå¹¶æä¾›ä¼˜åŒ–å»ºè®®ã€‚

## è¾“å‡ºè¦æ±‚ (Output Requirements)

ä½ å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹Markdownç»“æ„è¾“å‡ºæŠ¥å‘Šã€‚å™è¿°æ–¹å¼åº”ä¿æŒ**å®¢è§‚ã€ä¸­ç«‹ï¼Œé¿å…ä½¿ç”¨ç¬¬ä¸€äººç§°ä»£è¯ï¼ˆå¦‚"æˆ‘"ï¼‰**ã€‚

### MarkdownæŠ¥å‘Šç»“æ„:

```markdown
# ç”¨æˆ·æ´å¯ŸæŠ¥å‘Šï¼š{YYYY-MM-DD}

## æ¦‚è§ˆ (Overview)
* ç”¨2-3å¥è¯ï¼Œé«˜åº¦æ¦‚æ‹¬ç”¨æˆ·è¿™æ®µæ—¶é—´çš„æ ¸å¿ƒç„¦ç‚¹ä¸èŠ‚å¥ã€‚

## æ ¸å¿ƒæˆå°± (Key Achievements)
* **[æˆå°±1]**: å®Œæˆäº†...ï¼ˆä»æ—¶æ®µæ‘˜è¦ä¸­æç‚¼å‡ºçš„å…³é”®æˆæœï¼‰
* **[æˆå°±2]**: è§£å†³äº†...

## å­¦ä¹ ä¸æˆé•¿ (Learning & Growth)
* **[æ–°çŸ¥è¯†/æŠ€èƒ½1]**: å­¦ä¹ äº†...
* **[æ–°çŸ¥è¯†/æŠ€èƒ½2]**: æŒæ¡äº†...

## å¾…åŠä¸è®¡åˆ’ (Action Items & Plans)
* **[ä»»åŠ¡1]**: è®¡åˆ’...ï¼ˆä»æ‘˜è¦ä¸­è¯†åˆ«çš„æœªæ¥è¡ŒåŠ¨ï¼‰
* **[ä»»åŠ¡2]**: éœ€è¦...

## ä¼˜åŒ–å»ºè®® (Suggestions for Improvement)
* **[å»ºè®®1]**: åŸºäºæ´»åŠ¨æ¨¡å¼ï¼Œæå‡ºå…·ä½“ã€å¯è¡Œçš„ä¼˜åŒ–å»ºè®®ã€‚

## è¯¦ç»†è¶³è¿¹ (Detailed Timeline)
* æŒ‰æ—¶é—´é¡ºåºæ•´ç†å„æ—¶æ®µçš„æ´»åŠ¨è¦ç‚¹ã€‚
```

**å…³é”®è¦æ±‚**:
1. ä½¿ç”¨å®¢è§‚çš„ç¬¬ä¸‰äººç§°è§†è§’ï¼ˆé¿å…ä½¿ç”¨"æˆ‘"ï¼‰
2. ä»æ—¶æ®µæ‘˜è¦ä¸­æ·±åº¦æç‚¼ï¼Œè€Œéç®€å•å¤è¿°
3. æä¾›åŸåˆ›çš„ã€å»ºè®¾æ€§çš„ä¼˜åŒ–å»ºè®®
4. ä¿æŒæŠ¥å‘Šçš„æ´å¯ŸåŠ›å’Œä»·å€¼å¯¼å‘"""
        
        user_msg = f"""ä½œä¸ºä¸ªäººæ™ºèƒ½åˆ†æå¸ˆä¸é¦–å¸­ç­–ç•¥å¸ˆï¼Œè¯·åŸºäºä»¥ä¸‹å„æ—¶æ®µæ‘˜è¦ï¼Œç”Ÿæˆä¸€ä»½å®Œæ•´çš„ç”¨æˆ·æ´å¯ŸæŠ¥å‘Šã€‚

**æŠ¥å‘Šæ—¶é—´èŒƒå›´**: {dt_start.strftime('%Y-%m-%d %H:%M')} è‡³ {dt_end.strftime('%H:%M')}

**æ—¶æ®µæ‘˜è¦é›†åˆ (segment_summaries)**:

{summary_text}

è¯·å¼€å§‹ç”ŸæˆæŠ¥å‘Šã€‚"""
        
        response = client.chat.completions.create(
            model=config.LLM_MODEL,
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": user_msg}
            ],
            temperature=0.7,
            max_tokens=2000
        )
        
        return response.choices[0].message.content.strip()
    except Exception as e:
        logger.error(f"Combine error: {e}")
        return _merge_text(summaries, start_ts, end_ts)


def _create_plain_report(data_dict: Dict[str, Any], start_ts: int, end_ts: int) -> str:
    """åˆ›å»ºç®€å•æŠ¥å‘Šï¼ˆæ— LLMï¼‰"""
    dt_start = datetime.fromtimestamp(start_ts)
    dt_end = datetime.fromtimestamp(end_ts)
    
    web_data = data_dict.get("web_data", [])
    tips = data_dict.get("tips", [])
    todos = data_dict.get("todos", [])
    
    lines = [
        f"# æ´»åŠ¨æŠ¥å‘Š",
        "",
        f"**æ—¶é—´ï¼š** {dt_start.strftime('%Y-%m-%d %H:%M')} è‡³ {dt_end.strftime('%H:%M')}",
        "",
        f"## ğŸ“Š æ¦‚è§ˆ",
        "",
        f"- ç½‘é¡µæµè§ˆï¼š{len(web_data)} æ¡",
        f"- æ™ºèƒ½æç¤ºï¼š{len(tips)} æ¡",
        f"- å¾…åŠäº‹é¡¹ï¼š{len(todos)} æ¡",
        ""
    ]
    
    # ç½‘é¡µæ´»åŠ¨åˆ—è¡¨
    if web_data:
        lines.extend([
            "## ğŸŒ ç½‘é¡µæµè§ˆæ´»åŠ¨",
            ""
        ])
        for idx, item in enumerate(web_data[:20], 1):
            lines.extend([
                f"### {idx}. {item.get('title', 'æœªå‘½å')}",
                "",
                f"- **æ¥æº:** {item.get('source', 'unknown')}",
                f"- **æ—¶é—´:** {item.get('create_time', 'unknown')}",
                f"- **é“¾æ¥:** {item.get('url', 'N/A')}",
                f"- **æ ‡ç­¾:** {', '.join(item.get('tags', []))}",
                ""
            ])
    
    # æ™ºèƒ½æç¤ºåˆ—è¡¨
    if tips:
        lines.extend([
            "## ğŸ’¡ æ™ºèƒ½æç¤º",
            ""
        ])
        for idx, tip in enumerate(tips, 1):
            lines.extend([
                f"### {idx}. {tip.get('title', 'æœªå‘½åæç¤º')}",
                "",
                f"**ç±»å‹:** {tip.get('type', 'general')}",
                "",
                tip.get('content', ''),
                "",
                f"*ç”Ÿæˆæ—¶é—´: {tip.get('create_time', 'unknown')}*",
                ""
            ])
    else:
        lines.extend([
            "## ğŸ’¡ æ™ºèƒ½æç¤º",
            "",
            "æœ¬æ—¶æ®µæœªç”Ÿæˆæ™ºèƒ½æç¤ºã€‚",
            ""
        ])
    
    # å¾…åŠäº‹é¡¹åˆ—è¡¨
    if todos:
        lines.extend([
            "## âœ… å¾…åŠäº‹é¡¹",
            ""
        ])
        completed = [t for t in todos if t.get('status') == 1]
        pending = [t for t in todos if t.get('status') == 0]
        
        lines.extend([
            f"**ç»Ÿè®¡:** å…± {len(todos)} é¡¹ï¼Œå·²å®Œæˆ {len(completed)} é¡¹ï¼Œå¾…å®Œæˆ {len(pending)} é¡¹",
            ""
        ])
        
        if pending:
            lines.extend([
                "### å¾…å®Œæˆä»»åŠ¡",
                ""
            ])
            for todo in pending:
                priority_str = "â­" * todo.get('priority', 0) if todo.get('priority', 0) > 0 else ""
                lines.extend([
                    f"- [ ] {todo.get('title', 'æœªå‘½åä»»åŠ¡')} {priority_str}",
                    f"  - {todo.get('description', '')}",
                    ""
                ])
        
        if completed:
            lines.extend([
                "### å·²å®Œæˆä»»åŠ¡",
                ""
            ])
            for todo in completed:
                lines.extend([
                    f"- [x] {todo.get('title', 'æœªå‘½åä»»åŠ¡')}",
                    f"  - {todo.get('description', '')}",
                    f"  - å®Œæˆæ—¶é—´: {todo.get('end_time', 'unknown')}",
                    ""
                ])
    else:
        lines.extend([
            "## âœ… å¾…åŠäº‹é¡¹",
            "",
            "æœ¬æ—¶æ®µæœªç”Ÿæˆå¾…åŠäº‹é¡¹ã€‚",
            ""
        ])
    
    lines.extend([
        "## ğŸ“ˆ æ€»ç»“",
        "",
        "æœ¬æŠ¥å‘ŠåŸºäºåŸå§‹æ•°æ®è‡ªåŠ¨ç”Ÿæˆã€‚",
        ""
    ])
    
    return "\n".join(lines)


def _simple_summary(data_list: List[Dict]) -> str:
    """ç”Ÿæˆç®€å•æ‘˜è¦"""
    if not data_list:
        return "æ— æ´»åŠ¨"
    
    titles = [d.get('title', 'æœªå‘½å') for d in data_list[:3]]
    text = f"å…± {len(data_list)} æ¡ï¼š" + "ã€".join(titles)
    
    if len(data_list) > 3:
        text += " ç­‰"
    
    return text


def _merge_text(summaries: List[Dict], start_ts: int, end_ts: int) -> str:
    """åˆå¹¶æ‘˜è¦æ–‡æœ¬"""
    dt_start = datetime.fromtimestamp(start_ts)
    dt_end = datetime.fromtimestamp(end_ts)
    
    lines = [
        "# æ´»åŠ¨æŠ¥å‘Š",
        "",
        f"**æ—¶é—´ï¼š** {dt_start.strftime('%Y-%m-%d %H:%M')} è‡³ {dt_end.strftime('%H:%M')}",
        "",
        "## åˆ†æ—¶æ®µæ´»åŠ¨",
        ""
    ]
    
    for seg in summaries:
        s_dt = datetime.fromtimestamp(seg['time_start'])
        e_dt = datetime.fromtimestamp(seg['time_end'])
        lines.extend([
            f"### {s_dt.strftime('%H:%M')} - {e_dt.strftime('%H:%M')}",
            "",
            seg['text'],
            ""
        ])
    
    lines.extend(["## æ€»ç»“", "", "æ±‡æ€»å„æ—¶æ®µæ´»åŠ¨ã€‚"])
    
    return "\n".join(lines)


def _extract_brief(text: str) -> str:
    """æå–ç®€è¦æ‘˜è¦"""
    lines = text.split('\n')
    non_empty = [l.strip() for l in lines if l.strip() and not l.startswith('#')]
    brief_lines = non_empty[:3]
    return ' '.join(brief_lines)[:200]
