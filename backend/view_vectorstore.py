"""
æŸ¥çœ‹å‘é‡æ•°æ®åº“å†…å®¹çš„å·¥å…·è„šæœ¬
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
_backend_dir = Path(__file__).parent
if str(_backend_dir) not in sys.path:
    sys.path.insert(0, str(_backend_dir))

import json
from utils.vectorstore import get_collection, get_chroma_client
from utils.helpers import get_logger
import config

logger = get_logger(__name__)


def view_vectorstore(limit: int = None, show_content: bool = True):
    """
    æŸ¥çœ‹å‘é‡æ•°æ®åº“ä¸­çš„æ‰€æœ‰å†…å®¹
    
    Args:
        limit: é™åˆ¶æ˜¾ç¤ºçš„æ•°é‡ï¼ŒNone è¡¨ç¤ºæ˜¾ç¤ºå…¨éƒ¨
        show_content: æ˜¯å¦æ˜¾ç¤ºæ–‡æ¡£å†…å®¹ï¼ˆå†…å®¹å¯èƒ½å¾ˆé•¿ï¼‰
    """
    try:
        collection = get_collection()
        
        # è·å–æ€»æ•°
        count = collection.count()
        print(f"\n{'='*60}")
        print(f"å‘é‡æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯")
        print(f"{'='*60}")
        print(f"é›†åˆåç§°: {config.CHROMA_COLLECTION_NAME}")
        print(f"æ€»æ–‡æ¡£æ•°: {count}")
        print(f"{'='*60}\n")
        
        if count == 0:
            print("å‘é‡æ•°æ®åº“ä¸ºç©ºï¼Œæ²¡æœ‰å­˜å‚¨ä»»ä½•å†…å®¹ã€‚")
            return
        
        # è·å–æ‰€æœ‰æ•°æ®
        if limit:
            # ä½¿ç”¨ peek æŸ¥çœ‹å‰ N æ¡
            results = collection.peek(limit=limit)
            print(f"æ˜¾ç¤ºå‰ {limit} æ¡è®°å½•:\n")
        else:
            # è·å–æ‰€æœ‰æ•°æ®
            results = collection.get()
            print(f"æ˜¾ç¤ºæ‰€æœ‰ {count} æ¡è®°å½•:\n")
        
        # è§£æç»“æœ
        ids = results.get('ids', [])
        documents = results.get('documents', [])
        metadatas = results.get('metadatas', [])
        
        if not ids:
            print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è®°å½•ã€‚")
            return
        
        # æŒ‰ web_data_id åˆ†ç»„ç»Ÿè®¡
        web_data_stats = {}
        for i, metadata in enumerate(metadatas):
            web_data_id = metadata.get('web_data_id')
            if web_data_id:
                if web_data_id not in web_data_stats:
                    web_data_stats[web_data_id] = {
                        'title': metadata.get('title', 'Unknown'),
                        'url': metadata.get('url', ''),
                        'chunks_count': 0,
                        'chunks': []
                    }
                web_data_stats[web_data_id]['chunks_count'] += 1
                web_data_stats[web_data_id]['chunks'].append({
                    'id': ids[i],
                    'chunk_index': metadata.get('chunk_index'),
                    'content_preview': documents[i][:100] if documents[i] else ''
                })
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print(f"\n{'='*60}")
        print(f"æŒ‰ç½‘é¡µæ•°æ®åˆ†ç»„ç»Ÿè®¡")
        print(f"{'='*60}")
        for web_data_id, stats in sorted(web_data_stats.items()):
            print(f"\nğŸ“„ Web Data ID: {web_data_id}")
            print(f"   æ ‡é¢˜: {stats['title']}")
            print(f"   URL: {stats['url']}")
            print(f"   åˆ†å—æ•°é‡: {stats['chunks_count']}")
        
        # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        if show_content:
            print(f"\n{'='*60}")
            print(f"è¯¦ç»†ä¿¡æ¯")
            print(f"{'='*60}")
            
            for i, doc_id in enumerate(ids):
                print(f"\n--- è®°å½• {i+1}/{len(ids)} ---")
                print(f"ID: {doc_id}")
                
                if i < len(metadatas):
                    metadata = metadatas[i]
                    print(f"å…ƒæ•°æ®:")
                    for key, value in metadata.items():
                        if key not in ['tags']:  # tags æ˜¯ JSON å­—ç¬¦ä¸²ï¼Œç¨åå•ç‹¬å¤„ç†
                            print(f"  {key}: {value}")
                    
                    # å¤„ç† tags
                    if 'tags' in metadata:
                        try:
                            tags = json.loads(metadata['tags'])
                            print(f"  tags: {tags}")
                        except:
                            print(f"  tags: {metadata['tags']}")
                
                if i < len(documents) and documents[i]:
                    content = documents[i]
                    if show_content:
                        print(f"å†…å®¹é¢„è§ˆ (å‰200å­—ç¬¦):")
                        print(f"  {content[:200]}...")
                        if len(content) > 200:
                            print(f"  ... (æ€»é•¿åº¦: {len(content)} å­—ç¬¦)")
                    else:
                        print(f"å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        
    except Exception as e:
        logger.exception(f"æŸ¥çœ‹å‘é‡æ•°æ®åº“æ—¶å‡ºé”™: {e}")
        print(f"\né”™è¯¯: {e}")


def view_by_web_data_id(web_data_id: int):
    """
    æŸ¥çœ‹ç‰¹å®š web_data_id çš„æ‰€æœ‰åˆ†å—
    
    Args:
        web_data_id: ç½‘é¡µæ•°æ®ID
    """
    try:
        collection = get_collection()
        
        # æ ¹æ® web_data_id è¿‡æ»¤
        results = collection.get(
            where={"web_data_id": web_data_id}
        )
        
        ids = results.get('ids', [])
        documents = results.get('documents', [])
        metadatas = results.get('metadatas', [])
        
        if not ids:
            print(f"\næœªæ‰¾åˆ° web_data_id={web_data_id} çš„ä»»ä½•è®°å½•ã€‚")
            return
        
        print(f"\n{'='*60}")
        print(f"Web Data ID: {web_data_id}")
        print(f"æ‰¾åˆ° {len(ids)} ä¸ªåˆ†å—")
        print(f"{'='*60}\n")
        
        # æ˜¾ç¤ºæ ‡é¢˜å’ŒURLï¼ˆä»ç¬¬ä¸€ä¸ªåˆ†å—çš„å…ƒæ•°æ®ä¸­è·å–ï¼‰
        if metadatas:
            first_meta = metadatas[0]
            print(f"æ ‡é¢˜: {first_meta.get('title', 'Unknown')}")
            print(f"URL: {first_meta.get('url', '')}")
            print()
        
        # æŒ‰ chunk_index æ’åºæ˜¾ç¤º
        indexed_chunks = []
        for i, metadata in enumerate(metadatas):
            chunk_index = metadata.get('chunk_index', i)
            indexed_chunks.append({
                'index': chunk_index,
                'id': ids[i],
                'content': documents[i] if i < len(documents) else '',
                'metadata': metadata
            })
        
        indexed_chunks.sort(key=lambda x: x['index'])
        
        for chunk in indexed_chunks:
            print(f"\n--- åˆ†å— {chunk['index']} ---")
            print(f"ID: {chunk['id']}")
            print(f"å†…å®¹ (å‰300å­—ç¬¦):")
            print(f"{chunk['content'][:300]}...")
            if len(chunk['content']) > 300:
                print(f"... (æ€»é•¿åº¦: {len(chunk['content'])} å­—ç¬¦)")
        
    except Exception as e:
        logger.exception(f"æŸ¥çœ‹ç‰¹å®š web_data_id æ—¶å‡ºé”™: {e}")
        print(f"\né”™è¯¯: {e}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="æŸ¥çœ‹å‘é‡æ•°æ®åº“å†…å®¹")
    parser.add_argument(
        "--limit",
        type=int,
        default=None,
        help="é™åˆ¶æ˜¾ç¤ºçš„æ•°é‡ï¼ˆé»˜è®¤æ˜¾ç¤ºå…¨éƒ¨ï¼‰"
    )
    parser.add_argument(
        "--no-content",
        action="store_true",
        help="ä¸æ˜¾ç¤ºæ–‡æ¡£å†…å®¹ï¼ˆåªæ˜¾ç¤ºå…ƒæ•°æ®ï¼‰"
    )
    parser.add_argument(
        "--web-data-id",
        type=int,
        default=None,
        help="æŸ¥çœ‹ç‰¹å®š web_data_id çš„æ‰€æœ‰åˆ†å—"
    )
    
    args = parser.parse_args()
    
    if args.web_data_id:
        view_by_web_data_id(args.web_data_id)
    else:
        view_vectorstore(limit=args.limit, show_content=not args.no_content)
