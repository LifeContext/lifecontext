"""
LLM ç­–ç•¥æ¨¡å— - æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†å’Œå·¥å…·è°ƒç”¨ç­–ç•¥
"""
import asyncio
import sys
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from enum import Enum
from dataclasses import dataclass
from datetime import datetime
_backend_dir = Path(__file__).parent.parent
if str(_backend_dir) not in sys.path:
    sys.path.insert(0, str(_backend_dir))
from utils.helpers import get_logger
from utils.db import get_todos
import config
from tools.base import ToolsExecutor
logger = get_logger(__name__)

@dataclass
class Intent:
    """ç”¨æˆ·æ„å›¾"""
    query: str
    type: str = "general"  # general, question, task, search, etc.
    metadata: Dict[str, Any] = None
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}
@dataclass
class ContextItem:
    """ä¸Šä¸‹æ–‡é¡¹"""
    id: str
    content: str
    source: str  # retrieval, entity, web_search, etc.
    metadata: Dict[str, Any] = None
    relevance_score: float = 0.0
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}
class ContextCollection:
    """ä¸Šä¸‹æ–‡é›†åˆ"""
    def __init__(self):
        self.items: List[ContextItem] = []
    
    def add(self, item: ContextItem):
        self.items.append(item)
    
    def get_all(self) -> List[ContextItem]:
        return self.items
    
    def get_by_source(self, source: str) -> List[ContextItem]:
        return [item for item in self.items if item.source == source]
    
    def clear(self):
        self.items.clear()
class ContextSufficiency(str, Enum):
    """ä¸Šä¸‹æ–‡å……åˆ†æ€§è¯„ä¼°ç»“æœ"""
    SUFFICIENT = "sufficient"  # è¶³å¤Ÿå›ç­”
    INSUFFICIENT = "insufficient"  # ä¸å¤Ÿï¼Œéœ€è¦æ›´å¤šå·¥å…·è°ƒç”¨
    UNKNOWN = "unknown"  # æ— æ³•ç¡®å®š

# âœ… æ ¸å¿ƒé…ç½®
from llama_index.core import (
    Settings,
    SimpleDirectoryReader,
    VectorStoreIndex,
)

# âœ… OpenAI å…¼å®¹æ¨¡å‹ï¼ˆå®˜æ–¹OpenAI æˆ– OpenAI-likeæ¥å£ï¼‰
from llama_index.llms.openai_like import OpenAILike
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.core.storage import StorageContext
from llama_index.core.storage.docstore import SimpleDocumentStore
from llama_index.core.storage.index_store import SimpleIndexStore
# âœ… å‘é‡ä¸å›¾å­˜å‚¨
from llama_index.core.vector_stores import SimpleVectorStore
from llama_index.core.graph_stores import SimpleGraphStore
# âœ… è¯„ä¼°ä¸åå¤„ç†
from llama_index.core.evaluation import RelevancyEvaluator
from llama_index.core.postprocessor import SimilarityPostprocessor
class LlamaIndexContextStrategy:
    """
    å¤ç”¨ LLMContextStrategy çš„æ–¹æ³•ç­¾åï¼Œä½†å†…éƒ¨æ¢æˆ LlamaIndexã€‚
    å¤–éƒ¨ï¼ˆagent.pyï¼‰ä»ç„¶é€šè¿‡ analyze_and_plan_tools / evaluate_sufficiency ç­‰æ¥å£ä½¿ç”¨ã€‚
    """
    
    def __init__(self):
        # 1. åˆå§‹åŒ– LlamaIndex å…¨å±€è®¾ç½®
        Settings.llm = OpenAILike(
            model=config.LLM_MODEL,                  # ä¾‹å¦‚è±†åŒ…æ¨¡å‹
            api_base=config.LLM_BASE_URL,            # OpenAI å…¼å®¹åœ°å€
            api_key=config.LLM_API_KEY,
            is_chat_model=True,  # ç¡®ä¿ä½¿ç”¨ chat completions ç«¯ç‚¹
        )
        Settings.embed_model = OpenAIEmbedding(
            model_name=config.EMBEDDING_MODEL,
            api_base=config.EMBEDDING_BASE_URL,
            api_key=config.EMBEDDING_API_KEY,
            embed_batch_size=32 
        )
        # 2. å‡†å¤‡æŒä¹…åŒ–å­˜å‚¨ï¼ˆå‘é‡åº“ / Memoryï¼‰
        self.storage_context = self._build_storage_context(
            config.CHROMA_PERSIST_DIR
        )
        # 3. æ„å»ºå…¨å±€ç´¢å¼•ï¼ˆå¯æŒ‰éœ€åŠ è½½å·²æœ‰æ–‡æ¡£ï¼‰
        self.global_index = self._load_global_index()
        # 4. æ„å»ºè‡ªå·±çš„å·¥å…·ç±»å‹ä¾›æ¨¡å‹ä½¿ç”¨
        self.tools_executor = ToolsExecutor()
        # ä»å·¥å…·æ‰§è¡Œå™¨è·å–æ‰€æœ‰å·¥å…·å®šä¹‰
        self.all_tools = self.tools_executor.get_function_definitions()
        self.profile_tools = [
            tool.get_function_definition() 
            for tool in self.tools_executor.get_all_tools()
            if tool.get_metadata().get("category") == "profile_tools"
        ]
        self.operation_tools = [
            tool.get_function_definition() 
            for tool in self.tools_executor.get_all_tools()
            if tool.get_metadata().get("category") == "operation_tools"
        ]
        self.context_evaluator = RelevancyEvaluator(llm=Settings.llm)
        self.similarity_filter = SimilarityPostprocessor(
            threshold=0.4,          # ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œå¯æŒ‰éœ€è¦è°ƒæ•´
            top_k=6,                # æœ€å¤šä¿ç•™å¤šå°‘æ¡
        )
        self.session_indices: dict[str, VectorStoreIndex] = {}
        self.session_root = Path(config.CHROMA_PERSIST_DIR)
        self.session_root.mkdir(parents=True, exist_ok=True)
    def _load_global_index(self) -> VectorStoreIndex:
        """ä»ç£ç›˜åŠ è½½æˆ–åˆå§‹åŒ–ä¸€ä¸ª VectorStoreIndex"""
        try:
            # å°è¯•ä»æ–‡æ¡£åˆ›å»º
            try:
                if config.CHROMA_PERSIST_DIR.exists():
                    docs = SimpleDirectoryReader(config.CHROMA_PERSIST_DIR).load_data()
                    if docs:
                        return VectorStoreIndex.from_documents(
                            docs,
                            storage_context=self.storage_context,
                        )
            except Exception as e:
                logger.debug(f"Failed to load documents: {e}")
            
            # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œåˆ›å»ºä¸€ä¸ªç©ºçš„ç´¢å¼•
            from llama_index.core.schema import Document
            empty_doc = Document(text="", metadata={})
            return VectorStoreIndex.from_documents(
                [empty_doc],
                storage_context=self.storage_context,
            )
        except ValueError as exc:
            message = str(exc)
            if "No existing llama_index.core.vector_stores" in message or "One of nodes, objects, or index_struct must be provided" in message:
                logger.info("No persisted global index found, creating fresh one")
                fresh_context = StorageContext.from_defaults(
                    persist_dir=config.CHROMA_PERSIST_DIR
                )
                # åˆ›å»ºä¸€ä¸ªç©ºçš„ç´¢å¼•
                from llama_index.core.schema import Document
                empty_doc = Document(text="", metadata={})
                return VectorStoreIndex.from_documents(
                    [empty_doc],
                    storage_context=fresh_context,
                )
            raise

    def _get_context_summary(self, context: ContextCollection) -> str:
        """
        ç”Ÿæˆä¸Šä¸‹æ–‡æ‘˜è¦ï¼Œç”¨äºå·¥å…·è°ƒç”¨æˆ–agentè§„åˆ’
        
        Args:
            context: ä¸Šä¸‹æ–‡é›†åˆ
            
        Returns:
            ä¸Šä¸‹æ–‡æ‘˜è¦å­—ç¬¦ä¸²
        """
        if not context or not context.items:
            return "æ— å†å²ä¸Šä¸‹æ–‡ã€‚"
        
        try:
            # ç®€å•æ±‡æ€»ï¼šæŒ‰æ¥æºåˆ†ç»„ï¼Œæ˜¾ç¤ºå…³é”®ä¿¡æ¯
            summary_parts = []
            by_source = {}
            
            for item in context.items:
                source = item.source or "unknown"
                if source not in by_source:
                    by_source[source] = []
                by_source[source].append(item)
            
            for source, items in by_source.items():
                summary_parts.append(f"\n[{source}] ({len(items)} é¡¹):")
                for item in items[:3]:  # åªæ˜¾ç¤ºå‰3é¡¹
                    content_preview = item.content[:100] + "..." if len(item.content) > 100 else item.content
                    summary_parts.append(f"  - {content_preview}")
                if len(items) > 3:
                    summary_parts.append(f"  ... è¿˜æœ‰ {len(items) - 3} é¡¹")
            
            return "\n".join(summary_parts) if summary_parts else "æ— æœ‰æ•ˆä¸Šä¸‹æ–‡å†…å®¹ã€‚"
        except Exception as e:
            logger.warning(f"Failed to generate context summary: {e}")
            return f"æ— æ³•ç”Ÿæˆæ‘˜è¦: {e}"

    async def analyze_and_plan_tools(
        self,
        intent,
        existing_context,
        iteration: int = 1,
    ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """
        ä½¿ç”¨ OpenAI function calling æ¥åˆ†æå’Œè§„åˆ’å·¥å…·è°ƒç”¨
        ä¸ä¾èµ– LlamaIndex Agentï¼Œç›´æ¥ä½¿ç”¨ OpenAI APIï¼Œé¿å…äº‹ä»¶å¾ªç¯é—®é¢˜
        """
        try:
            from utils.llm import get_openai_client
            import json
            
            client = get_openai_client()
            if not client:
                raise RuntimeError("LLM å®¢æˆ·ç«¯ä¸å¯ç”¨")
            
            # 1. è·å–æ‰€æœ‰å¯ç”¨å·¥å…·çš„å‡½æ•°å®šä¹‰
            available_tools = self.tools_executor.get_function_definitions()
            
            # 2. æ•´ç†å·²æœ‰ä¸Šä¸‹æ–‡
            context_summary = self._get_context_summary(existing_context)
            
            # 3. æ„å»ºç³»ç»Ÿæç¤ºè¯
            system_prompt = (
                "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè´Ÿè´£åˆ†æç”¨æˆ·æŸ¥è¯¢å¹¶å†³å®šéœ€è¦è°ƒç”¨å“ªäº›å·¥å…·æ¥è·å–ä¿¡æ¯ã€‚\n"
                "æ ¹æ®ç”¨æˆ·çš„é—®é¢˜ï¼Œé€‰æ‹©åˆé€‚çš„å·¥å…·æ¥æ£€ç´¢ç›¸å…³ä¿¡æ¯ã€‚\n"
                "å¦‚æœç”¨æˆ·çš„é—®é¢˜æ¶‰åŠæ—¶é—´ã€è®¡åˆ’ã€å¾…åŠäº‹é¡¹ã€ä»»åŠ¡ç­‰ï¼Œåº”è¯¥è°ƒç”¨ get_user_profile å·¥å…·æ¥æ£€ç´¢ç›¸å…³çš„å¾…åŠäº‹é¡¹ã€‚\n"
                "å¦‚æœç”¨æˆ·çš„é—®é¢˜æ¶‰åŠæç¤ºã€å»ºè®®ç­‰ï¼Œåº”è¯¥è°ƒç”¨ get_user_profile å·¥å…·æ¥æ£€ç´¢ç›¸å…³çš„æç¤ºä¿¡æ¯ã€‚\n"
                "å¦‚æœç”¨æˆ·çš„é—®é¢˜éœ€è¦æœç´¢ç½‘ç»œä¿¡æ¯ï¼Œåº”è¯¥è°ƒç”¨ web_search å·¥å…·ã€‚\n"
                "åªæœ‰åœ¨ç¡®å®éœ€è¦é¢å¤–ä¿¡æ¯æ—¶æ‰è°ƒç”¨å·¥å…·ï¼Œå¦‚æœå·²æœ‰ä¸Šä¸‹æ–‡è¶³å¤Ÿå›ç­”é—®é¢˜ï¼Œåˆ™ä¸éœ€è¦è°ƒç”¨å·¥å…·ã€‚"
            )
            
            # 4. æ„å»ºç”¨æˆ·æç¤ºè¯
            user_prompt = f"ç”¨æˆ·é—®é¢˜: {intent.query}\n\n"
            
            if context_summary:
                user_prompt += f"å·²æœ‰ä¸Šä¸‹æ–‡ä¿¡æ¯:\n{context_summary}\n\n"
            
            user_prompt += (
                f"è¿™æ˜¯ç¬¬ {iteration} è½®åˆ†æã€‚è¯·æ ¹æ®ç”¨æˆ·é—®é¢˜ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·æ¥è·å–æ›´å¤šä¿¡æ¯ã€‚\n"
                "å¦‚æœéœ€è¦è°ƒç”¨å·¥å…·ï¼Œè¯·ä½¿ç”¨ function calling åŠŸèƒ½ã€‚"
            )
            
            # 5. è°ƒç”¨ LLMï¼Œå¯ç”¨ function calling
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
            
            logger.info(f"Analyzing query for tool planning: {intent.query[:100]}...")
            logger.debug(f"Available tools: {[t['function']['name'] for t in available_tools]}")
            
            response = client.chat.completions.create(
                model=config.LLM_MODEL,
                messages=messages,
                tools=available_tools if available_tools else None,
                tool_choice="auto",  # è®©æ¨¡å‹è‡ªåŠ¨å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·
                temperature=0.3,  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¨³å®šçš„å·¥å…·è°ƒç”¨å†³ç­–
            )
            
            # 6. æå–å·¥å…·è°ƒç”¨å’Œå“åº”æ¶ˆæ¯
            tool_calls: List[Dict[str, Any]] = []
            analysis_message = {"content": ""}
            
            message = response.choices[0].message
            
            # æå–æ–‡æœ¬å“åº”
            if message.content:
                analysis_message["content"] = message.content
            
            # æå–å·¥å…·è°ƒç”¨
            if message.tool_calls:
                for idx, tool_call in enumerate(message.tool_calls):
                    function_name = tool_call.function.name
                    try:
                        # è§£æå‚æ•°
                        arguments = json.loads(tool_call.function.arguments) if tool_call.function.arguments else {}
                    except json.JSONDecodeError:
                        logger.warning(f"Failed to parse tool arguments for {function_name}: {tool_call.function.arguments}")
                        arguments = {}
                    
                    tool_calls.append({
                        "id": tool_call.id or f"call-{idx}",
                        "function_name": function_name,
                        "arguments": arguments,
                    })
                    logger.info(f"Tool call planned: {function_name} with args: {arguments}")
            
            # 7. å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè®°å½•åˆ†æç»“æœ
            if not tool_calls:
                analysis_content = analysis_message.get("content", "")
                if not analysis_content:
                    analysis_message["content"] = "åˆ†æå®Œæˆï¼Œå½“å‰ä¸Šä¸‹æ–‡ä¿¡æ¯è¶³å¤Ÿå›ç­”é—®é¢˜ï¼Œæ— éœ€è°ƒç”¨å·¥å…·ã€‚"
                logger.info("No tool calls planned. Analysis: %s", analysis_message["content"][:200])
            
            logger.info(f"Tool planning completed: {len(tool_calls)} tool calls planned")
            return tool_calls, analysis_message
            
        except Exception as e:
            logger.exception(f"analyze_and_plan_tools error: {e}")
            return [], {"content": f"å·¥å…·è§„åˆ’å¤±è´¥: {e}"}
    async def execute_tool_calls_parallel(
        self,
        tool_calls: list[dict[str, Any]]
    ) -> list[ContextItem]:
        """
        å¹¶å‘æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼ˆå¤ç”¨ç°æœ‰ ToolsExecutorï¼‰
        tool_calls ç»“æ„ä¸ä¹‹å‰ä¿æŒä¸€è‡´ï¼š
        [
            {
                "id": "...",
                "function_name": "get_user_profile",
                "arguments": {...}
            },
            ...
        ]
        """
        if not tool_calls:
            return []
        tasks: list[tuple[str, str, asyncio.Task]] = []
        for call in tool_calls:
            fn = call["function_name"]
            args = call.get("arguments", {})
            task = self.tools_executor.run_async(fn, **args)
            tasks.append((fn, call.get("id", ""), asyncio.create_task(task)))
        results = await asyncio.gather(
            *[t for _, _, t in tasks],
            return_exceptions=True
        )
        context_items: list[ContextItem] = []
        for (fn, call_id, _), result in zip(tasks, results):
            if isinstance(result, Exception):
                logger.error(f"Tool {fn} failed: {result}")
                continue
            items = self._convert_tool_result_to_context_items(fn, result, call_id)
            context_items.extend(items)
        logger.info(f"Executed {len(tool_calls)} tool calls, got {len(context_items)} context items")
        
        # è¾“å‡ºè½¬æ¢åçš„ context items çš„è¯¦ç»†ä¿¡æ¯
        for idx, item in enumerate(context_items):
            logger.info(
                f"  Converted item {idx+1}: id={item.id}, source={item.source}, "
                f"relevance_score={item.relevance_score:.4f}, "
                f"metadata={item.metadata}, content_preview={item.content[:60]}..."
            )
        
        return context_items

    def _convert_tool_result_to_context_items(
        self,
        tool_name: str,
        result: Any,
        call_id: str,
    ) -> List[ContextItem]:
        """å°†å·¥å…·è¿”å›ç»“æœç»Ÿä¸€è½¬æ¢ä¸º ContextItem åˆ—è¡¨ã€‚"""
        if tool_name == "get_user_profile" and isinstance(result, dict):
            context_items: List[ContextItem] = []
            context_search = result.get("context_search", {}) or {}
            for section in ["todos","tasks", "tips", "memories", "pages"]:
                entries = context_search.get(section, []) or []
                for idx, entry in enumerate(entries):
                    if isinstance(entry, dict):
                        raw_metadata = entry.get("metadata") or {}
                        metadata = dict(raw_metadata) if isinstance(raw_metadata, dict) else {}
                        metadata["context_type"] = section
                        content = entry.get("content") or ""
                        relevance = float(entry.get("relevance_score", 0.0))
                    else:
                        metadata = {"context_type": section}
                        content = str(entry)
                        relevance = 0.0

                    context_items.append(
                        ContextItem(
                            id=f"{call_id or tool_name}_{section}_{idx}",
                            content=content,
                            source="user_profile",
                            metadata=metadata,
                            relevance_score=relevance,
                        )
                    )

            # å¦‚æœæœ‰ä¸Šä¸‹æ–‡æ¡ç›®åˆ™ç›´æ¥è¿”å›
            if context_items:
                return context_items
        items: List[ContextItem] = []
        if isinstance(result, list):
            for idx, entry in enumerate(result):
                if isinstance(entry, dict):
                    items.append(
                        ContextItem(
                            id=f"{call_id or tool_name}_{idx}",
                            content=str(entry.get("content", "")),
                            source=str(entry.get("source", tool_name)),
                            metadata=entry.get("metadata"),
                            relevance_score=float(entry.get("relevance_score", 0.0)),
                        )
                    )
                else:
                    items.append(
                        ContextItem(
                            id=f"{call_id or tool_name}_{idx}",
                            content=str(entry),
                            source=tool_name,
                        )
                    )
            return items
        if isinstance(result, dict):
            items.append(
                ContextItem(
                    id=call_id or f"{tool_name}_0",
                    content=str(result.get("content", result)),
                    source=str(result.get("source", tool_name)),
                    metadata=result.get("metadata"),
                    relevance_score=float(result.get("relevance_score", 0.0)),
                )
            )
            return items
        items.append(
            ContextItem(
                id=call_id or f"{tool_name}_0",
                content=str(result),
                source=tool_name,
            )
        )
        return items
    async def evaluate_and_filter_context(
    self,
    intent,
    context_items: List,
) -> Tuple[ContextSufficiency, List]:
        """
        æ”¹è¿›ç‰ˆä¸Šä¸‹æ–‡è¯„ä¼°å‡½æ•°ï¼š
        - åŠ¨æ€é˜ˆå€¼é€‚é…é—®å¥
        - æ”¯æŒ LLM è¯­ä¹‰è¡¥å¿
        - è¾“å‡ºè¯¦ç»†æ—¥å¿—
        """
        import re
        if not context_items:
            return ContextSufficiency.INSUFFICIENT, []

        query = intent.query.strip()
        is_question = query.endswith("?") or "ï¼Ÿ" in query
        base_threshold = 0.15
        threshold = 0.15 if is_question else base_threshold

        logger.info(f"Evaluating {len(context_items)} context items for query: '{query}' (threshold={threshold:.2f})")

        surviving_items = []
        high_score_count = 0

        for idx, item in enumerate(context_items):
            raw_score = item.relevance_score or 0.0
            item.metadata = item.metadata or {}

            # --- Step 1: åˆ¤æ–­æ˜¯å¦éœ€è¦è¯­ä¹‰è¡¥å¿ ---
            semantic_boost = 0.0
            if raw_score < threshold:
                # è°ƒç”¨ LLM æ£€æŸ¥è¯­ä¹‰ç›¸å…³æ€§ï¼ˆè½»é‡æ–¹å¼ï¼‰
                prompt = (
                    f"åˆ¤æ–­ä»¥ä¸‹ä¸¤å¥è¯æ˜¯å¦è¯­ä¹‰ç›¸å…³ã€‚åªè¾“å‡ºä¸€ä¸ª0åˆ°1ä¹‹é—´çš„æ•°å­—ã€‚\n"
                    f"å¥å­1: {query}\nå¥å­2: {item.content[:200]}"
                )
                try:
                    # ç›´æ¥ä½¿ç”¨ OpenAI å®¢æˆ·ç«¯ï¼Œé¿å… LlamaIndex çš„è¿”å›æ ¼å¼é—®é¢˜
                    from utils.llm import get_openai_client
                    client = get_openai_client()
                    if client:
                        response = await asyncio.to_thread(
                            client.chat.completions.create,
                            model=config.LLM_MODEL,
                            messages=[{"role": "user", "content": prompt}],
                            temperature=0.3,
                            max_tokens=50
                        )
                        llm_text = response.choices[0].message.content.strip()
                        match = re.search(r"([0-1](?:\.\d+)?)", llm_text)
                        if match:
                            semantic_score = float(match.group(1))
                            # åŠ¨æ€è®¡ç®— boostï¼šæ ¹æ® LLM çš„è¯­ä¹‰ç›¸å…³æ€§åˆ†æ•°æ¥è°ƒæ•´
                            # semantic_score è¶Šé«˜ï¼Œboost è¶Šå¤§ï¼Œä½†ä¸è¶…è¿‡ 0.3
                            # ä¾‹å¦‚ï¼š0.6 -> 0.1, 0.7 -> 0.15, 0.8 -> 0.2, 0.9 -> 0.25, 1.0 -> 0.3
                            if semantic_score > 0.6:
                                # çº¿æ€§æ˜ å°„ï¼š0.6 -> 0.1, 1.0 -> 0.3
                                semantic_boost = 0.1 + (semantic_score - 0.6) * 0.5  # (1.0-0.6) * 0.5 = 0.2, æ‰€ä»¥ 1.0 -> 0.3
                                semantic_boost = min(0.3, semantic_boost)  # é™åˆ¶æœ€å¤§ boost ä¸º 0.3
                                logger.info(f"ğŸ” LLM semantic boost applied: semantic_score={semantic_score:.2f}, boost={semantic_boost:.2f} for {item.source}")
                except Exception as e:
                    logger.warning(f"LLM relevance check failed for {item.source}: {e}")

            # --- Step 2: ç»¼åˆåˆ†æ•° ---
            final_score = min(1.0, raw_score + semantic_boost)
            item.metadata["final_relevance_score"] = final_score

            logger.info(
                f"  Item {idx+1}: source={item.source}, "
                f"raw={raw_score:.4f}, boost={semantic_boost:.2f}, "
                f"final={final_score:.4f}, "
                f"content_preview={item.content[:60]}..."
            )

            # --- Step 3: è¿‡æ»¤é€»è¾‘ ---
            if final_score >= threshold:
                surviving_items.append(item)
                if final_score >= 0.6:
                    high_score_count += 1
                logger.info(f"âœ“ Passed: {item.source}, score={final_score:.4f}")
            else:
                logger.warning(f"âœ— Filtered: {item.source}, score={final_score:.4f} < {threshold:.2f}")
        # --- Step 4: åˆ¤æ–­å……åˆ†æ€§ ---
        if not surviving_items:
            suff = ContextSufficiency.INSUFFICIENT
            logger.info(f"Context evaluation: INSUFFICIENT (no items passed filter)")
        else:
            # è·å–æœ€é«˜åˆ†æ•°
            max_score = max((i.metadata.get("final_relevance_score", 0.0) for i in surviving_items), default=0.0)
            
            # åˆ¤æ–­æ¡ä»¶ï¼š
            # 1. æœ‰ 2 ä¸ªæˆ–æ›´å¤šé«˜åˆ†é¡¹ï¼ˆscore >= 0.6ï¼‰-> SUFFICIENT
            # 2. æˆ–è€…æœ‰ 1 ä¸ªé«˜åˆ†é¡¹ï¼ˆscore >= 0.6ï¼‰-> SUFFICIENTï¼ˆé™ä½è¦æ±‚ï¼Œåªè¦æœ‰ 1 ä¸ªé«˜åˆ†é¡¹å°±è¶³å¤Ÿï¼‰
            # 3. æˆ–è€…æœ€é«˜åˆ†æ•° >= 0.5 -> SUFFICIENTï¼ˆè¿›ä¸€æ­¥é™ä½è¦æ±‚ï¼Œåªè¦æœ‰ä¸­ç­‰ç›¸å…³åº¦å°±è¶³å¤Ÿï¼‰
            # 4. å¦åˆ™ -> UNKNOWN
            if high_score_count >= 2:
                suff = ContextSufficiency.SUFFICIENT
                logger.info(f"Context evaluation: SUFFICIENT ({high_score_count} high-score items >= 0.6)")
            elif high_score_count >= 1:
                suff = ContextSufficiency.SUFFICIENT
                logger.info(f"Context evaluation: SUFFICIENT (1 high-score item >= 0.6, max_score={max_score:.4f})")
            elif max_score >= 0.5:
                suff = ContextSufficiency.SUFFICIENT
                logger.info(f"Context evaluation: SUFFICIENT (max_score={max_score:.4f} >= 0.5, {len(surviving_items)} items)")
            else:
                suff = ContextSufficiency.UNKNOWN
                logger.info(f"Context evaluation: UNKNOWN ({len(surviving_items)} surviving items, max_score={max_score:.4f} < 0.5)")
        return suff, surviving_items
    def _create_session_index(self, session_id: str) -> VectorStoreIndex:
        """é¦–æ¬¡é‡åˆ°æŸä¸ªä¼šè¯æ—¶åˆ›å»ºä¸“å±ç´¢å¼•ï¼ˆæŒä¹…åŒ–åˆ° session_root/session_idï¼‰"""
        persist_dir = self.session_root / session_id
        storage = self._build_storage_context(persist_dir)
        # åˆ›å»ºä¸€ä¸ªç©ºçš„ç´¢å¼•ï¼Œåç»­å¯ä»¥é€šè¿‡ insert_documents æ·»åŠ æ–‡æ¡£
        from llama_index.core.schema import Document
        empty_doc = Document(text="", metadata={})
        return VectorStoreIndex.from_documents(
            [empty_doc],
            storage_context=storage,
        )

    def _get_session_index(self, session_id: str) -> VectorStoreIndex:
        index = self.session_indices.get(session_id)
        if index is None:
            index = self._create_session_index(session_id)
            self.session_indices[session_id] = index
        return index
    
    async def store_to_session_memory(
        self,
        session_id: str,
        context: ContextCollection,
        query: str
    ) -> None:
        """
        å°†æœ¬è½® context.items å†™å…¥å½“å‰ä¼šè¯çš„å‘é‡æ•°æ®åº“ï¼ˆChromaDBï¼‰ï¼Œä¾›åç»­æ£€ç´¢ä½¿ç”¨ã€‚
        ç»Ÿä¸€ä½¿ç”¨ ChromaDB å­˜å‚¨ï¼Œç¡®ä¿ä¸æ£€ç´¢ä¸€è‡´ã€‚
        """
        if not context.items:
            return
        try:
            from utils.vectorstore import add_session_memory_to_vectorstore
            
            # å°†æ¯ä¸ª context item å­˜å‚¨åˆ° ChromaDB
            stored_count = 0
            for item in context.items:
                # æ„å»ºå­˜å‚¨å†…å®¹
                content = item.content
                
                # å‡†å¤‡å…ƒæ•°æ®
                metadata = {
                    "session_id": session_id,
                    "source": item.source,
                    "relevance_score": item.relevance_score,
                    "query": query,
                    **(item.metadata or {}),
                }
                
                # ç¡®å®šå†…å®¹ç±»å‹
                content_type = item.metadata.get("context_type", "general") if item.metadata else "general"
                
                # å­˜å‚¨åˆ° ChromaDB
                success = add_session_memory_to_vectorstore(
                    session_id=session_id,
                    content=content,
                    content_type=content_type,
                    metadata=metadata
                )
                
                if success:
                    stored_count += 1
                else:
                    logger.warning(f"Failed to store item to session memory: {item.source}, content={content[:50]}...")
            
            logger.info(f"Stored {stored_count}/{len(context.items)} items to session {session_id} memory (ChromaDB)")
        except Exception as e:
            logger.warning(f"Failed to store session memory: {e}", exc_info=True)
            
    async def retrieve_session_memory(
        self,
        session_id: str,
        query: str,
        top_k: int = 6,
    ) -> list[ContextItem]:
        """ä»å½“å‰ä¼šè¯ä¸“å±ç´¢å¼•é‡Œæ£€ç´¢ä¸ query ç›¸å…³çš„è®°å¿†"""
        try:
            from functools import partial
            session_index = self._get_session_index(session_id)
            retriever = session_index.as_retriever(similarity_top_k=top_k)
            # LlamaIndex çš„ retrieve æ˜¯åŒæ­¥è°ƒç”¨ï¼Œæ”¾åˆ°çº¿ç¨‹æ± é‡Œè·‘
            loop = asyncio.get_event_loop()
            nodes = await loop.run_in_executor(
                None,
                partial(retriever.retrieve, query),
            )
            items: list[ContextItem] = []
            for idx, node in enumerate(nodes):
                # node æ˜¯ NodeWithScoreï¼Œéœ€è¦é€šè¿‡ node.node è®¿é—®å®é™…èŠ‚ç‚¹
                node_content = node.node.get_content() if hasattr(node.node, 'get_content') else node.node.text
                node_metadata = node.node.metadata if hasattr(node.node, 'metadata') else {}
                items.append(
                    ContextItem(
                        id=f"{session_id}_memory_{idx}",
                        content=node_content,
                        source=node_metadata.get("source", "session_memory"),
                        metadata={
                            **node_metadata,
                            "similarity": node.score,
                            "query": query,
                        },
                        relevance_score=node.score or 0.8,
                    )
                )
            return items
        except Exception as e:
            logger.warning(f"retrieve_session_memory failed for {session_id}: {e}", exc_info=True)
            return []

    async def check_schedule_conflict(
        self,
        context: ContextCollection,
        user_query: str,
    ) -> Dict[str, Any]:
        """
        æ£€æŸ¥ç”¨æˆ·æŸ¥è¯¢ä¸å¾…åŠäº‹é¡¹ä¹‹é—´æ˜¯å¦å­˜åœ¨æ—¶é—´å†²çªã€‚
        è¿”å›ç»“æ„:
        {
            "has_conflict": bool,
            "warning_message": str,
            "conflict_details": List[Dict[str, Any]]
        }
        """
        try:
            time_keywords = [
                "æ˜å¤©",
                "åå¤©",
                "ä»Šå¤©",
                "ä¸‹å‘¨ä¸€",
                "ä¸‹å‘¨äºŒ",
                "ä¸‹å‘¨ä¸‰",
                "ä¸‹å‘¨å››",
                "ä¸‹å‘¨äº”",
                "ä¸‹å‘¨",
                "ä¸‹ä¸ªæœˆ",
                "å‘¨æœ«",
                "è®¡åˆ’",
                "å‡ºè¡Œ",
                "å»",
                "ç©",
                "æ—…è¡Œ",
                "æ´»åŠ¨",
                "å®‰æ’",
                "æ—¶é—´",
                "å‡ ç‚¹",
                "ä»€ä¹ˆæ—¶å€™",
                "ä½•æ—¶",
                "æ—¥æœŸ",
                "è¡Œç¨‹",
            ]
            query_lower = user_query.lower()
            if not any(keyword in query_lower for keyword in time_keywords):
                return {"has_conflict": False}

            task_items = [
                item
                for item in context.items
                if item.source == "user_profile"
                and item.metadata.get("context_type") == "task"
            ]
            if not task_items:
                return {"has_conflict": False}

            todo_ids = [
                int(todo_id)
                for item in task_items
                for todo_id in [item.metadata.get("todo_id")]
                if todo_id
            ]
            if not todo_ids:
                return {"has_conflict": False}

            all_todos = get_todos(limit=100)
            relevant_todos = []
            for todo in all_todos:
                if todo.get("id") in todo_ids:
                    relevant_todos.append(
                        {
                            "id": todo.get("id"),
                            "title": todo.get("title", ""),
                            "description": todo.get("description", ""),
                            "status": todo.get("status", 0),
                        }
                    )
            if not relevant_todos:
                return {"has_conflict": False}
            result = await self.tools_executor.run_async(
                "schedule_conflict_check",
                user_query=user_query,
                todos=[
                    {
                        "title": todo.get("title", ""),
                        "description": todo.get("description", ""),
                    }
                    for todo in relevant_todos
                ],
            )

            if isinstance(result, dict):
                return result

        except Exception as e:
            logger.exception(f"Error checking schedule conflict: {e}")

        return {"has_conflict": False}

    # ------------------------------------------------------------------
    # è¾…åŠ©æ–¹æ³•

    def _build_storage_context(self, persist_dir: Path | str) -> StorageContext:
        """æ„å»º StorageContextï¼Œè‹¥æŒä¹…åŒ–æ•°æ®ç¼ºå¤±åˆ™è‡ªåŠ¨å¼•å¯¼åˆ›å»ºã€‚"""
        target = Path(persist_dir)
        target.mkdir(parents=True, exist_ok=True)
        try:
            return StorageContext.from_defaults(persist_dir=str(target))
        except (ValueError, FileNotFoundError) as exc:
            message = str(exc)
            if (
                "No existing llama_index.core.vector_stores.simple" not in message
                and "docstore.json" not in message
            ):
                raise

            logger.info(
                "Persisted storage not found at %s, initializing fresh store",
                target,
            )

            fresh_context = StorageContext.from_defaults(
                docstore=SimpleDocumentStore(),
                index_store=SimpleIndexStore(),
                vector_store=SimpleVectorStore(),
                graph_store=SimpleGraphStore(),
            )
            fresh_context.persist(persist_dir=str(target))
            return StorageContext.from_defaults(persist_dir=str(target))
        
        

